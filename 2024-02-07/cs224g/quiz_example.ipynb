{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a5dd47a419fb6c4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This example shows a slightly more complex LLM example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T05:18:48.297686Z",
     "start_time": "2024-02-08T05:18:46.522434Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# load some extensions / magic...\n",
    "%load_ext hamilton_magic\n",
    "%reload_ext hamilton_magic\n",
    "# load extension\n",
    "%load_ext autoreload\n",
    "# configure autoreload to only affect specified files\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "777658990e2a4cfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T05:18:52.527817Z",
     "start_time": "2024-02-08T05:18:52.094722Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the base libraries\n",
    "from hamilton import driver\n",
    "import pprint\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4dc88e682f5e0bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T05:18:54.017406Z",
     "start_time": "2024-02-08T05:18:53.607201Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 9.0.0 (20230911.1827)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"532pt\" height=\"405pt\"\n",
       " viewBox=\"0.00 0.00 531.92 405.30\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 401.3)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-401.3 527.92,-401.3 527.92,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster__legend</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-259.3 8,-389.3 92.85,-389.3 92.85,-259.3 8,-259.3\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.42\" y=\"-372\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Legend</text>\n",
       "</g>\n",
       "<!-- quiz_bank -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>quiz_bank</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M82.22,-249.1C82.22,-249.1 18.62,-249.1 18.62,-249.1 12.62,-249.1 6.62,-243.1 6.62,-237.1 6.62,-237.1 6.62,-197.5 6.62,-197.5 6.62,-191.5 12.62,-185.5 18.62,-185.5 18.62,-185.5 82.22,-185.5 82.22,-185.5 88.22,-185.5 94.22,-191.5 94.22,-197.5 94.22,-197.5 94.22,-237.1 94.22,-237.1 94.22,-243.1 88.22,-249.1 82.22,-249.1\"/>\n",
       "<text text-anchor=\"start\" x=\"17.42\" y=\"-226\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">quiz_bank</text>\n",
       "<text text-anchor=\"start\" x=\"42.92\" y=\"-198\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- quiz_generator_system_prompt -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>quiz_generator_system_prompt</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M339.07,-208.1C339.07,-208.1 135.22,-208.1 135.22,-208.1 129.22,-208.1 123.22,-202.1 123.22,-196.1 123.22,-196.1 123.22,-156.5 123.22,-156.5 123.22,-150.5 129.22,-144.5 135.22,-144.5 135.22,-144.5 339.07,-144.5 339.07,-144.5 345.07,-144.5 351.07,-150.5 351.07,-156.5 351.07,-156.5 351.07,-196.1 351.07,-196.1 351.07,-202.1 345.07,-208.1 339.07,-208.1\"/>\n",
       "<text text-anchor=\"start\" x=\"134.02\" y=\"-185\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">quiz_generator_system_prompt</text>\n",
       "<text text-anchor=\"start\" x=\"229.65\" y=\"-157\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- quiz_bank&#45;&gt;quiz_generator_system_prompt -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>quiz_bank&#45;&gt;quiz_generator_system_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M94.27,-207.79C99.8,-206.56 105.66,-205.26 111.74,-203.91\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"112.4,-207.35 121.4,-201.77 110.88,-200.52 112.4,-207.35\"/>\n",
       "</g>\n",
       "<!-- delimiter -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>delimiter</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M77.72,-167.1C77.72,-167.1 23.12,-167.1 23.12,-167.1 17.12,-167.1 11.12,-161.1 11.12,-155.1 11.12,-155.1 11.12,-115.5 11.12,-115.5 11.12,-109.5 17.12,-103.5 23.12,-103.5 23.12,-103.5 77.72,-103.5 77.72,-103.5 83.72,-103.5 89.72,-109.5 89.72,-115.5 89.72,-115.5 89.72,-155.1 89.72,-155.1 89.72,-161.1 83.72,-167.1 77.72,-167.1\"/>\n",
       "<text text-anchor=\"start\" x=\"21.92\" y=\"-144\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">delimiter</text>\n",
       "<text text-anchor=\"start\" x=\"42.92\" y=\"-116\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- delimiter&#45;&gt;quiz_generator_system_prompt -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>delimiter&#45;&gt;quiz_generator_system_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M90.15,-143.89C96.91,-145.4 104.25,-147.03 111.94,-148.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"110.99,-152.11 121.51,-150.86 112.51,-145.27 110.99,-152.11\"/>\n",
       "</g>\n",
       "<!-- llm_quiz_response -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>llm_quiz_response</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M511.92,-126.1C511.92,-126.1 392.07,-126.1 392.07,-126.1 386.07,-126.1 380.07,-120.1 380.07,-114.1 380.07,-114.1 380.07,-74.5 380.07,-74.5 380.07,-68.5 386.07,-62.5 392.07,-62.5 392.07,-62.5 511.92,-62.5 511.92,-62.5 517.92,-62.5 523.92,-68.5 523.92,-74.5 523.92,-74.5 523.92,-114.1 523.92,-114.1 523.92,-120.1 517.92,-126.1 511.92,-126.1\"/>\n",
       "<text text-anchor=\"start\" x=\"390.87\" y=\"-103\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">llm_quiz_response</text>\n",
       "<text text-anchor=\"start\" x=\"444.5\" y=\"-75\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- quiz_generator_system_prompt&#45;&gt;llm_quiz_response -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>quiz_generator_system_prompt&#45;&gt;llm_quiz_response</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M327.96,-144.07C335.79,-141.15 343.58,-138.19 351.07,-135.3 357.01,-133 363.13,-130.6 369.29,-128.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"370.25,-131.53 378.23,-124.56 367.65,-125.03 370.25,-131.53\"/>\n",
       "</g>\n",
       "<!-- llm_client -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>llm_client</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M267.45,-126.1C267.45,-126.1 206.85,-126.1 206.85,-126.1 200.85,-126.1 194.85,-120.1 194.85,-114.1 194.85,-114.1 194.85,-74.5 194.85,-74.5 194.85,-68.5 200.85,-62.5 206.85,-62.5 206.85,-62.5 267.45,-62.5 267.45,-62.5 273.45,-62.5 279.45,-68.5 279.45,-74.5 279.45,-74.5 279.45,-114.1 279.45,-114.1 279.45,-120.1 273.45,-126.1 267.45,-126.1\"/>\n",
       "<text text-anchor=\"start\" x=\"205.65\" y=\"-103\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">llm_client</text>\n",
       "<text text-anchor=\"start\" x=\"213.9\" y=\"-75\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">OpenAI</text>\n",
       "</g>\n",
       "<!-- llm_client&#45;&gt;llm_quiz_response -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>llm_client&#45;&gt;llm_quiz_response</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M279.95,-94.3C305.18,-94.3 338.2,-94.3 368.5,-94.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"368.44,-97.8 378.44,-94.3 368.44,-90.8 368.44,-97.8\"/>\n",
       "</g>\n",
       "<!-- _llm_quiz_response_inputs -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>_llm_quiz_response_inputs</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"287.95,-44.6 186.35,-44.6 186.35,0 287.95,0 287.95,-44.6\"/>\n",
       "<text text-anchor=\"start\" x=\"201.15\" y=\"-16.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">question</text>\n",
       "<text text-anchor=\"start\" x=\"258.15\" y=\"-16.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- _llm_quiz_response_inputs&#45;&gt;llm_quiz_response -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>_llm_quiz_response_inputs&#45;&gt;llm_quiz_response</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M288.3,-34.67C308.03,-39.9 330.77,-46.38 351.07,-53.3 357.04,-55.34 363.17,-57.54 369.31,-59.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"367.62,-62.95 378.21,-63.25 370.12,-56.41 367.62,-62.95\"/>\n",
       "</g>\n",
       "<!-- input -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>input</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"77.42,-358.6 23.42,-358.6 23.42,-322 77.42,-322 77.42,-358.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.42\" y=\"-334.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">input</text>\n",
       "</g>\n",
       "<!-- function -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>function</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M72.85,-303.6C72.85,-303.6 28,-303.6 28,-303.6 22,-303.6 16,-297.6 16,-291.6 16,-291.6 16,-279 16,-279 16,-273 22,-267 28,-267 28,-267 72.85,-267 72.85,-267 78.85,-267 84.85,-273 84.85,-279 84.85,-279 84.85,-291.6 84.85,-291.6 84.85,-297.6 78.85,-303.6 72.85,-303.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.42\" y=\"-279.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">function</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x104b8c490>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%with_functions -m quiz\n",
    "import openai\n",
    "\n",
    "def quiz_bank() -> str:\n",
    "    return \"\"\"1. Subject: Leonardo DaVinci\n",
    "   Categories: Art, Science\n",
    "   Facts:\n",
    "    - Painted the Mona Lisa\n",
    "    - Studied zoology, anatomy, geology, optics\n",
    "    - Designed a flying machine\n",
    "\n",
    "2. Subject: Paris\n",
    "   Categories: Art, Geography\n",
    "   Facts:\n",
    "    - Location of the Louvre, the museum where the Mona Lisa is displayed\n",
    "    - Capital of France\n",
    "    - Most populous city in France\n",
    "    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\n",
    "\n",
    "3. Subject: Telescopes\n",
    "   Category: Science\n",
    "   Facts:\n",
    "    - Device to observe different objects\n",
    "    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n",
    "    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n",
    "\n",
    "4. Subject: Starry Night\n",
    "   Category: Art\n",
    "   Facts:\n",
    "    - Painted by Vincent van Gogh in 1889\n",
    "    - Captures the east-facing view of van Gogh's room in Saint-RÃ©my-de-Provence\n",
    "\n",
    "5. Subject: Physics\n",
    "   Category: Science\n",
    "   Facts:\n",
    "    - The sun doesn't change color during sunset.\n",
    "    - Water slows the speed of light\n",
    "    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\n",
    "\"\"\"\n",
    "\n",
    "def delimiter() -> str:\n",
    "    return \"####\"\n",
    "\n",
    "def quiz_generator_system_prompt(delimiter: str, quiz_bank: str) -> str:\n",
    "    return f\"\"\"Follow these steps to generate a customized quiz for the user.\n",
    "The question will be delimited with four hashtags i.e {delimiter}\n",
    "\n",
    "Step 1:{delimiter} First identify the category user is asking about from the following list:\n",
    "* Geography\n",
    "* Science\n",
    "* Art\n",
    "\n",
    "Step 2:{delimiter} Determine the subjects to generate questions about. The list of topics are below:\n",
    "\n",
    "{quiz_bank}\n",
    "\n",
    "Pick up to two subjects that fit the user's category.\n",
    "\n",
    "Step 3:{delimiter} Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\n",
    "* Include any facts that might be interesting\n",
    "\n",
    "Use the following format:\n",
    "Question 1:{delimiter} <question 1>\n",
    "\n",
    "Question 2:{delimiter} <question 2>\n",
    "\n",
    "Question 3:{delimiter} <question 3>\n",
    "\"\"\"\n",
    "\n",
    "def llm_client() -> openai.OpenAI:\n",
    "    \"\"\"The LLM client to use for the RAG model.\"\"\"\n",
    "    return openai.OpenAI()\n",
    "\n",
    "def llm_quiz_response(\n",
    "    quiz_generator_system_prompt: str, question: str, llm_client: openai.OpenAI\n",
    ") -> str:\n",
    "    \"\"\"Creates the RAG response from the LLM model for the given prompt.\n",
    "\n",
    "    :param quiz_generator_system_prompt: the prompt to send to the LLM.\n",
    "    :param question: the prompt to send to the LLM.\n",
    "    :param llm_client: the LLM client to use.\n",
    "    :return: the response from the LLM.\n",
    "    \"\"\"\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": quiz_generator_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e9c037963199fe9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T05:18:54.617883Z",
     "start_time": "2024-02-08T05:18:54.605039Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dr1 = (\n",
    "        driver.Builder()\n",
    "        .with_modules(quiz)\n",
    "        .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7227a7a25871b0f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T05:18:58.484301Z",
     "start_time": "2024-02-08T05:18:55.999200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1:####\n",
      "What famous painting did Leonardo DaVinci create?\n",
      "a) The Starry Night\n",
      "b) The Last Supper\n",
      "c) The Scream\n",
      "d) Guernica\n",
      "\n",
      "Question 2:####\n",
      "In addition to being an artist, what other fields did Leonardo DaVinci study?\n",
      "a) Zoology, anatomy, geology, optics\n",
      "b) Mathematics, astronomy, chemistry\n",
      "c) Literature, philosophy, psychology\n",
      "d) Music, dance, theater\n",
      "\n",
      "Question 3:####\n",
      "What invention did Leonardo DaVinci design?\n",
      "a) The printing press\n",
      "b) The telephone\n",
      "c) The flying machine\n",
      "d) The steam engine\n"
     ]
    }
   ],
   "source": [
    "result = dr1.execute([\"llm_quiz_response\"], inputs={\"question\": \"Write me a quiz about Leonardo DaVinci\"})\n",
    "print(result[\"llm_quiz_response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38e65750ca0e0f2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T05:19:17.260420Z",
     "start_time": "2024-02-08T05:19:15.766796Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1:####\n",
      "What famous painting did Leonardo DaVinci create?\n",
      "\n",
      "Question 2:####\n",
      "In addition to being an artist, what other fields did Leonardo DaVinci study?\n",
      "\n",
      "Question 3:####\n",
      "What invention did Leonardo DaVinci design?\n"
     ]
    }
   ],
   "source": [
    "result2 = dr1.execute([\"llm_quiz_response\"], inputs={\"question\": \"I would like a quiz about Leonardo DaVinci please.\"})\n",
    "print(result2[\"llm_quiz_response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e1f5660467d0dca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T05:36:19.041784Z",
     "start_time": "2024-02-08T05:36:18.801835Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Follow these steps to generate a customized quiz for the user.\n",
      "The question will be delimited with four hashtags i.e ####\n",
      "\n",
      "Step 1:#### First identify the category user is asking about from the following list:\n",
      "* Geography\n",
      "* Science\n",
      "* Art\n",
      "\n",
      "Step 2:#### Determine the subjects to generate questions about. The list of topics are below:\n",
      "\n",
      "1. Subject: Leonardo DaVinci\n",
      "   Categories: Art, Science\n",
      "   Facts:\n",
      "    - Painted the Mona Lisa\n",
      "    - Studied zoology, anatomy, geology, optics\n",
      "    - Designed a flying machine\n",
      "\n",
      "2. Subject: Paris\n",
      "   Categories: Art, Geography\n",
      "   Facts:\n",
      "    - Location of the Louvre, the museum where the Mona Lisa is displayed\n",
      "    - Capital of France\n",
      "    - Most populous city in France\n",
      "    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\n",
      "\n",
      "3. Subject: Telescopes\n",
      "   Category: Science\n",
      "   Facts:\n",
      "    - Device to observe different objects\n",
      "    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n",
      "    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n",
      "\n",
      "4. Subject: Starry Night\n",
      "   Category: Art\n",
      "   Facts:\n",
      "    - Painted by Vincent van Gogh in 1889\n",
      "    - Captures the east-facing view of van Gogh's room in Saint-RÃ©my-de-Provence\n",
      "\n",
      "5. Subject: Physics\n",
      "   Category: Science\n",
      "   Facts:\n",
      "    - The sun doesn't change color during sunset.\n",
      "    - Water slows the speed of light\n",
      "    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\n",
      "\n",
      "\n",
      "Pick up to two subjects that fit the user's category.\n",
      "\n",
      "Step 3:#### Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\n",
      "* Include any facts that might be interesting\n",
      "\n",
      "Use the following format:\n",
      "Question 1:#### <question 1>\n",
      "\n",
      "Question 2:#### <question 2>\n",
      "\n",
      "Question 3:#### <question 3>\n"
     ]
    }
   ],
   "source": [
    "result3 = dr1.execute([\"quiz_generator_system_prompt\"], inputs={\"question\": \"Write me a quiz about Leonardo DaVinci\"})\n",
    "print(result3[\"quiz_generator_system_prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56cacad13007137",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Let's talk traditional testing \n",
    "\n",
    "* Unit testing\n",
    "* Integration testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53b2e5c0cd81d41d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T05:36:22.192285Z",
     "start_time": "2024-02-08T05:36:22.173019Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pytest style tests\n",
    "def test_quiz_generator_system_prompt():\n",
    "    assert quiz_generator_system_prompt(quiz.delimiter(), quiz.quiz_bank()).startswith(\"Follow these steps to generate a customized quiz for the user.\")\n",
    "    \n",
    "test_quiz_generator_system_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cb785dfa89f2074",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T05:36:25.361713Z",
     "start_time": "2024-02-08T05:36:23.383191Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pytest style tests\n",
    "def test_end_to_end():\n",
    "    dr = (\n",
    "        driver.Builder()\n",
    "        .with_modules(quiz)\n",
    "        .build()\n",
    "    )\n",
    "    result = dr.execute([\"llm_quiz_response\", \"quiz_generator_system_prompt\"], inputs={\"question\": \"Write me a quiz about Leonardo DaVinci\"})\n",
    "    # these tests are short to save space\n",
    "    assert result[\"llm_quiz_response\"].startswith(\"Question 1:####\")\n",
    "    assert result[\"quiz_generator_system_prompt\"].startswith(\"Follow these steps to generate a customized quiz for the user.\")\n",
    "    # etc...\n",
    "test_end_to_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e6c94c53233743",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# note about overrides:\n",
    "\n",
    "* You can inject a result using the `overrides` parameter in the `execute` method.\n",
    "* This is useful for integration testing -- if you want to keep a few things \"constant\".\n",
    "* This is also useful for iteration and development. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55c000fd42810191",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T05:36:25.404468Z",
     "start_time": "2024-02-08T05:36:25.366848Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from unittest import mock\n",
    "import openai\n",
    "\n",
    "class MockReturn:\n",
    "    def __init__(self, content: str):\n",
    "        self.choices = [mock.MagicMock(message=mock.MagicMock(content=content))]\n",
    "    \n",
    "\n",
    "def test_end_to_end_with_mock():\n",
    "    dr = (\n",
    "        driver.Builder()\n",
    "        .with_modules(quiz)\n",
    "        .build()\n",
    "    )\n",
    "    mock_client = mock.MagicMock(openai.OpenAI())\n",
    "    mock_client.chat.completions.create.return_value = MockReturn(\"proof that this mock works!\")\n",
    "    result = dr.execute([\"llm_quiz_response\", \"quiz_generator_system_prompt\"], \n",
    "                        inputs={\"question\": \"Write me a quiz about Leonardo DaVinci\"},\n",
    "                        overrides={\"llm_client\": mock_client})\n",
    "    # these tests are short to save space\n",
    "    assert result[\"llm_quiz_response\"].startswith(\"proof that this mock works!\")\n",
    "    assert result[\"quiz_generator_system_prompt\"].startswith(\"Follow these steps to generate a customized quiz for the user.\")\n",
    "    # etc...\n",
    "\n",
    "test_end_to_end_with_mock()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952ae592b72d88b1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# But what should we really be testing/evaluating?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f288c6364feb31a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T05:36:26.697274Z",
     "start_time": "2024-02-08T05:36:26.691607Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ðŸ¤”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ff76cc98afbee5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# LLM Testing & Evaluation\n",
    "Testing & evaluation are inherently coupled with LLM apps. \n",
    "\n",
    "Things we might want to test/\"grade\":\n",
    " - formatting\n",
    " - prompt injection / maybe you'd augment things to check that the question can be satisfied?\n",
    " - relevance of quiz to the question\n",
    " - factuality of the quiz\n",
    " - likely a lot more around context generation than just the prompt for the quiz...\n",
    " \n",
    "\n",
    "# My mental model\n",
    "Prompt + LLM API == equivalent to a \"Model\" in the ML world.\n",
    "i.e. prompts are just hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffe7e397e0ca267",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# LLM Graders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8868bc9d6a3ea1d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T05:36:32.262140Z",
     "start_time": "2024-02-08T05:36:31.900128Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 9.0.0 (20230911.1827)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"603pt\" height=\"355pt\"\n",
       " viewBox=\"0.00 0.00 603.05 354.80\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 350.8)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-350.8 599.05,-350.8 599.05,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster__legend</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"39.5,-208.8 39.5,-338.8 124.35,-338.8 124.35,-208.8 39.5,-208.8\"/>\n",
       "<text text-anchor=\"middle\" x=\"81.92\" y=\"-321.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Legend</text>\n",
       "</g>\n",
       "<!-- eval_format_user_prompt -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>eval_format_user_prompt</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M379.07,-63.6C379.07,-63.6 214.22,-63.6 214.22,-63.6 208.22,-63.6 202.22,-57.6 202.22,-51.6 202.22,-51.6 202.22,-12 202.22,-12 202.22,-6 208.22,0 214.22,0 214.22,0 379.07,0 379.07,0 385.07,0 391.07,-6 391.07,-12 391.07,-12 391.07,-51.6 391.07,-51.6 391.07,-57.6 385.07,-63.6 379.07,-63.6\"/>\n",
       "<text text-anchor=\"start\" x=\"213.02\" y=\"-40.5\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">eval_format_user_prompt</text>\n",
       "<text text-anchor=\"start\" x=\"289.15\" y=\"-12.5\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- eval_format_response -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>eval_format_response</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M583.05,-135.6C583.05,-135.6 441.45,-135.6 441.45,-135.6 435.45,-135.6 429.45,-129.6 429.45,-123.6 429.45,-123.6 429.45,-84 429.45,-84 429.45,-78 435.45,-72 441.45,-72 441.45,-72 583.05,-72 583.05,-72 589.05,-72 595.05,-78 595.05,-84 595.05,-84 595.05,-123.6 595.05,-123.6 595.05,-129.6 589.05,-135.6 583.05,-135.6\"/>\n",
       "<text text-anchor=\"start\" x=\"440.25\" y=\"-112.5\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">eval_format_response</text>\n",
       "<text text-anchor=\"start\" x=\"504.75\" y=\"-84.5\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- eval_format_user_prompt&#45;&gt;eval_format_response -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>eval_format_user_prompt&#45;&gt;eval_format_response</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M391.34,-63.38C400.26,-66.39 409.31,-69.44 418.23,-72.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"417,-75.72 427.6,-75.6 419.24,-69.09 417,-75.72\"/>\n",
       "</g>\n",
       "<!-- eval_format_system_prompt -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>eval_format_system_prompt</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M388.45,-207.6C388.45,-207.6 204.85,-207.6 204.85,-207.6 198.85,-207.6 192.85,-201.6 192.85,-195.6 192.85,-195.6 192.85,-156 192.85,-156 192.85,-150 198.85,-144 204.85,-144 204.85,-144 388.45,-144 388.45,-144 394.45,-144 400.45,-150 400.45,-156 400.45,-156 400.45,-195.6 400.45,-195.6 400.45,-201.6 394.45,-207.6 388.45,-207.6\"/>\n",
       "<text text-anchor=\"start\" x=\"203.65\" y=\"-184.5\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">eval_format_system_prompt</text>\n",
       "<text text-anchor=\"start\" x=\"289.15\" y=\"-156.5\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- eval_format_system_prompt&#45;&gt;eval_format_response -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>eval_format_system_prompt&#45;&gt;eval_format_response</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M393.2,-143.59C401.55,-140.78 409.98,-137.94 418.3,-135.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"419.36,-138.47 427.72,-131.96 417.13,-131.83 419.36,-138.47\"/>\n",
       "</g>\n",
       "<!-- _eval_format_user_prompt_inputs -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>_eval_format_user_prompt_inputs</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"163.85,-54.1 0,-54.1 0,-9.5 163.85,-9.5 163.85,-54.1\"/>\n",
       "<text text-anchor=\"start\" x=\"14.8\" y=\"-26\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">llm_quiz_response</text>\n",
       "<text text-anchor=\"start\" x=\"134.05\" y=\"-26\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- _eval_format_user_prompt_inputs&#45;&gt;eval_format_user_prompt -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>_eval_format_user_prompt_inputs&#45;&gt;eval_format_user_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M164.26,-31.8C172.92,-31.8 181.81,-31.8 190.69,-31.8\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"190.4,-35.3 200.4,-31.8 190.4,-28.3 190.4,-35.3\"/>\n",
       "</g>\n",
       "<!-- _eval_format_response_inputs -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>_eval_format_response_inputs</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"366.2,-126.1 227.1,-126.1 227.1,-81.5 366.2,-81.5 366.2,-126.1\"/>\n",
       "<text text-anchor=\"start\" x=\"241.9\" y=\"-98\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">llm_client</text>\n",
       "<text text-anchor=\"start\" x=\"304.9\" y=\"-98\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">OpenAI</text>\n",
       "</g>\n",
       "<!-- _eval_format_response_inputs&#45;&gt;eval_format_response -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>_eval_format_response_inputs&#45;&gt;eval_format_response</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M366.41,-103.8C382.82,-103.8 400.6,-103.8 417.91,-103.8\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"417.68,-107.3 427.68,-103.8 417.68,-100.3 417.68,-107.3\"/>\n",
       "</g>\n",
       "<!-- _eval_format_system_prompt_inputs -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>_eval_format_system_prompt_inputs</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"133.1,-198.1 30.75,-198.1 30.75,-153.5 133.1,-153.5 133.1,-198.1\"/>\n",
       "<text text-anchor=\"start\" x=\"45.55\" y=\"-170\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">delimiter</text>\n",
       "<text text-anchor=\"start\" x=\"103.3\" y=\"-170\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- _eval_format_system_prompt_inputs&#45;&gt;eval_format_system_prompt -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>_eval_format_system_prompt_inputs&#45;&gt;eval_format_system_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M133.34,-175.8C147.95,-175.8 164.54,-175.8 181.39,-175.8\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"180.97,-179.3 190.97,-175.8 180.97,-172.3 180.97,-179.3\"/>\n",
       "</g>\n",
       "<!-- input -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>input</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"108.92,-308.1 54.92,-308.1 54.92,-271.5 108.92,-271.5 108.92,-308.1\"/>\n",
       "<text text-anchor=\"middle\" x=\"81.92\" y=\"-284\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">input</text>\n",
       "</g>\n",
       "<!-- function -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>function</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M104.35,-253.1C104.35,-253.1 59.5,-253.1 59.5,-253.1 53.5,-253.1 47.5,-247.1 47.5,-241.1 47.5,-241.1 47.5,-228.5 47.5,-228.5 47.5,-222.5 53.5,-216.5 59.5,-216.5 59.5,-216.5 104.35,-216.5 104.35,-216.5 110.35,-216.5 116.35,-222.5 116.35,-228.5 116.35,-228.5 116.35,-241.1 116.35,-241.1 116.35,-247.1 110.35,-253.1 104.35,-253.1\"/>\n",
       "<text text-anchor=\"middle\" x=\"81.92\" y=\"-229\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">function</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x139606e90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%with_functions -m eval_format\n",
    "import openai \n",
    "\n",
    "def eval_format_system_prompt(delimiter: str) -> str:\n",
    "    return (\n",
    "        \"You are an assistant that evaluates whether or not an assistant is producing valid quizzes. \"\n",
    "        f\"The assistant should be producing output in the format of Question N:{delimiter} <question N>?\"\n",
    "    )\n",
    "\n",
    "\n",
    "def eval_format_user_prompt(llm_quiz_response: str) -> str:\n",
    "    # one-shot\n",
    "    return (\n",
    "        \"You are evaluating a generated quiz based on the context that the assistant uses to create the quiz.\\n\"\n",
    "        \"Read the response carefully and determine if it looks like a quiz or test. \"\n",
    "        \"Do not evaluate if the information is correct only evaluate if the data is \"\n",
    "        \"in the expected format.\\n\\n\"\n",
    "        \"Output Y if the response is a quiz, output N if the response does not look like a quiz.\"\n",
    "        \"Here is the data:\\n\"\n",
    "        \"[BEGIN DATA]\\n\"\n",
    "        \"************\\n\"\n",
    "        f\"[Response]: {llm_quiz_response}\\n\"\n",
    "        \"************\\n\"\n",
    "        \"[END DATA]\\n\\n\"\n",
    "    )\n",
    "\n",
    "def eval_format_response(\n",
    "    eval_format_system_prompt: str, eval_format_user_prompt: str, llm_client: openai.OpenAI\n",
    ") -> str:\n",
    "    \"\"\"Creates the RAG response from the LLM model for the given prompt.\n",
    "\n",
    "    :param eval_format_system_prompt: the prompt to send to the LLM.\n",
    "    :param eval_format_user_prompt: the prompt to send to the LLM.\n",
    "    :param llm_client: the LLM client to use.\n",
    "    :return: the response from the LLM.\n",
    "    \"\"\"\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": eval_format_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": eval_format_user_prompt},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67285dff326bc15e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T05:36:33.736849Z",
     "start_time": "2024-02-08T05:36:33.327986Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_format_response': 'Y'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_format_driver = (\n",
    "        driver.Builder()\n",
    "        .with_modules(eval_format)\n",
    "        .build()\n",
    ")\n",
    "eval_format_driver.execute(\n",
    "    [\"eval_format_response\"], \n",
    "    inputs={\n",
    "        \"delimiter\": \"####\",\n",
    "        \"llm_client\": openai.OpenAI(),\n",
    "        \"llm_quiz_response\": \"Question 1:#### <question 1>\\n\\nQuestion 2:#### <question 2>\\n\\nQuestion 3:#### <question 3>\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccbb4327f1389a22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T05:36:36.522281Z",
     "start_time": "2024-02-08T05:36:35.972807Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1:####\n",
      "What famous painting did Leonardo DaVinci create?\n",
      "\n",
      "Question 2:####\n",
      "In addition to being an artist, what other fields did Leonardo DaVinci study?\n",
      "\n",
      "Question 3:####\n",
      "What invention did Leonardo DaVinci design?\n",
      "{'eval_format_response': 'Y'}\n",
      "Question 1:####\n",
      "What famous painting did Leonardo DaVinci create?\n",
      "a) The Starry Night\n",
      "b) The Last Supper\n",
      "c) The Scream\n",
      "d) Guernica\n",
      "\n",
      "Question 2:####\n",
      "In addition to being an artist, what other fields did Leonardo DaVinci study?\n",
      "a) Zoology, anatomy, geology, optics\n",
      "b) Mathematics, astronomy, chemistry\n",
      "c) Literature, philosophy, psychology\n",
      "d) Music, dance, theater\n",
      "\n",
      "Question 3:####\n",
      "What invention did Leonardo DaVinci design?\n",
      "a) The printing press\n",
      "b) The telephone\n",
      "c) The flying machine\n",
      "d) The steam engine\n",
      "{'eval_format_response': 'Y'}\n"
     ]
    }
   ],
   "source": [
    "for response in [result2[\"llm_quiz_response\"], result[\"llm_quiz_response\"]]:\n",
    "    print(response)\n",
    "    print(eval_format_driver.execute(\n",
    "        [\"eval_format_response\"], \n",
    "        inputs={\n",
    "            \"delimiter\": \"####\",\n",
    "            \"llm_client\": openai.OpenAI(),\n",
    "            \"llm_quiz_response\": response}\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3ac71edb970f8ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T05:36:38.253384Z",
     "start_time": "2024-02-08T05:36:37.965255Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 9.0.0 (20230911.1827)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"944pt\" height=\"446pt\"\n",
       " viewBox=\"0.00 0.00 944.38 445.80\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 441.8)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-441.8 940.38,-441.8 940.38,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster__legend</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-299.8 8,-429.8 92.85,-429.8 92.85,-299.8 8,-299.8\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.42\" y=\"-412.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Legend</text>\n",
       "</g>\n",
       "<!-- quiz_bank -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>quiz_bank</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M82.22,-289.6C82.22,-289.6 18.62,-289.6 18.62,-289.6 12.62,-289.6 6.62,-283.6 6.62,-277.6 6.62,-277.6 6.62,-238 6.62,-238 6.62,-232 12.62,-226 18.62,-226 18.62,-226 82.22,-226 82.22,-226 88.22,-226 94.22,-232 94.22,-238 94.22,-238 94.22,-277.6 94.22,-277.6 94.22,-283.6 88.22,-289.6 82.22,-289.6\"/>\n",
       "<text text-anchor=\"start\" x=\"17.42\" y=\"-266.5\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">quiz_bank</text>\n",
       "<text text-anchor=\"start\" x=\"42.92\" y=\"-238.5\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- quiz_generator_system_prompt -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>quiz_generator_system_prompt</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M339.07,-289.6C339.07,-289.6 135.22,-289.6 135.22,-289.6 129.22,-289.6 123.22,-283.6 123.22,-277.6 123.22,-277.6 123.22,-238 123.22,-238 123.22,-232 129.22,-226 135.22,-226 135.22,-226 339.07,-226 339.07,-226 345.07,-226 351.07,-232 351.07,-238 351.07,-238 351.07,-277.6 351.07,-277.6 351.07,-283.6 345.07,-289.6 339.07,-289.6\"/>\n",
       "<text text-anchor=\"start\" x=\"134.02\" y=\"-266.5\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">quiz_generator_system_prompt</text>\n",
       "<text text-anchor=\"start\" x=\"229.65\" y=\"-238.5\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- quiz_bank&#45;&gt;quiz_generator_system_prompt -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>quiz_bank&#45;&gt;quiz_generator_system_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M94.27,-257.8C99.8,-257.8 105.66,-257.8 111.74,-257.8\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"111.36,-261.3 121.36,-257.8 111.36,-254.3 111.36,-261.3\"/>\n",
       "</g>\n",
       "<!-- delimiter -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>delimiter</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M77.72,-135.6C77.72,-135.6 23.12,-135.6 23.12,-135.6 17.12,-135.6 11.12,-129.6 11.12,-123.6 11.12,-123.6 11.12,-84 11.12,-84 11.12,-78 17.12,-72 23.12,-72 23.12,-72 77.72,-72 77.72,-72 83.72,-72 89.72,-78 89.72,-84 89.72,-84 89.72,-123.6 89.72,-123.6 89.72,-129.6 83.72,-135.6 77.72,-135.6\"/>\n",
       "<text text-anchor=\"start\" x=\"21.92\" y=\"-112.5\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">delimiter</text>\n",
       "<text text-anchor=\"start\" x=\"42.92\" y=\"-84.5\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- eval_format_system_prompt -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>eval_format_system_prompt</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M328.95,-63.6C328.95,-63.6 145.35,-63.6 145.35,-63.6 139.35,-63.6 133.35,-57.6 133.35,-51.6 133.35,-51.6 133.35,-12 133.35,-12 133.35,-6 139.35,0 145.35,0 145.35,0 328.95,0 328.95,0 334.95,0 340.95,-6 340.95,-12 340.95,-12 340.95,-51.6 340.95,-51.6 340.95,-57.6 334.95,-63.6 328.95,-63.6\"/>\n",
       "<text text-anchor=\"start\" x=\"144.15\" y=\"-40.5\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">eval_format_system_prompt</text>\n",
       "<text text-anchor=\"start\" x=\"229.65\" y=\"-12.5\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- delimiter&#45;&gt;eval_format_system_prompt -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>delimiter&#45;&gt;eval_format_system_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M90,-86.64C100.71,-82.02 112.37,-77.11 123.23,-72.8 127.25,-71.2 131.37,-69.59 135.54,-67.98\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"136.47,-71.37 144.56,-64.54 133.97,-64.83 136.47,-71.37\"/>\n",
       "</g>\n",
       "<!-- delimiter&#45;&gt;quiz_generator_system_prompt -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>delimiter&#45;&gt;quiz_generator_system_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M64.78,-135.92C76.73,-161.09 96.49,-195.34 123.23,-216.8 124.2,-217.58 125.19,-218.35 126.2,-219.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"124.15,-221.95 134.38,-224.68 128.09,-216.16 124.15,-221.95\"/>\n",
       "</g>\n",
       "<!-- eval_format_response -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>eval_format_response</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M924.38,-175.6C924.38,-175.6 782.78,-175.6 782.78,-175.6 776.78,-175.6 770.78,-169.6 770.78,-163.6 770.78,-163.6 770.78,-124 770.78,-124 770.78,-118 776.78,-112 782.78,-112 782.78,-112 924.38,-112 924.38,-112 930.38,-112 936.38,-118 936.38,-124 936.38,-124 936.38,-163.6 936.38,-163.6 936.38,-169.6 930.38,-175.6 924.38,-175.6\"/>\n",
       "<text text-anchor=\"start\" x=\"781.58\" y=\"-152.5\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">eval_format_response</text>\n",
       "<text text-anchor=\"start\" x=\"846.08\" y=\"-124.5\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- eval_format_system_prompt&#45;&gt;eval_format_response -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>eval_format_system_prompt&#45;&gt;eval_format_response</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M453,-113.8C581.05,-125.91 613.59,-122.22 741.77,-132.8 747.41,-133.26 753.2,-133.77 759.05,-134.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"758.53,-137.76 768.81,-135.19 759.17,-130.79 758.53,-137.76\"/>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M327.96,-64.03C335.79,-66.95 343.58,-69.91 351.07,-72.8 395.85,-90.11 403.4,-107.57 451,-113.8\"/>\n",
       "</g>\n",
       "<!-- eval_format_user_prompt -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>eval_format_user_prompt</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M729.77,-205.6C729.77,-205.6 564.92,-205.6 564.92,-205.6 558.92,-205.6 552.92,-199.6 552.92,-193.6 552.92,-193.6 552.92,-154 552.92,-154 552.92,-148 558.92,-142 564.92,-142 564.92,-142 729.77,-142 729.77,-142 735.77,-142 741.77,-148 741.77,-154 741.77,-154 741.77,-193.6 741.77,-193.6 741.77,-199.6 735.77,-205.6 729.77,-205.6\"/>\n",
       "<text text-anchor=\"start\" x=\"563.72\" y=\"-182.5\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">eval_format_user_prompt</text>\n",
       "<text text-anchor=\"start\" x=\"639.85\" y=\"-154.5\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- eval_format_user_prompt&#45;&gt;eval_format_response -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>eval_format_user_prompt&#45;&gt;eval_format_response</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M742.09,-160.03C747.85,-159.18 753.63,-158.33 759.38,-157.49\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"759.49,-161.01 768.87,-156.1 758.47,-154.09 759.49,-161.01\"/>\n",
       "</g>\n",
       "<!-- llm_quiz_response -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>llm_quiz_response</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M511.92,-211.6C511.92,-211.6 392.07,-211.6 392.07,-211.6 386.07,-211.6 380.07,-205.6 380.07,-199.6 380.07,-199.6 380.07,-160 380.07,-160 380.07,-154 386.07,-148 392.07,-148 392.07,-148 511.92,-148 511.92,-148 517.92,-148 523.92,-154 523.92,-160 523.92,-160 523.92,-199.6 523.92,-199.6 523.92,-205.6 517.92,-211.6 511.92,-211.6\"/>\n",
       "<text text-anchor=\"start\" x=\"390.87\" y=\"-188.5\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">llm_quiz_response</text>\n",
       "<text text-anchor=\"start\" x=\"444.5\" y=\"-160.5\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- quiz_generator_system_prompt&#45;&gt;llm_quiz_response -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>quiz_generator_system_prompt&#45;&gt;llm_quiz_response</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M327.24,-225.51C335.3,-222.57 343.34,-219.64 351.07,-216.8 357.03,-214.62 363.18,-212.36 369.37,-210.08\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"370.2,-213.5 378.37,-206.76 367.77,-206.93 370.2,-213.5\"/>\n",
       "</g>\n",
       "<!-- llm_quiz_response&#45;&gt;eval_format_user_prompt -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>llm_quiz_response&#45;&gt;eval_format_user_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M524.17,-177.59C529.82,-177.42 535.58,-177.24 541.39,-177.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"541.19,-180.57 551.07,-176.76 540.97,-173.57 541.19,-180.57\"/>\n",
       "</g>\n",
       "<!-- llm_client -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>llm_client</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M267.45,-145.6C267.45,-145.6 206.85,-145.6 206.85,-145.6 200.85,-145.6 194.85,-139.6 194.85,-133.6 194.85,-133.6 194.85,-94 194.85,-94 194.85,-88 200.85,-82 206.85,-82 206.85,-82 267.45,-82 267.45,-82 273.45,-82 279.45,-88 279.45,-94 279.45,-94 279.45,-133.6 279.45,-133.6 279.45,-139.6 273.45,-145.6 267.45,-145.6\"/>\n",
       "<text text-anchor=\"start\" x=\"205.65\" y=\"-122.5\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">llm_client</text>\n",
       "<text text-anchor=\"start\" x=\"213.9\" y=\"-94.5\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">OpenAI</text>\n",
       "</g>\n",
       "<!-- llm_client&#45;&gt;llm_quiz_response -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>llm_client&#45;&gt;llm_quiz_response</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M279.95,-126.76C305.4,-134.65 338.79,-145 369.3,-154.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"367.92,-157.7 378.51,-157.32 370,-151.02 367.92,-157.7\"/>\n",
       "</g>\n",
       "<!-- llm_client&#45;&gt;eval_format_response -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>llm_client&#45;&gt;eval_format_response</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M279.86,-111.49C322.85,-109.64 391.77,-108.2 451,-113.8\"/>\n",
       "</g>\n",
       "<!-- _llm_quiz_response_inputs -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>_llm_quiz_response_inputs</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"287.95,-208.1 186.35,-208.1 186.35,-163.5 287.95,-163.5 287.95,-208.1\"/>\n",
       "<text text-anchor=\"start\" x=\"201.15\" y=\"-180\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">question</text>\n",
       "<text text-anchor=\"start\" x=\"258.15\" y=\"-180\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- _llm_quiz_response_inputs&#45;&gt;llm_quiz_response -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>_llm_quiz_response_inputs&#45;&gt;llm_quiz_response</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M288.32,-184.39C312.2,-183.71 341.37,-182.89 368.39,-182.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"368.4,-185.63 378.3,-181.85 368.21,-178.63 368.4,-185.63\"/>\n",
       "</g>\n",
       "<!-- input -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>input</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"77.42,-399.1 23.42,-399.1 23.42,-362.5 77.42,-362.5 77.42,-399.1\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.42\" y=\"-375\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">input</text>\n",
       "</g>\n",
       "<!-- function -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>function</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M72.85,-344.1C72.85,-344.1 28,-344.1 28,-344.1 22,-344.1 16,-338.1 16,-332.1 16,-332.1 16,-319.5 16,-319.5 16,-313.5 22,-307.5 28,-307.5 28,-307.5 72.85,-307.5 72.85,-307.5 78.85,-307.5 84.85,-313.5 84.85,-319.5 84.85,-319.5 84.85,-332.1 84.85,-332.1 84.85,-338.1 78.85,-344.1 72.85,-344.1\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.42\" y=\"-320\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">function</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x139677b80>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_format_driver = (\n",
    "        driver.Builder()\n",
    "        .with_modules(quiz, eval_format)\n",
    "        .build()\n",
    ")\n",
    "eval_format_driver.display_all_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62e85827683cd6d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T05:36:43.129333Z",
     "start_time": "2024-02-08T05:36:40.277053Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_format_response': 'Y'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_format_driver.execute(\n",
    "    [\"eval_format_response\"], \n",
    "    inputs={\"question\": \"Write me a quiz about Leonardo DaVinci\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa039ce2ca6775d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# question to ask yourself -- can a regex do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d1050657bc927a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# let's take this to scale ... what happens?\n",
    "\n",
    "You come up with a series of questions and then have responses / logic you want to grade against. \n",
    "```python\n",
    "list_of_test_cases = [{\"question\": \"Write me a quiz about Leonardo DaVinci\",\n",
    " \"response\": \"Question 1:#### <question 1>\\n\\nQuestion 2:#### <question 2>\\n\\nQuestion 3:#### <question 3>\", \n",
    "\"grade\": \"what this must pass or not\"}, ...]\n",
    "```\n",
    "\n",
    "You need to version your prompt eval code as well as your LLM code, else you'll likely find it hard to reproduce results. \"When did this test ever pass?\"\n",
    "\n",
    "# How could you come up with the test cases?\n",
    "- Use LLMs!\n",
    "\n",
    "# Can you use LLMs to suggest prompts? You can do that too...\n",
    "- Use \"generative\" AI for what it's built for ;).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ae09bd06ffd03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T05:36:51.567855Z",
     "start_time": "2024-02-08T05:36:51.291181Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 9.0.0 (20230911.1827)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"733pt\" height=\"283pt\"\n",
       " viewBox=\"0.00 0.00 733.30 283.30\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 279.3)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-279.3 729.3,-279.3 729.3,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster__legend</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"39.38,-137.3 39.38,-267.3 124.22,-267.3 124.22,-137.3 39.38,-137.3\"/>\n",
       "<text text-anchor=\"middle\" x=\"81.8\" y=\"-250\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Legend</text>\n",
       "</g>\n",
       "<!-- eval_relevance_check_system_prompt -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>eval_relevance_check_system_prompt</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M453.45,-208.1C453.45,-208.1 204.6,-208.1 204.6,-208.1 198.6,-208.1 192.6,-202.1 192.6,-196.1 192.6,-196.1 192.6,-156.5 192.6,-156.5 192.6,-150.5 198.6,-144.5 204.6,-144.5 204.6,-144.5 453.45,-144.5 453.45,-144.5 459.45,-144.5 465.45,-150.5 465.45,-156.5 465.45,-156.5 465.45,-196.1 465.45,-196.1 465.45,-202.1 459.45,-208.1 453.45,-208.1\"/>\n",
       "<text text-anchor=\"start\" x=\"203.4\" y=\"-185\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">eval_relevance_check_system_prompt</text>\n",
       "<text text-anchor=\"start\" x=\"321.52\" y=\"-157\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- eval_relevance_check_response -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>eval_relevance_check_response</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M713.3,-126.1C713.3,-126.1 506.45,-126.1 506.45,-126.1 500.45,-126.1 494.45,-120.1 494.45,-114.1 494.45,-114.1 494.45,-74.5 494.45,-74.5 494.45,-68.5 500.45,-62.5 506.45,-62.5 506.45,-62.5 713.3,-62.5 713.3,-62.5 719.3,-62.5 725.3,-68.5 725.3,-74.5 725.3,-74.5 725.3,-114.1 725.3,-114.1 725.3,-120.1 719.3,-126.1 713.3,-126.1\"/>\n",
       "<text text-anchor=\"start\" x=\"505.25\" y=\"-103\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">eval_relevance_check_response</text>\n",
       "<text text-anchor=\"start\" x=\"602.38\" y=\"-75\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- eval_relevance_check_system_prompt&#45;&gt;eval_relevance_check_response -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>eval_relevance_check_system_prompt&#45;&gt;eval_relevance_check_response</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M439.83,-144.01C455.84,-139.3 472.38,-134.44 488.58,-129.67\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"489.11,-133.17 497.71,-126.99 487.13,-126.45 489.11,-133.17\"/>\n",
       "</g>\n",
       "<!-- eval_relevance_check_user_prompt -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>eval_relevance_check_user_prompt</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M444.07,-126.1C444.07,-126.1 213.97,-126.1 213.97,-126.1 207.97,-126.1 201.97,-120.1 201.97,-114.1 201.97,-114.1 201.97,-74.5 201.97,-74.5 201.97,-68.5 207.97,-62.5 213.97,-62.5 213.97,-62.5 444.07,-62.5 444.07,-62.5 450.07,-62.5 456.07,-68.5 456.07,-74.5 456.07,-74.5 456.07,-114.1 456.07,-114.1 456.07,-120.1 450.07,-126.1 444.07,-126.1\"/>\n",
       "<text text-anchor=\"start\" x=\"212.77\" y=\"-103\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">eval_relevance_check_user_prompt</text>\n",
       "<text text-anchor=\"start\" x=\"321.52\" y=\"-75\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- eval_relevance_check_user_prompt&#45;&gt;eval_relevance_check_response -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>eval_relevance_check_user_prompt&#45;&gt;eval_relevance_check_response</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M456.4,-94.3C465.11,-94.3 473.9,-94.3 482.62,-94.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"482.51,-97.8 492.51,-94.3 482.51,-90.8 482.51,-97.8\"/>\n",
       "</g>\n",
       "<!-- _eval_relevance_check_user_prompt_inputs -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>_eval_relevance_check_user_prompt_inputs</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"163.6,-127.1 0,-127.1 0,-61.5 163.6,-61.5 163.6,-127.1\"/>\n",
       "<text text-anchor=\"start\" x=\"14.67\" y=\"-99\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">llm_quiz_response</text>\n",
       "<text text-anchor=\"start\" x=\"133.8\" y=\"-99\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">str</text>\n",
       "<text text-anchor=\"start\" x=\"45.8\" y=\"-78\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">question</text>\n",
       "<text text-anchor=\"start\" x=\"133.8\" y=\"-78\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- _eval_relevance_check_user_prompt_inputs&#45;&gt;eval_relevance_check_user_prompt -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>_eval_relevance_check_user_prompt_inputs&#45;&gt;eval_relevance_check_user_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.78,-94.3C172.31,-94.3 181.15,-94.3 190.1,-94.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"189.97,-97.8 199.97,-94.3 189.97,-90.8 189.97,-97.8\"/>\n",
       "</g>\n",
       "<!-- _eval_relevance_check_response_inputs -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>_eval_relevance_check_response_inputs</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"398.57,-44.6 259.47,-44.6 259.47,0 398.57,0 398.57,-44.6\"/>\n",
       "<text text-anchor=\"start\" x=\"274.27\" y=\"-16.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">llm_client</text>\n",
       "<text text-anchor=\"start\" x=\"337.27\" y=\"-16.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">OpenAI</text>\n",
       "</g>\n",
       "<!-- _eval_relevance_check_response_inputs&#45;&gt;eval_relevance_check_response -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>_eval_relevance_check_response_inputs&#45;&gt;eval_relevance_check_response</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M398.74,-37.28C420.17,-42.15 443.84,-47.74 465.45,-53.3 472.61,-55.14 479.96,-57.08 487.37,-59.08\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"486.21,-62.39 496.78,-61.64 488.05,-55.64 486.21,-62.39\"/>\n",
       "</g>\n",
       "<!-- input -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>input</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"108.8,-236.6 54.8,-236.6 54.8,-200 108.8,-200 108.8,-236.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"81.8\" y=\"-212.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">input</text>\n",
       "</g>\n",
       "<!-- function -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>function</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M104.22,-181.6C104.22,-181.6 59.37,-181.6 59.37,-181.6 53.37,-181.6 47.37,-175.6 47.37,-169.6 47.37,-169.6 47.37,-157 47.37,-157 47.37,-151 53.37,-145 59.37,-145 59.37,-145 104.22,-145 104.22,-145 110.22,-145 116.22,-151 116.22,-157 116.22,-157 116.22,-169.6 116.22,-169.6 116.22,-175.6 110.22,-181.6 104.22,-181.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"81.8\" y=\"-157.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">function</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x13972b880>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%with_functions -m eval_relevance\n",
    "import openai\n",
    "\n",
    "def eval_relevance_check_system_prompt() -> str:\n",
    "    return (\n",
    "        \"You are an assistant that evaluates how well the quiz assistant \"\n",
    "        \"creates quizzes for a user by looking at the generated quize questions for relevance to the original ask.\\n\"\n",
    "        \"Your primary concern is making sure that the quiz is relevant to the original user ask.\\n\"\n",
    "        \"Helpful quizzes are what the user asked for.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def eval_relevance_check_user_prompt(question: str, llm_quiz_response: str) -> str:\n",
    "    return (\n",
    "        \"You are evaluating a generated quiz based on the context that the assistant used to create the quiz.\\n\"\n",
    "        \"Here is the data:\\n\"\n",
    "        \"[BEGIN DATA]\\n\"\n",
    "        \"************\\n\"\n",
    "        f\"[Original Ask]: {question}\\n\"\n",
    "        \"************\\n\"\n",
    "        f\"[Response]: {llm_quiz_response}\\n\"\n",
    "        \"************\\n\"\n",
    "        \"[END DATA]\\n\\n\"\n",
    "        \"\"\"## Steps to make a decision\n",
    "Compare the content of the response with the original ask using the following steps\n",
    "\n",
    "1. Review the original ask. Every question in the quiz should be relevant to the original ask.\n",
    "2. Compare the information in the quiz to the original ask.\n",
    "3. Ignore differences in grammar or punctuation.\n",
    "\n",
    "Remember, the quizzes should only contain questions that provide what as specified in the original ask.\n",
    "\n",
    "Read the response carefully and determine if the quiz satisfies to the original ask. \"\n",
    "For example, ask yourself whether each quiz question satisfies the original ask. If any quiz question does \"\n",
    "not satisfy the original ask, output No.\n",
    "\n",
    "## Additional rules\n",
    "- Output an explanation of whether the quiz satisfies the original ask.\n",
    "- Make the explanation brief only include a summary of your reasoning for the decision.\n",
    "- Include a clear \"Yes\" or \"No\" as the first paragraph.\n",
    "\n",
    "Separate the decision and the explanation. For example:\n",
    "\n",
    "************\n",
    "Decision: <Y>\n",
    "************\n",
    "Explanation: <Explanation>\n",
    "************\n",
    "\"\"\"\n",
    "    ) \n",
    "\n",
    "\n",
    "def eval_relevance_check_response(\n",
    "    eval_relevance_check_system_prompt: str,\n",
    "    eval_relevance_check_user_prompt: str,\n",
    "    llm_client: openai.OpenAI,\n",
    ") -> str:\n",
    "    \"\"\"Creates the RAG response from the LLM model for the given prompt.\n",
    "\n",
    "    :param eval_relevance_check_system_prompt: the prompt to send to the LLM.\n",
    "    :param eval_relevance_check_user_prompt: the prompt to send to the LLM.\n",
    "    :param llm_client: the LLM client to use.\n",
    "    :return: the response from the LLM.\n",
    "    \"\"\"\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": eval_relevance_check_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": eval_relevance_check_user_prompt},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "272bc724602a9bbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T05:36:53.030418Z",
     "start_time": "2024-02-08T05:36:52.700837Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 9.0.0 (20230911.1827)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"642pt\" height=\"283pt\"\n",
       " viewBox=\"0.00 0.00 641.80 283.30\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 279.3)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-279.3 637.8,-279.3 637.8,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster__legend</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"39.38,-137.3 39.38,-267.3 124.22,-267.3 124.22,-137.3 39.38,-137.3\"/>\n",
       "<text text-anchor=\"middle\" x=\"81.8\" y=\"-250\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Legend</text>\n",
       "</g>\n",
       "<!-- eval_factcheck_system_prompt -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>eval_factcheck_system_prompt</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M407.7,-208.1C407.7,-208.1 204.6,-208.1 204.6,-208.1 198.6,-208.1 192.6,-202.1 192.6,-196.1 192.6,-196.1 192.6,-156.5 192.6,-156.5 192.6,-150.5 198.6,-144.5 204.6,-144.5 204.6,-144.5 407.7,-144.5 407.7,-144.5 413.7,-144.5 419.7,-150.5 419.7,-156.5 419.7,-156.5 419.7,-196.1 419.7,-196.1 419.7,-202.1 413.7,-208.1 407.7,-208.1\"/>\n",
       "<text text-anchor=\"start\" x=\"203.4\" y=\"-185\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">eval_factcheck_system_prompt</text>\n",
       "<text text-anchor=\"start\" x=\"298.65\" y=\"-157\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- eval_factcheck_response -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>eval_factcheck_response</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M621.8,-126.1C621.8,-126.1 460.7,-126.1 460.7,-126.1 454.7,-126.1 448.7,-120.1 448.7,-114.1 448.7,-114.1 448.7,-74.5 448.7,-74.5 448.7,-68.5 454.7,-62.5 460.7,-62.5 460.7,-62.5 621.8,-62.5 621.8,-62.5 627.8,-62.5 633.8,-68.5 633.8,-74.5 633.8,-74.5 633.8,-114.1 633.8,-114.1 633.8,-120.1 627.8,-126.1 621.8,-126.1\"/>\n",
       "<text text-anchor=\"start\" x=\"459.5\" y=\"-103\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">eval_factcheck_response</text>\n",
       "<text text-anchor=\"start\" x=\"533.75\" y=\"-75\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- eval_factcheck_system_prompt&#45;&gt;eval_factcheck_response -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>eval_factcheck_system_prompt&#45;&gt;eval_factcheck_response</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M398.94,-144.01C411.68,-139.53 424.84,-134.9 437.76,-130.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"438.84,-133.68 447.11,-127.06 436.52,-127.08 438.84,-133.68\"/>\n",
       "</g>\n",
       "<!-- eval_factcheck_user_prompt -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>eval_factcheck_user_prompt</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M398.32,-126.1C398.32,-126.1 213.97,-126.1 213.97,-126.1 207.97,-126.1 201.97,-120.1 201.97,-114.1 201.97,-114.1 201.97,-74.5 201.97,-74.5 201.97,-68.5 207.97,-62.5 213.97,-62.5 213.97,-62.5 398.32,-62.5 398.32,-62.5 404.32,-62.5 410.32,-68.5 410.32,-74.5 410.32,-74.5 410.32,-114.1 410.32,-114.1 410.32,-120.1 404.32,-126.1 398.32,-126.1\"/>\n",
       "<text text-anchor=\"start\" x=\"212.77\" y=\"-103\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">eval_factcheck_user_prompt</text>\n",
       "<text text-anchor=\"start\" x=\"298.65\" y=\"-75\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- eval_factcheck_user_prompt&#45;&gt;eval_factcheck_response -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>eval_factcheck_user_prompt&#45;&gt;eval_factcheck_response</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M410.75,-94.3C419.46,-94.3 428.26,-94.3 436.97,-94.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"436.82,-97.8 446.82,-94.3 436.82,-90.8 436.82,-97.8\"/>\n",
       "</g>\n",
       "<!-- _eval_factcheck_response_inputs -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>_eval_factcheck_response_inputs</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"375.7,-44.6 236.6,-44.6 236.6,0 375.7,0 375.7,-44.6\"/>\n",
       "<text text-anchor=\"start\" x=\"251.4\" y=\"-16.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">llm_client</text>\n",
       "<text text-anchor=\"start\" x=\"314.4\" y=\"-16.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">OpenAI</text>\n",
       "</g>\n",
       "<!-- _eval_factcheck_response_inputs&#45;&gt;eval_factcheck_response -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>_eval_factcheck_response_inputs&#45;&gt;eval_factcheck_response</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M376.08,-40.59C390.54,-44.62 405.65,-48.98 419.7,-53.3 425.51,-55.09 431.46,-56.96 437.46,-58.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"436.26,-62.18 446.85,-61.93 438.41,-55.52 436.26,-62.18\"/>\n",
       "</g>\n",
       "<!-- _eval_factcheck_user_prompt_inputs -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>_eval_factcheck_user_prompt_inputs</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"163.6,-127.1 0,-127.1 0,-61.5 163.6,-61.5 163.6,-127.1\"/>\n",
       "<text text-anchor=\"start\" x=\"40.55\" y=\"-99\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">quiz_bank</text>\n",
       "<text text-anchor=\"start\" x=\"133.8\" y=\"-99\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">str</text>\n",
       "<text text-anchor=\"start\" x=\"14.67\" y=\"-78\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">llm_quiz_response</text>\n",
       "<text text-anchor=\"start\" x=\"133.8\" y=\"-78\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- _eval_factcheck_user_prompt_inputs&#45;&gt;eval_factcheck_user_prompt -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>_eval_factcheck_user_prompt_inputs&#45;&gt;eval_factcheck_user_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M164.02,-94.3C172.65,-94.3 181.55,-94.3 190.48,-94.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"190.27,-97.8 200.27,-94.3 190.27,-90.8 190.27,-97.8\"/>\n",
       "</g>\n",
       "<!-- input -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>input</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"108.8,-236.6 54.8,-236.6 54.8,-200 108.8,-200 108.8,-236.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"81.8\" y=\"-212.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">input</text>\n",
       "</g>\n",
       "<!-- function -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>function</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M104.22,-181.6C104.22,-181.6 59.37,-181.6 59.37,-181.6 53.37,-181.6 47.37,-175.6 47.37,-169.6 47.37,-169.6 47.37,-157 47.37,-157 47.37,-151 53.37,-145 59.37,-145 59.37,-145 104.22,-145 104.22,-145 110.22,-145 116.22,-151 116.22,-157 116.22,-157 116.22,-169.6 116.22,-169.6 116.22,-175.6 110.22,-181.6 104.22,-181.6\"/>\n",
       "<text text-anchor=\"middle\" x=\"81.8\" y=\"-157.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">function</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1396079d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%with_functions -m eval_factuality\n",
    "import openai\n",
    "\n",
    "def eval_factcheck_system_prompt() -> str:\n",
    "    return (\n",
    "        \"You are an assistant that evaluates how well the quiz assistant \"\n",
    "        \"creates quizzes for a user by looking at the set of facts available to the assistant.\\n\"\n",
    "        \"Your primary concern is making sure that ONLY facts available are used.\\n\"\n",
    "        \"Helpful quizzes only use facts contained in the question bank.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def eval_factcheck_user_prompt(quiz_bank: str, llm_quiz_response: str) -> str:\n",
    "    return f\"\"\"You are evaluating a generated quiz based on the question bank that the assistant uses to create the quiz. Your job is to evaluate the factuality of the quiz and ensure that answers to the quiz questions are found in the question bank.\n",
    "Here is the data:\n",
    "[BEGIN DATA]\n",
    "************\n",
    "[Question Bank]: {quiz_bank}\n",
    "************\n",
    "[Quiz]: {llm_quiz_response}\n",
    "************\n",
    "[END DATA]\n",
    "\n",
    "## The question bank follows the following format:\n",
    "Subject: <subject>\n",
    "   Categories: <category1>, <category2>\n",
    "   Facts:\n",
    "    - <fact 1>\n",
    "    - <fact 2>\n",
    "\n",
    "## Steps to make a decision\n",
    "Compare the quiz provided with the question bank using the following steps\n",
    "\n",
    "1. Review the question bank carefully. These are the only facts the quiz can reference and ask questions about.\n",
    "2. Compare the answer options the quiz asks for to the facts in the question bank. One of the answers should be found in the question bank. \n",
    "3. Ignore differences in grammar or punctuation.\n",
    "\n",
    "Remember, a valid quiz answer should match a fact in the question bank. \n",
    "\n",
    "## Additional rules\n",
    "- Output an explanation of whether each quiz question is asking a question about data in the quiz bank, and that one of the answer options is factually correct and found in the quiz bank.\n",
    "- Make the explanation brief only include a summary of your reasoning for the decision.\n",
    "- Include a clear \"Yes\" or \"No\" as the first paragraph.\n",
    "- Reference the facts in full from the quiz bank if the answer is yes.\n",
    "\n",
    "Separate the decision and the explanation. For example:\n",
    "\n",
    "************\n",
    "Decision: <Y>\n",
    "************\n",
    "Explanation: <Explanation>\n",
    "************\n",
    "\"\"\"\n",
    "\n",
    "def eval_factcheck_response(\n",
    "    eval_factcheck_system_prompt: str, eval_factcheck_user_prompt: str, llm_client: openai.OpenAI\n",
    ") -> str:\n",
    "    \"\"\"Creates the RAG response from the LLM model for the given prompt.\n",
    "\n",
    "    :param eval_factcheck_system_prompt: the prompt to send to the LLM.\n",
    "    :param eval_factcheck_user_prompt: the prompt to send to the LLM.\n",
    "    :param llm_client: the LLM client to use.\n",
    "    :return: the response from the LLM.\n",
    "    \"\"\"\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": eval_factcheck_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": eval_factcheck_user_prompt},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea83c2fdb678f6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T05:36:58.115677Z",
     "start_time": "2024-02-08T05:36:53.908891Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_factcheck_response': '************\\nDecision: Yes\\n************\\nExplanation: \\n\\nQuestion 1: The quiz asks for the famous painting created by Leonardo DaVinci. The correct answer is \"b) The Last Supper,\" which is not found in the question bank. However, the fact \"Painted the Mona Lisa\" is available in the question bank, which is a famous painting created by Leonardo DaVinci. Therefore, the quiz question is asking about data in the question bank, but none of the answer options are factually correct and found in the question bank.\\n\\nQuestion 2: The quiz asks about the other fields that Leonardo DaVinci studied. The correct answer is \"a) Zoology, anatomy, geology, optics,\" which is found in the question bank. The fact \"Studied zoology, anatomy, geology, optics\" is available in the question bank, which matches the correct answer option. Therefore, the quiz question is asking about data in the question bank, and one of the answer options is factually correct and found in the question bank.\\n\\nQuestion 3: The quiz asks about the invention designed by Leonardo DaVinci. The correct answer is \"c) The flying machine,\" which is found in the question bank. The fact \"Designed a flying machine\" is available in the question bank, which matches the correct answer option. Therefore, the quiz question is asking about data in the question bank, and one of the answer options is factually correct and found in the question bank.\\n************'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_factcheck_driver = (\n",
    "        driver.Builder()\n",
    "        .with_modules(eval_factuality)\n",
    "        .build()\n",
    ")\n",
    "eval_factcheck_driver.execute(\n",
    "    [\"eval_factcheck_response\"], \n",
    "    inputs={\n",
    "        \"quiz_bank\": quiz.quiz_bank(),\n",
    "        \"llm_quiz_response\": result[\"llm_quiz_response\"],\n",
    "        \"llm_client\": openai.OpenAI()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "589b520cfa5881fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T05:37:01.960074Z",
     "start_time": "2024-02-08T05:37:01.943547Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question 1:####\\nWhat famous painting did Leonardo DaVinci create?\\na) The Starry Night\\nb) The Last Supper\\nc) The Scream\\nd) Guernica\\n\\nQuestion 2:####\\nIn addition to being an artist, what other fields did Leonardo DaVinci study?\\na) Zoology, anatomy, geology, optics\\nb) Mathematics, astronomy, chemistry\\nc) Literature, philosophy, psychology\\nd) Music, dance, theater\\n\\nQuestion 3:####\\nWhat invention did Leonardo DaVinci design?\\na) The printing press\\nb) The telephone\\nc) The flying machine\\nd) The steam engine'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"llm_quiz_response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20f96f2e237c8eba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T05:37:10.077323Z",
     "start_time": "2024-02-08T05:37:03.439794Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_factcheck_response': '************\\n'\n",
      "                            'Decision: Y\\n'\n",
      "                            '************\\n'\n",
      "                            'Explanation: \\n'\n",
      "                            '\\n'\n",
      "                            'Question 1: Yes, this question is asking about '\n",
      "                            'the largest telescope in space and the material '\n",
      "                            'of its mirror. The answer can be found in the '\n",
      "                            'question bank. The largest telescope in space is '\n",
      "                            'called the James Webb space telescope and its '\n",
      "                            'mirror is made of gold-beryllium.\\n'\n",
      "                            '\\n'\n",
      "                            'Question 2: Yes, this question is asking about '\n",
      "                            'whether water slows down the speed of light. The '\n",
      "                            'answer can be found in the question bank. Water '\n",
      "                            'does slow down the speed of light.\\n'\n",
      "                            '\\n'\n",
      "                            'Question 3: Yes, this question is asking about '\n",
      "                            'what Marie and Pierre Curie discovered in Paris '\n",
      "                            'and where it is displayed today. The answer can '\n",
      "                            'be found in the question bank. Marie and Pierre '\n",
      "                            'Curie discovered Radium and Polonium in Paris, '\n",
      "                            'and Radium is displayed in the Louvre museum in '\n",
      "                            'Paris.\\n'\n",
      "                            '\\n'\n",
      "                            'All the quiz questions are factually correct and '\n",
      "                            'can be answered using information from the '\n",
      "                            'question bank.\\n'\n",
      "                            '************',\n",
      " 'eval_format_response': 'Y',\n",
      " 'eval_relevance_check_response': 'Decision: Yes\\n'\n",
      "                                  '\\n'\n",
      "                                  'Explanation: The quiz contains three '\n",
      "                                  'science questions, which aligns with the '\n",
      "                                  'original ask for a science quiz. Each '\n",
      "                                  'question is relevant to the field of '\n",
      "                                  'science and covers different topics such as '\n",
      "                                  'telescopes, the speed of light, and the '\n",
      "                                  'discoveries of Marie and Pierre Curie. '\n",
      "                                  'Therefore, the quiz satisfies the original '\n",
      "                                  'ask.',\n",
      " 'llm_quiz_response': 'Great! Here are three science questions for you:\\n'\n",
      "                      '\\n'\n",
      "                      'Question 1:####\\n'\n",
      "                      'What is the largest telescope in space called and what '\n",
      "                      'material is its mirror made of?\\n'\n",
      "                      '\\n'\n",
      "                      'Question 2:####\\n'\n",
      "                      'True or False: Water slows down the speed of light.\\n'\n",
      "                      '\\n'\n",
      "                      'Question 3:####\\n'\n",
      "                      'What did Marie and Pierre Curie discover in Paris and '\n",
      "                      'where is it displayed today?'}\n"
     ]
    }
   ],
   "source": [
    "eval_factcheck_driver = (\n",
    "        driver.Builder()\n",
    "        .with_modules(eval_factuality, eval_format, eval_relevance, quiz)\n",
    "        .build()\n",
    ")\n",
    "eval_response_1 = eval_factcheck_driver.execute(\n",
    "    [\"llm_quiz_response\", \"eval_factcheck_response\", \"eval_relevance_check_response\", \"eval_format_response\"], \n",
    "    inputs={\n",
    "        \"question\": \"I would like a science quiz please.\",\n",
    "    }\n",
    ")\n",
    "pprint.pprint(eval_response_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3459379d564f29a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-08T05:37:15.858953Z",
     "start_time": "2024-02-08T05:37:10.079724Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_factcheck_response': '************\\n'\n",
      "                            'Decision: Yes\\n'\n",
      "                            '************\\n'\n",
      "                            'Explanation: \\n'\n",
      "                            '\\n'\n",
      "                            'Question 1: The quiz asks for the famous painting '\n",
      "                            'created by Leonardo DaVinci. The fact \"Painted '\n",
      "                            'the Mona Lisa\" from the question bank matches one '\n",
      "                            'of the answer options, so the quiz question is '\n",
      "                            'factually correct and found in the question '\n",
      "                            'bank.\\n'\n",
      "                            '\\n'\n",
      "                            'Question 2: The quiz asks about the other fields '\n",
      "                            'Leonardo DaVinci studied. The fact \"Studied '\n",
      "                            'zoology, anatomy, geology, optics\" from the '\n",
      "                            'question bank matches one of the answer options, '\n",
      "                            'so the quiz question is factually correct and '\n",
      "                            'found in the question bank.\\n'\n",
      "                            '\\n'\n",
      "                            'Question 3: The quiz asks about the invention '\n",
      "                            'designed by Leonardo DaVinci. The fact \"Designed '\n",
      "                            'a flying machine\" from the question bank matches '\n",
      "                            'one of the answer options, so the quiz question '\n",
      "                            'is factually correct and found in the question '\n",
      "                            'bank.\\n'\n",
      "                            '************',\n",
      " 'eval_format_response': 'Y',\n",
      " 'eval_relevance_check_response': '************\\n'\n",
      "                                  'Decision: Yes\\n'\n",
      "                                  '************\\n'\n",
      "                                  'Explanation: The generated quiz contains '\n",
      "                                  'three questions that are all relevant to '\n",
      "                                  'the original ask about Leonardo DaVinci. '\n",
      "                                  'The questions ask about his famous '\n",
      "                                  'paintings, his fields of study, and his '\n",
      "                                  'inventions, which are all key aspects of '\n",
      "                                  \"Leonardo DaVinci's life and work. \"\n",
      "                                  'Therefore, the quiz satisfies the original '\n",
      "                                  'ask.',\n",
      " 'llm_quiz_response': 'Question 1:####\\n'\n",
      "                      'What famous painting did Leonardo DaVinci create?\\n'\n",
      "                      '\\n'\n",
      "                      'Question 2:####\\n'\n",
      "                      'In addition to being an artist, what other fields did '\n",
      "                      'Leonardo DaVinci study?\\n'\n",
      "                      '\\n'\n",
      "                      'Question 3:####\\n'\n",
      "                      'What invention did Leonardo DaVinci design?'}\n"
     ]
    }
   ],
   "source": [
    "eval_response_2 = eval_factcheck_driver.execute(\n",
    "    [\"llm_quiz_response\", \"eval_factcheck_response\", \"eval_relevance_check_response\", \"eval_format_response\"], \n",
    "    inputs={\n",
    "        \"question\": \"I would like a quiz about Leonardo DaVinci please.\",\n",
    "    }\n",
    ")\n",
    "pprint.pprint(eval_response_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d8c6b9929f0b77",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
