{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\r\n",
      "  Using cached openai-0.27.8-py3-none-any.whl (73 kB)\r\n",
      "Collecting PyPDF2\r\n",
      "  Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\r\n",
      "Collecting python-multipart\r\n",
      "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m45.7/45.7 KB\u001B[0m \u001B[31m1.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: sf-hamilton[visualization] in /Users/stefankrawczyk/.pyenv/versions/3.10.4/envs/hamilton-tutorials-py310/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (1.25.0)\r\n",
      "Collecting tenacity\r\n",
      "  Using cached tenacity-8.2.2-py3-none-any.whl (24 kB)\r\n",
      "Collecting tiktoken\r\n",
      "  Downloading tiktoken-0.4.0-cp310-cp310-macosx_10_9_x86_64.whl (797 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m798.0/798.0 KB\u001B[0m \u001B[31m25.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting tqdm\r\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\r\n",
      "Collecting dagworks-sdk\r\n",
      "  Downloading dagworks_sdk-0.0.12-py3-none-any.whl (335 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m335.6/335.6 KB\u001B[0m \u001B[31m12.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting aiohttp\r\n",
      "  Downloading aiohttp-3.8.5-cp310-cp310-macosx_10_9_x86_64.whl (365 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m365.8/365.8 KB\u001B[0m \u001B[31m13.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: requests>=2.20 in /Users/stefankrawczyk/.pyenv/versions/3.10.4/envs/hamilton-tutorials-py310/lib/python3.10/site-packages (from openai->-r requirements.txt (line 1)) (2.31.0)\r\n",
      "Requirement already satisfied: typing-extensions>4.0.0 in /Users/stefankrawczyk/.pyenv/versions/3.10.4/envs/hamilton-tutorials-py310/lib/python3.10/site-packages (from sf-hamilton[visualization]->-r requirements.txt (line 4)) (4.7.1)\r\n",
      "Requirement already satisfied: pandas in /Users/stefankrawczyk/.pyenv/versions/3.10.4/envs/hamilton-tutorials-py310/lib/python3.10/site-packages (from sf-hamilton[visualization]->-r requirements.txt (line 4)) (2.0.3)\r\n",
      "Requirement already satisfied: numpy in /Users/stefankrawczyk/.pyenv/versions/3.10.4/envs/hamilton-tutorials-py310/lib/python3.10/site-packages (from sf-hamilton[visualization]->-r requirements.txt (line 4)) (1.25.2)\r\n",
      "Requirement already satisfied: typing-inspect in /Users/stefankrawczyk/.pyenv/versions/3.10.4/envs/hamilton-tutorials-py310/lib/python3.10/site-packages (from sf-hamilton[visualization]->-r requirements.txt (line 4)) (0.9.0)\r\n",
      "Requirement already satisfied: networkx in /Users/stefankrawczyk/.pyenv/versions/3.10.4/envs/hamilton-tutorials-py310/lib/python3.10/site-packages (from sf-hamilton[visualization]->-r requirements.txt (line 4)) (3.1)\r\n",
      "Requirement already satisfied: graphviz in /Users/stefankrawczyk/.pyenv/versions/3.10.4/envs/hamilton-tutorials-py310/lib/python3.10/site-packages (from sf-hamilton[visualization]->-r requirements.txt (line 4)) (0.20.1)\r\n",
      "Collecting regex>=2022.1.18\r\n",
      "  Using cached regex-2023.6.3-cp310-cp310-macosx_10_9_x86_64.whl (294 kB)\r\n",
      "Requirement already satisfied: packaging in /Users/stefankrawczyk/.pyenv/versions/3.10.4/envs/hamilton-tutorials-py310/lib/python3.10/site-packages (from dagworks-sdk->-r requirements.txt (line 8)) (23.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/stefankrawczyk/.pyenv/versions/3.10.4/envs/hamilton-tutorials-py310/lib/python3.10/site-packages (from dagworks-sdk->-r requirements.txt (line 8)) (3.1.2)\r\n",
      "Collecting httpx>=0.15.4\r\n",
      "  Using cached httpx-0.24.1-py3-none-any.whl (75 kB)\r\n",
      "Collecting click\r\n",
      "  Using cached click-8.1.6-py3-none-any.whl (97 kB)\r\n",
      "Requirement already satisfied: attrs>=21.3.0 in /Users/stefankrawczyk/.pyenv/versions/3.10.4/envs/hamilton-tutorials-py310/lib/python3.10/site-packages (from dagworks-sdk->-r requirements.txt (line 8)) (23.1.0)\r\n",
      "Requirement already satisfied: python-dateutil>2.8.0 in /Users/stefankrawczyk/.pyenv/versions/3.10.4/envs/hamilton-tutorials-py310/lib/python3.10/site-packages (from dagworks-sdk->-r requirements.txt (line 8)) (2.8.2)\r\n",
      "Collecting posthog\r\n",
      "  Using cached posthog-3.0.1-py2.py3-none-any.whl (37 kB)\r\n",
      "Collecting loguru\r\n",
      "  Using cached loguru-0.7.0-py3-none-any.whl (59 kB)\r\n",
      "Collecting pyarrow\r\n",
      "  Using cached pyarrow-12.0.1-cp310-cp310-macosx_10_14_x86_64.whl (24.7 MB)\r\n",
      "Collecting gitpython\r\n",
      "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m188.5/188.5 KB\u001B[0m \u001B[31m7.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: idna in /Users/stefankrawczyk/.pyenv/versions/3.10.4/envs/hamilton-tutorials-py310/lib/python3.10/site-packages (from httpx>=0.15.4->dagworks-sdk->-r requirements.txt (line 8)) (3.4)\r\n",
      "Requirement already satisfied: certifi in /Users/stefankrawczyk/.pyenv/versions/3.10.4/envs/hamilton-tutorials-py310/lib/python3.10/site-packages (from httpx>=0.15.4->dagworks-sdk->-r requirements.txt (line 8)) (2023.7.22)\r\n",
      "Requirement already satisfied: sniffio in /Users/stefankrawczyk/.pyenv/versions/3.10.4/envs/hamilton-tutorials-py310/lib/python3.10/site-packages (from httpx>=0.15.4->dagworks-sdk->-r requirements.txt (line 8)) (1.3.0)\r\n",
      "Collecting httpcore<0.18.0,>=0.15.0\r\n",
      "  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m74.5/74.5 KB\u001B[0m \u001B[31m2.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: six>=1.5 in /Users/stefankrawczyk/.pyenv/versions/3.10.4/envs/hamilton-tutorials-py310/lib/python3.10/site-packages (from python-dateutil>2.8.0->dagworks-sdk->-r requirements.txt (line 8)) (1.16.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/stefankrawczyk/.pyenv/versions/3.10.4/envs/hamilton-tutorials-py310/lib/python3.10/site-packages (from requests>=2.20->openai->-r requirements.txt (line 1)) (3.2.0)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/stefankrawczyk/.pyenv/versions/3.10.4/envs/hamilton-tutorials-py310/lib/python3.10/site-packages (from requests>=2.20->openai->-r requirements.txt (line 1)) (2.0.4)\r\n",
      "Collecting yarl<2.0,>=1.0\r\n",
      "  Using cached yarl-1.9.2-cp310-cp310-macosx_10_9_x86_64.whl (65 kB)\r\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\r\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\r\n",
      "Collecting aiosignal>=1.1.2\r\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\r\n",
      "Collecting multidict<7.0,>=4.5\r\n",
      "  Using cached multidict-6.0.4-cp310-cp310-macosx_10_9_x86_64.whl (29 kB)\r\n",
      "Collecting frozenlist>=1.1.1\r\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-macosx_10_9_x86_64.whl (46 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m46.9/46.9 KB\u001B[0m \u001B[31m1.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting gitdb<5,>=4.0.1\r\n",
      "  Using cached gitdb-4.0.10-py3-none-any.whl (62 kB)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/stefankrawczyk/.pyenv/versions/3.10.4/envs/hamilton-tutorials-py310/lib/python3.10/site-packages (from jinja2->dagworks-sdk->-r requirements.txt (line 8)) (2.1.3)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/stefankrawczyk/.pyenv/versions/3.10.4/envs/hamilton-tutorials-py310/lib/python3.10/site-packages (from pandas->sf-hamilton[visualization]->-r requirements.txt (line 4)) (2023.3)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/stefankrawczyk/.pyenv/versions/3.10.4/envs/hamilton-tutorials-py310/lib/python3.10/site-packages (from pandas->sf-hamilton[visualization]->-r requirements.txt (line 4)) (2023.3)\r\n",
      "Collecting backoff>=1.10.0\r\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\r\n",
      "Collecting monotonic>=1.5\r\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\r\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/stefankrawczyk/.pyenv/versions/3.10.4/envs/hamilton-tutorials-py310/lib/python3.10/site-packages (from typing-inspect->sf-hamilton[visualization]->-r requirements.txt (line 4)) (1.0.0)\r\n",
      "Collecting smmap<6,>=3.0.1\r\n",
      "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\r\n",
      "Collecting h11<0.15,>=0.13\r\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\r\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /Users/stefankrawczyk/.pyenv/versions/3.10.4/envs/hamilton-tutorials-py310/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx>=0.15.4->dagworks-sdk->-r requirements.txt (line 8)) (3.7.1)\r\n",
      "Requirement already satisfied: exceptiongroup in /Users/stefankrawczyk/.pyenv/versions/3.10.4/envs/hamilton-tutorials-py310/lib/python3.10/site-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx>=0.15.4->dagworks-sdk->-r requirements.txt (line 8)) (1.1.2)\r\n",
      "Installing collected packages: monotonic, tqdm, tenacity, smmap, regex, python-multipart, PyPDF2, pyarrow, multidict, loguru, h11, frozenlist, click, backoff, async-timeout, yarl, tiktoken, posthog, httpcore, gitdb, aiosignal, httpx, gitpython, aiohttp, openai, dagworks-sdk\r\n",
      "Successfully installed PyPDF2-3.0.1 aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.2 backoff-2.2.1 click-8.1.6 dagworks-sdk-0.0.12 frozenlist-1.4.0 gitdb-4.0.10 gitpython-3.1.32 h11-0.14.0 httpcore-0.17.3 httpx-0.24.1 loguru-0.7.0 monotonic-1.6 multidict-6.0.4 openai-0.27.8 posthog-3.0.1 pyarrow-12.0.1 python-multipart-0.0.6 regex-2023.6.3 smmap-5.0.0 tenacity-8.2.2 tiktoken-0.4.0 tqdm-4.65.0 yarl-1.9.2\r\n",
      "\u001B[33mWARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\r\n",
      "You should consider upgrading via the '/Users/stefankrawczyk/.pyenv/versions/3.10.4/envs/hamilton-tutorials-py310/bin/python3.10 -m pip install --upgrade pip' command.\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T20:46:23.504330Z",
     "start_time": "2023-08-07T20:46:13.024557Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting summarization.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile summarization.py\n",
    "\n",
    "import io\n",
    "import concurrent\n",
    "from typing import Generator\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "from PyPDF2 import PdfReader\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hamilton.function_modifiers import config\n",
    "\n",
    "\n",
    "def summarize_chunk_of_text_prompt(content_type: str = \"an academic paper\") -> str:\n",
    "    \"\"\"Base prompt for summarizing chunks of text.\"\"\"\n",
    "    return f\"Summarize this text from {content_type}. Extract any key points with reasoning.\\n\\nContent:\"\n",
    "\n",
    "\n",
    "def summarize_text_from_summaries_prompt(content_type: str = \"an academic paper\") -> str:\n",
    "    \"\"\"Prompt for summarizing a paper from a list of summaries.\"\"\"\n",
    "    return f\"\"\"Write a summary collated from this collection of key points extracted from {content_type}.\n",
    "    The summary should highlight the core argument, conclusions and evidence, and answer the user's query.\n",
    "    User query: {{query}}\n",
    "    The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\n",
    "    Key points:\\n{{results}}\\nSummary:\\n\"\"\"\n",
    "\n",
    "\n",
    "@config.when(file_type=\"pdf\")\n",
    "def raw_text(pdf_source: io.BufferedReader) -> str:\n",
    "    \"\"\"Takes a filepath to a PDF and returns a string of the PDF's contents\n",
    "    :param pdf_source: Series of filepaths to PDFs\n",
    "    :return: Series of strings of the PDFs' contents\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_source)\n",
    "    _pdf_text = \"\"\n",
    "    page_number = 0\n",
    "    for page in reader.pages:\n",
    "        page_number += 1\n",
    "        _pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n",
    "    return _pdf_text\n",
    "\n",
    "\n",
    "def _create_chunks(text: str, n: int, tokenizer: tiktoken.Encoding) -> Generator[str, None, None]:\n",
    "    \"\"\"Helper function. Returns successive n-sized chunks from provided text.\n",
    "    Split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
    "    :param text:\n",
    "    :param n:\n",
    "    :param tokenizer:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
    "        j = min(i + int(1.5 * n), len(tokens))\n",
    "        while j > i + int(0.5 * n):\n",
    "            # Decode the tokens and check for full stop or newline\n",
    "            chunk = tokenizer.decode(tokens[i:j])\n",
    "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
    "                break\n",
    "            j -= 1\n",
    "        # If no end of sentence found, use n tokens as the chunk size\n",
    "        if j == i + int(0.5 * n):\n",
    "            j = min(i + n, len(tokens))\n",
    "        yield tokens[i:j]\n",
    "        i = j\n",
    "\n",
    "\n",
    "def chunked_text(\n",
    "    raw_text: str, max_token_length: int = 1500, tokenizer_encoding: str = \"cl100k_base\"\n",
    ") -> list[str]:\n",
    "    \"\"\"Chunks the pdf text into smaller chunks of size max_token_length.\n",
    "    :param pdf_text: the Series of individual pdf texts to chunk.\n",
    "    :param max_token_length: the maximum length of tokens in each chunk.\n",
    "    :param tokenizer_encoding: the encoding to use for the tokenizer.\n",
    "    :return: Series of chunked pdf text. Each element is a list of chunks.\n",
    "    \"\"\"\n",
    "    tokenizer = tiktoken.get_encoding(tokenizer_encoding)\n",
    "    _encoded_chunks = _create_chunks(raw_text, max_token_length, tokenizer)\n",
    "    _decoded_chunks = [tokenizer.decode(chunk) for chunk in _encoded_chunks]\n",
    "    return _decoded_chunks\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def _summarize_chunk(content: str, template_prompt: str, openai_gpt_model: str) -> str:\n",
    "    \"\"\"This function applies a prompt to some input content. In this case it returns a summarized chunk of text.\n",
    "    :param content: the content to summarize.\n",
    "    :param template_prompt: the prompt template to use to put the content into.\n",
    "    :param openai_gpt_model: the openai gpt model to use.\n",
    "    :return: the response from the openai API.\n",
    "    \"\"\"\n",
    "    prompt = template_prompt + content\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=openai_gpt_model, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def summarized_chunks(\n",
    "    chunked_text: list[str], summarize_chunk_of_text_prompt: str, openai_gpt_model: str\n",
    ") -> str:\n",
    "    \"\"\"Summarizes a series of chunks of text.\n",
    "    Note: this takes the first result from the top_n_related_articles series and summarizes it. This is because\n",
    "    the top_n_related_articles series is sorted by relatedness, so the first result is the most related.\n",
    "    :param top_n_related_articles: series with each entry being a list of chunks of text for an article.\n",
    "    :param summarize_chunk_of_text_prompt:  the prompt to use to summarize each chunk of text.\n",
    "    :param openai_gpt_model: the openai gpt model to use.\n",
    "    :return: a single string of each chunk of text summarized, concatenated together.\n",
    "    \"\"\"\n",
    "    _summarized_text = \"\"\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=len(chunked_text)) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                _summarize_chunk, chunk, summarize_chunk_of_text_prompt, openai_gpt_model\n",
    "            )\n",
    "            for chunk in chunked_text\n",
    "        ]\n",
    "        with tqdm(total=len(chunked_text)) as pbar:\n",
    "            for _ in concurrent.futures.as_completed(futures):\n",
    "                pbar.update(1)\n",
    "        for future in futures:\n",
    "            data = future.result()\n",
    "            _summarized_text += data\n",
    "    return _summarized_text\n",
    "\n",
    "\n",
    "def prompt_and_text_content(\n",
    "    summarize_text_from_summaries_prompt: str, user_query: str, summarized_chunks: str\n",
    ") -> str:\n",
    "    \"\"\"Creates the prompt for summarizing the text from the summarized chunks of the pdf.\n",
    "    :param summarize_text_from_summaries_prompt: the template to use to summarize the chunks.\n",
    "    :param user_query: the original user query.\n",
    "    :param summarized_chunks: a long string of chunked summaries of a file.\n",
    "    :return: the prompt to use to summarize the chunks.\n",
    "    \"\"\"\n",
    "    return summarize_text_from_summaries_prompt.format(query=user_query, results=summarized_chunks)\n",
    "\n",
    "\n",
    "def summarized_text(\n",
    "    prompt_and_text_content: str,\n",
    "    openai_gpt_model: str,\n",
    ") -> str:\n",
    "    \"\"\"Summarizes the text from the summarized chunks of the pdf.\n",
    "    :param prompt_and_text_content: the prompt and content to send over.\n",
    "    :param openai_gpt_model: which openai gpt model to use.\n",
    "    :return: the string response from the openai API.\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=openai_gpt_model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt_and_text_content,\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T21:00:12.871059Z",
     "start_time": "2023-08-07T21:00:12.866449Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 8.0.5 (20230430.1635)\n -->\n<!-- Pages: 1 -->\n<svg width=\"915pt\" height=\"404pt\"\n viewBox=\"0.00 0.00 915.01 404.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 400)\">\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-400 911.01,-400 911.01,4 -4,4\"/>\n<!-- summarize_text_from_summaries_prompt -->\n<g id=\"node1\" class=\"node\">\n<title>summarize_text_from_summaries_prompt</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"564.58\" cy=\"-162\" rx=\"171.11\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"564.58\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarize_text_from_summaries_prompt</text>\n</g>\n<!-- prompt_and_text_content -->\n<g id=\"node12\" class=\"node\">\n<title>prompt_and_text_content</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"401.58\" cy=\"-90\" rx=\"106.11\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"401.58\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">prompt_and_text_content</text>\n</g>\n<!-- summarize_text_from_summaries_prompt&#45;&gt;prompt_and_text_content -->\n<g id=\"edge12\" class=\"edge\">\n<title>summarize_text_from_summaries_prompt&#45;&gt;prompt_and_text_content</title>\n<path fill=\"none\" stroke=\"black\" d=\"M525.12,-144.05C502.29,-134.25 473.38,-121.84 449.28,-111.48\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"450.84,-107.92 440.27,-107.19 448.08,-114.35 450.84,-107.92\"/>\n</g>\n<!-- pdf_source -->\n<g id=\"node2\" class=\"node\">\n<title>pdf_source</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"287.58\" cy=\"-378\" rx=\"76.43\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"287.58\" y=\"-372.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: pdf_source</text>\n</g>\n<!-- raw_text -->\n<g id=\"node8\" class=\"node\">\n<title>raw_text</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"287.58\" cy=\"-306\" rx=\"43.16\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"287.58\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">raw_text</text>\n</g>\n<!-- pdf_source&#45;&gt;raw_text -->\n<g id=\"edge6\" class=\"edge\">\n<title>pdf_source&#45;&gt;raw_text</title>\n<path fill=\"none\" stroke=\"black\" d=\"M287.58,-359.7C287.58,-352.24 287.58,-343.32 287.58,-334.97\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"291.08,-335.1 287.58,-325.1 284.08,-335.1 291.08,-335.1\"/>\n</g>\n<!-- tokenizer_encoding -->\n<g id=\"node3\" class=\"node\">\n<title>tokenizer_encoding</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"118.58\" cy=\"-306\" rx=\"108.16\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"118.58\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: tokenizer_encoding</text>\n</g>\n<!-- chunked_text -->\n<g id=\"node6\" class=\"node\">\n<title>chunked_text</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"287.58\" cy=\"-234\" rx=\"60.56\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"287.58\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">chunked_text</text>\n</g>\n<!-- tokenizer_encoding&#45;&gt;chunked_text -->\n<g id=\"edge4\" class=\"edge\">\n<title>tokenizer_encoding&#45;&gt;chunked_text</title>\n<path fill=\"none\" stroke=\"black\" d=\"M157.78,-288.76C183.26,-278.21 216.54,-264.42 243.02,-253.46\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"244.28,-256.31 252.18,-249.25 241.6,-249.84 244.28,-256.31\"/>\n</g>\n<!-- content_type -->\n<g id=\"node4\" class=\"node\">\n<title>content_type</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"669.58\" cy=\"-306\" rx=\"83.08\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"669.58\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: content_type</text>\n</g>\n<!-- content_type&#45;&gt;summarize_text_from_summaries_prompt -->\n<g id=\"edge1\" class=\"edge\">\n<title>content_type&#45;&gt;summarize_text_from_summaries_prompt</title>\n<path fill=\"none\" stroke=\"black\" d=\"M672.04,-287.7C673.83,-268.53 673.87,-237.49 659.58,-216 650.74,-202.71 637.39,-192.42 623.46,-184.6\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"625.27,-181.07 614.78,-179.57 622.04,-187.28 625.27,-181.07\"/>\n</g>\n<!-- summarize_chunk_of_text_prompt -->\n<g id=\"node7\" class=\"node\">\n<title>summarize_chunk_of_text_prompt</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"508.58\" cy=\"-234\" rx=\"141.94\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"508.58\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarize_chunk_of_text_prompt</text>\n</g>\n<!-- content_type&#45;&gt;summarize_chunk_of_text_prompt -->\n<g id=\"edge5\" class=\"edge\">\n<title>content_type&#45;&gt;summarize_chunk_of_text_prompt</title>\n<path fill=\"none\" stroke=\"black\" d=\"M633.85,-289.46C611.21,-279.62 581.65,-266.77 556.93,-256.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"558.59,-252.49 548.02,-251.71 555.8,-258.91 558.59,-252.49\"/>\n</g>\n<!-- file_type -->\n<g id=\"node5\" class=\"node\">\n<title>file_type</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"450.58\" cy=\"-378\" rx=\"68.24\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"450.58\" y=\"-372.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: file_type</text>\n</g>\n<!-- summarized_chunks -->\n<g id=\"node11\" class=\"node\">\n<title>summarized_chunks</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"287.58\" cy=\"-162\" rx=\"87.69\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"287.58\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarized_chunks</text>\n</g>\n<!-- chunked_text&#45;&gt;summarized_chunks -->\n<g id=\"edge9\" class=\"edge\">\n<title>chunked_text&#45;&gt;summarized_chunks</title>\n<path fill=\"none\" stroke=\"black\" d=\"M287.58,-215.7C287.58,-208.24 287.58,-199.32 287.58,-190.97\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"291.08,-191.1 287.58,-181.1 284.08,-191.1 291.08,-191.1\"/>\n</g>\n<!-- summarize_chunk_of_text_prompt&#45;&gt;summarized_chunks -->\n<g id=\"edge10\" class=\"edge\">\n<title>summarize_chunk_of_text_prompt&#45;&gt;summarized_chunks</title>\n<path fill=\"none\" stroke=\"black\" d=\"M457.32,-216.76C423.43,-206.03 378.99,-191.95 344.08,-180.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"345.63,-177.4 335.04,-177.72 343.52,-184.07 345.63,-177.4\"/>\n</g>\n<!-- raw_text&#45;&gt;chunked_text -->\n<g id=\"edge2\" class=\"edge\">\n<title>raw_text&#45;&gt;chunked_text</title>\n<path fill=\"none\" stroke=\"black\" d=\"M287.58,-287.7C287.58,-280.24 287.58,-271.32 287.58,-262.97\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"291.08,-263.1 287.58,-253.1 284.08,-263.1 291.08,-263.1\"/>\n</g>\n<!-- summarized_text -->\n<g id=\"node9\" class=\"node\">\n<title>summarized_text</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"205.58\" cy=\"-18\" rx=\"75.41\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"205.58\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarized_text</text>\n</g>\n<!-- max_token_length -->\n<g id=\"node10\" class=\"node\">\n<title>max_token_length</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"453.58\" cy=\"-306\" rx=\"104.58\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"453.58\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: max_token_length</text>\n</g>\n<!-- max_token_length&#45;&gt;chunked_text -->\n<g id=\"edge3\" class=\"edge\">\n<title>max_token_length&#45;&gt;chunked_text</title>\n<path fill=\"none\" stroke=\"black\" d=\"M415.5,-288.94C390.48,-278.39 357.68,-264.56 331.56,-253.55\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"333.12,-249.98 322.54,-249.32 330.4,-256.43 333.12,-249.98\"/>\n</g>\n<!-- summarized_chunks&#45;&gt;prompt_and_text_content -->\n<g id=\"edge14\" class=\"edge\">\n<title>summarized_chunks&#45;&gt;prompt_and_text_content</title>\n<path fill=\"none\" stroke=\"black\" d=\"M314.6,-144.41C329.68,-135.15 348.67,-123.49 365.06,-113.42\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"366.67,-115.93 373.36,-107.71 363.01,-109.96 366.67,-115.93\"/>\n</g>\n<!-- prompt_and_text_content&#45;&gt;summarized_text -->\n<g id=\"edge7\" class=\"edge\">\n<title>prompt_and_text_content&#45;&gt;summarized_text</title>\n<path fill=\"none\" stroke=\"black\" d=\"M357.59,-73.29C327.68,-62.61 288,-48.44 256.68,-37.25\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"258,-33.65 247.41,-33.58 255.65,-40.24 258,-33.65\"/>\n</g>\n<!-- user_query -->\n<g id=\"node13\" class=\"node\">\n<title>user_query</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"830.58\" cy=\"-162\" rx=\"76.43\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"830.58\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: user_query</text>\n</g>\n<!-- user_query&#45;&gt;prompt_and_text_content -->\n<g id=\"edge13\" class=\"edge\">\n<title>user_query&#45;&gt;prompt_and_text_content</title>\n<path fill=\"none\" stroke=\"black\" d=\"M774.21,-149.51C764.34,-147.6 754.17,-145.69 744.58,-144 659.1,-128.95 561.22,-114.06 491.96,-103.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"492.49,-100.31 482.09,-102.32 491.48,-107.23 492.49,-100.31\"/>\n</g>\n<!-- openai_gpt_model -->\n<g id=\"node14\" class=\"node\">\n<title>openai_gpt_model</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"104.58\" cy=\"-234\" rx=\"104.58\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"104.58\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: openai_gpt_model</text>\n</g>\n<!-- openai_gpt_model&#45;&gt;summarized_text -->\n<g id=\"edge8\" class=\"edge\">\n<title>openai_gpt_model&#45;&gt;summarized_text</title>\n<path fill=\"none\" stroke=\"black\" d=\"M112.67,-215.85C130.18,-178.75 171.69,-90.81 192.79,-46.1\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"196.34,-47.77 197.45,-37.23 190.01,-44.78 196.34,-47.77\"/>\n</g>\n<!-- openai_gpt_model&#45;&gt;summarized_chunks -->\n<g id=\"edge11\" class=\"edge\">\n<title>openai_gpt_model&#45;&gt;summarized_chunks</title>\n<path fill=\"none\" stroke=\"black\" d=\"M146.1,-217.12C173.15,-206.77 208.61,-193.21 237.26,-182.25\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"238.39,-185.18 246.48,-178.34 235.89,-178.64 238.39,-185.18\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": "<graphviz.graphs.Digraph at 0x12c189450>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # run as a script to test Hamilton's execution\n",
    "import summarization\n",
    "import importlib\n",
    "importlib.reload(summarization)\n",
    "\n",
    "from hamilton import base, driver\n",
    "\n",
    "dr = driver.Driver(\n",
    "    {\"file_type\": \"pdf\"},\n",
    "    summarization,\n",
    "    adapter=base.SimplePythonGraphAdapter(base.DictResult()),\n",
    ")\n",
    "dr.display_all_functions(None)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T21:00:13.895749Z",
     "start_time": "2023-08-07T21:00:13.601726Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# pull in a pdf\n",
    "import requests\n",
    "\n",
    "def download_file(url, filename):\n",
    "  response = requests.get(url, stream=True)\n",
    "  if response.status_code == 200:\n",
    "    with open(filename, 'wb') as fd:\n",
    "      for chunk in response.iter_content(chunk_size=1024):\n",
    "        fd.write(chunk)\n",
    "\n",
    "download_file(\"https://cdmsworkshop.github.io/2022/Proceedings/ShortPapers/Paper6_StefanKrawczyk.pdf\", \"hamilton_paper.pdf\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T21:00:15.087385Z",
     "start_time": "2023-08-07T21:00:15.021201Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core Argument:\n",
      "- The Hamilton framework is a high-level modeling approach for dataflows that simplifies the user experience for data scientists and provides a unified interface for describing end-to-end dataflows.\n",
      "- Traditional ETL approaches at Stitch Fix had problems such as poorly documented code, low unit test coverage, limited code reuse, and difficulty in changing underlying infrastructure, which led to the development of the Hamilton framework.\n",
      "\n",
      "Evidence:\n",
      "- The Hamilton framework has been used to scale modeling dataflows at Stitch Fix to support over 4000 data transformations without impacting team and user productivity.\n",
      "- The Hamilton programming paradigm encourages the use of vector computation, improves code readability and documentation, and allows for easy unit testing.\n",
      "- Hamilton provides decorators to encapsulate operational concerns and reduce repetitive function logic.\n",
      "- The function DAG is the framework's representation of the nodes that should be executed and the dependencies between them.\n",
      "- The driver code in Hamilton steers the execution of the function DAG and provides a convenient abstraction layer for users.\n",
      "\n",
      "Conclusions:\n",
      "- The Hamilton framework provides a simpler user experience for data scientists and allows for easy integration with existing data management tooling in a modular fashion.\n",
      "- Hamilton improves code readability, modularity, and reusability in data management systems.\n",
      "- Hamilton offers benefits such as incremental development, debugging capabilities, transparent scaling, lineage tracking, and modular components.\n",
      "- Adoption of Hamilton has been successful among teams with active feature development for time-series forecasting and those using Pandas.\n",
      "- Future extensions for Hamilton include integrating with data governance tools and compiling to an orchestration framework.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "openai.api_key = \"YOUR_KEY_HERE\"\n",
    "with open(\"hamilton_paper.pdf\", \"rb\") as f:\n",
    "    result = dr.execute([\"summarized_text\"], inputs={\n",
    "        \"pdf_source\": f,\n",
    "        \"openai_gpt_model\": \"gpt-3.5-turbo-0613\",\n",
    "        \"content_type\": \"Scientific article\",\n",
    "        \"user_query\": \"Can you ELI5 the paper?\"\n",
    "    })\n",
    "print(result[\"summarized_text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T21:02:06.537036Z",
     "start_time": "2023-08-07T21:02:06.529040Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
