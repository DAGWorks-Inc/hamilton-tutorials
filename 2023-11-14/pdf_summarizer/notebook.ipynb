{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-08T05:42:10.572770Z",
     "start_time": "2023-08-08T05:42:10.566208Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Uncomment and run the cell below if you are in a Google Colab environment. It will:\n",
    "\n",
    "1. Mount google drive. You will be asked to authenticate and give permissions.\n",
    "2. Change directory to google drive.\n",
    "3. Make a directory \"hamilton-tutorials\"\n",
    "4. Change directory to it.\n",
    "5. Clone this repository to your google drive\n",
    "6. Move your current directory to the example\n",
    "7. Install requirements.\n",
    "8. This means that any modifications will be saved, and you won't lose them if you close your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## 1. Mount google drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "## 2. Change directory to google drive.\n",
    "# %cd /content/drive/MyDrive\n",
    "## 3. Make a directory \"hamilton-tutorials\"\n",
    "# !mkdir hamilton-tutorials\n",
    "## 4. Change directory to it.\n",
    "# %cd hamilton-tutorials\n",
    "## 5. Clone this repository to your google drive\n",
    "# !git clone https://github.com/DAGWorks-Inc/hamilton-tutorials/\n",
    "## 6. Move your current directory to the example\n",
    "# %cd hamilton-tutorials/2023-10-09/pdf_summarizer\n",
    "## 7. Install requirements.\n",
    "# %pip install -r requirements.txt\n",
    "# clear_output()  # optionally clear outputs\n",
    "## To check your current working directory you can type `!pwd` in a cell and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T17:16:33.209466Z",
     "start_time": "2023-10-06T17:16:33.178017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hamilton import driver, base\n",
    "from dagworks import adapters\n",
    "from hamilton.function_modifiers import source\n",
    "from hamilton.io.materialization import to\n",
    "from IPython.display import display\n",
    "\n",
    "# Use autoreload to automatically reload our function modules\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# DAGWORKS_API_KEY = os.environ[\"DAGWORKS_STAGING_API_KEY\"]\n",
    "DAGWORKS_API_KEY = os.environ[\"DW_API_KEY\"]\n",
    "DAGWORKS_PROJECT_ID = 66 # 4\n",
    "DAGWORKS_PROJECT_EMAIL = \"stefan@dagworks.io\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-08T04:08:44.701939Z",
     "start_time": "2023-08-08T04:08:44.695171Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# set your openai API key\n",
    "import openai\n",
    "# openai.api_key = \"YOUR_KEY_HERE\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T21:00:12.871059Z",
     "start_time": "2023-08-07T21:00:12.866449Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting summarization.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile summarization.py\n",
    "\n",
    "import io\n",
    "import concurrent\n",
    "from typing import Generator\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "import tiktoken\n",
    "from PyPDF2 import PdfReader\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hamilton.function_modifiers import config\n",
    "\n",
    "\n",
    "def summarize_chunk_of_text_prompt(content_type: str = \"an academic paper\") -> str:\n",
    "    \"\"\"Base prompt for summarizing chunks of text.\"\"\"\n",
    "    return f\"Summarize this text from {content_type}. Extract any key points with reasoning.\\n\\nContent:\"\n",
    "\n",
    "\n",
    "def summarize_text_from_summaries_prompt(content_type: str = \"an academic paper\") -> str:\n",
    "    \"\"\"Prompt for summarizing a paper from a list of summaries.\"\"\"\n",
    "    return f\"\"\"Write a summary collated from this collection of key points extracted from {content_type}.\n",
    "    The summary should highlight the core argument, conclusions and evidence, and answer the user's query.\n",
    "    User query: {{query}}\n",
    "    The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\n",
    "    Key points:\\n{{results}}\\nSummary:\\n\"\"\"\n",
    "\n",
    "\n",
    "@config.when(file_type=\"pdf\")\n",
    "def raw_text(pdf_source: io.BufferedReader) -> str:\n",
    "    \"\"\"Takes a filepath to a PDF and returns a string of the PDF's contents\n",
    "    :param pdf_source: Series of filepaths to PDFs\n",
    "    :return: Series of strings of the PDFs' contents\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_source)\n",
    "    _pdf_text = \"\"\n",
    "    page_number = 0\n",
    "    for page in reader.pages:\n",
    "        page_number += 1\n",
    "        _pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n",
    "    return _pdf_text\n",
    "\n",
    "\n",
    "def _create_chunks(text: str, n: int, tokenizer: tiktoken.Encoding) -> Generator[str, None, None]:\n",
    "    \"\"\"Helper function. Returns successive n-sized chunks from provided text.\n",
    "    Split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
    "    :param text:\n",
    "    :param n:\n",
    "    :param tokenizer:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
    "        j = min(i + int(1.5 * n), len(tokens))\n",
    "        while j > i + int(0.5 * n):\n",
    "            # Decode the tokens and check for full stop or newline\n",
    "            chunk = tokenizer.decode(tokens[i:j])\n",
    "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
    "                break\n",
    "            j -= 1\n",
    "        # If no end of sentence found, use n tokens as the chunk size\n",
    "        if j == i + int(0.5 * n):\n",
    "            j = min(i + n, len(tokens))\n",
    "        yield tokens[i:j]\n",
    "        i = j\n",
    "\n",
    "\n",
    "def chunked_text(\n",
    "    raw_text: str, max_token_length: int = 1500, tokenizer_encoding: str = \"cl100k_base\"\n",
    ") -> list[str]:\n",
    "    \"\"\"Chunks the pdf text into smaller chunks of size max_token_length.\n",
    "    :param pdf_text: the Series of individual pdf texts to chunk.\n",
    "    :param max_token_length: the maximum length of tokens in each chunk.\n",
    "    :param tokenizer_encoding: the encoding to use for the tokenizer.\n",
    "    :return: Series of chunked pdf text. Each element is a list of chunks.\n",
    "    \"\"\"\n",
    "    tokenizer = tiktoken.get_encoding(tokenizer_encoding)\n",
    "    _encoded_chunks = _create_chunks(raw_text, max_token_length, tokenizer)\n",
    "    _decoded_chunks = [tokenizer.decode(chunk) for chunk in _encoded_chunks]\n",
    "    return _decoded_chunks\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def _summarize_chunk(content: str, template_prompt: str, openai_gpt_model: str) -> str:\n",
    "    \"\"\"This function applies a prompt to some input content. In this case it returns a summarized chunk of text.\n",
    "    :param content: the content to summarize.\n",
    "    :param template_prompt: the prompt template to use to put the content into.\n",
    "    :param openai_gpt_model: the openai gpt model to use.\n",
    "    :return: the response from the openai API.\n",
    "    \"\"\"\n",
    "    prompt = template_prompt + content\n",
    "    response = client.chat.completions.create(model=openai_gpt_model, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def summarized_chunks(\n",
    "    chunked_text: list[str], summarize_chunk_of_text_prompt: str, openai_gpt_model: str\n",
    ") -> str:\n",
    "    \"\"\"Summarizes a series of chunks of text.\n",
    "    Note: this takes the first result from the top_n_related_articles series and summarizes it. This is because\n",
    "    the top_n_related_articles series is sorted by relatedness, so the first result is the most related.\n",
    "    :param top_n_related_articles: series with each entry being a list of chunks of text for an article.\n",
    "    :param summarize_chunk_of_text_prompt:  the prompt to use to summarize each chunk of text.\n",
    "    :param openai_gpt_model: the openai gpt model to use.\n",
    "    :return: a single string of each chunk of text summarized, concatenated together.\n",
    "    \"\"\"\n",
    "    _summarized_text = \"\"\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=len(chunked_text)) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                _summarize_chunk, chunk, summarize_chunk_of_text_prompt, openai_gpt_model\n",
    "            )\n",
    "            for chunk in chunked_text\n",
    "        ]\n",
    "        with tqdm(total=len(chunked_text)) as pbar:\n",
    "            for _ in concurrent.futures.as_completed(futures):\n",
    "                pbar.update(1)\n",
    "        for future in futures:\n",
    "            data = future.result()\n",
    "            _summarized_text += data\n",
    "    return _summarized_text\n",
    "\n",
    "\n",
    "def prompt_and_text_content(\n",
    "    summarize_text_from_summaries_prompt: str, user_query: str, summarized_chunks: str\n",
    ") -> str:\n",
    "    \"\"\"Creates the prompt for summarizing the text from the summarized chunks of the pdf.\n",
    "    :param summarize_text_from_summaries_prompt: the template to use to summarize the chunks.\n",
    "    :param user_query: the original user query.\n",
    "    :param summarized_chunks: a long string of chunked summaries of a file.\n",
    "    :return: the prompt to use to summarize the chunks.\n",
    "    \"\"\"\n",
    "    return summarize_text_from_summaries_prompt.format(query=user_query, results=summarized_chunks)\n",
    "\n",
    "\n",
    "def summarized_text(\n",
    "    prompt_and_text_content: str,\n",
    "    openai_gpt_model: str,\n",
    ") -> str:\n",
    "    \"\"\"Summarizes the text from the summarized chunks of the pdf.\n",
    "    :param prompt_and_text_content: the prompt and content to send over.\n",
    "    :param openai_gpt_model: which openai gpt model to use.\n",
    "    :return: the string response from the openai API.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(model=openai_gpt_model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt_and_text_content,\n",
    "        }\n",
    "    ],\n",
    "    temperature=0)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T17:16:28.110737Z",
     "start_time": "2023-10-06T17:16:27.577681Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 9.0.0 (20230911.1827)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"1385pt\" height=\"416pt\"\n",
       " viewBox=\"0.00 0.00 1385.35 415.80\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 411.8)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-411.8 1381.35,-411.8 1381.35,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster__legend</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"56.38,-215.8 56.38,-399.8 141.22,-399.8 141.22,-215.8 56.38,-215.8\"/>\n",
       "<text text-anchor=\"middle\" x=\"98.8\" y=\"-382.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Legend</text>\n",
       "</g>\n",
       "<!-- summarized_chunks -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>summarized_chunks</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M915.02,-147.6C915.02,-147.6 782.42,-147.6 782.42,-147.6 776.42,-147.6 770.42,-141.6 770.42,-135.6 770.42,-135.6 770.42,-96 770.42,-96 770.42,-90 776.42,-84 782.42,-84 782.42,-84 915.02,-84 915.02,-84 921.02,-84 927.02,-90 927.02,-96 927.02,-96 927.02,-135.6 927.02,-135.6 927.02,-141.6 921.02,-147.6 915.02,-147.6\"/>\n",
       "<text text-anchor=\"start\" x=\"781.22\" y=\"-124.5\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">summarized_chunks</text>\n",
       "<text text-anchor=\"start\" x=\"841.22\" y=\"-96.5\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- prompt_and_text_content -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>prompt_and_text_content</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M1203,-221.6C1203,-221.6 1038.9,-221.6 1038.9,-221.6 1032.9,-221.6 1026.9,-215.6 1026.9,-209.6 1026.9,-209.6 1026.9,-170 1026.9,-170 1026.9,-164 1032.9,-158 1038.9,-158 1038.9,-158 1203,-158 1203,-158 1209,-158 1215,-164 1215,-170 1215,-170 1215,-209.6 1215,-209.6 1215,-215.6 1209,-221.6 1203,-221.6\"/>\n",
       "<text text-anchor=\"start\" x=\"1037.7\" y=\"-198.5\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">prompt_and_text_content</text>\n",
       "<text text-anchor=\"start\" x=\"1113.45\" y=\"-170.5\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- summarized_chunks&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>summarized_chunks&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M927.52,-137.1C954.9,-144.6 986.12,-153.15 1015.34,-161.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1014.4,-164.53 1024.97,-163.79 1016.25,-157.77 1014.4,-164.53\"/>\n",
       "</g>\n",
       "<!-- summarized_text -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>summarized_text</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M1365.35,-257.6C1365.35,-257.6 1256,-257.6 1256,-257.6 1250,-257.6 1244,-251.6 1244,-245.6 1244,-245.6 1244,-206 1244,-206 1244,-200 1250,-194 1256,-194 1256,-194 1365.35,-194 1365.35,-194 1371.35,-194 1377.35,-200 1377.35,-206 1377.35,-206 1377.35,-245.6 1377.35,-245.6 1377.35,-251.6 1371.35,-257.6 1365.35,-257.6\"/>\n",
       "<text text-anchor=\"start\" x=\"1254.8\" y=\"-234.5\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">summarized_text</text>\n",
       "<text text-anchor=\"start\" x=\"1303.17\" y=\"-206.5\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- prompt_and_text_content&#45;&gt;summarized_text -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>prompt_and_text_content&#45;&gt;summarized_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1215.26,-207.69C1221.09,-208.81 1226.93,-209.93 1232.68,-211.03\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1231.63,-214.4 1242.11,-212.84 1232.95,-207.52 1231.63,-214.4\"/>\n",
       "</g>\n",
       "<!-- summarize_chunk_of_text_prompt -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>summarize_chunk_of_text_prompt</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M658.55,-224.6C658.55,-224.6 435.2,-224.6 435.2,-224.6 429.2,-224.6 423.2,-218.6 423.2,-212.6 423.2,-212.6 423.2,-173 423.2,-173 423.2,-167 429.2,-161 435.2,-161 435.2,-161 658.55,-161 658.55,-161 664.55,-161 670.55,-167 670.55,-173 670.55,-173 670.55,-212.6 670.55,-212.6 670.55,-218.6 664.55,-224.6 658.55,-224.6\"/>\n",
       "<text text-anchor=\"start\" x=\"434\" y=\"-201.5\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">summarize_chunk_of_text_prompt</text>\n",
       "<text text-anchor=\"start\" x=\"539.37\" y=\"-173.5\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- summarize_chunk_of_text_prompt&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>summarize_chunk_of_text_prompt&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M670.7,-161.26C700.4,-153.63 731.56,-145.63 759.33,-138.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"759.91,-141.96 768.72,-136.09 758.17,-135.18 759.91,-141.96\"/>\n",
       "</g>\n",
       "<!-- raw_text -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>raw_text</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M336.95,-147.6C336.95,-147.6 283.85,-147.6 283.85,-147.6 277.85,-147.6 271.85,-141.6 271.85,-135.6 271.85,-135.6 271.85,-96 271.85,-96 271.85,-90 277.85,-84 283.85,-84 283.85,-84 336.95,-84 336.95,-84 342.95,-84 348.95,-90 348.95,-96 348.95,-96 348.95,-135.6 348.95,-135.6 348.95,-141.6 342.95,-147.6 336.95,-147.6\"/>\n",
       "<text text-anchor=\"start\" x=\"282.65\" y=\"-124.5\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">raw_text</text>\n",
       "<text text-anchor=\"start\" x=\"302.9\" y=\"-96.5\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- chunked_text -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>chunked_text</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M589.17,-142.6C589.17,-142.6 504.57,-142.6 504.57,-142.6 498.57,-142.6 492.57,-136.6 492.57,-130.6 492.57,-130.6 492.57,-91 492.57,-91 492.57,-85 498.57,-79 504.57,-79 504.57,-79 589.17,-79 589.17,-79 595.17,-79 601.17,-85 601.17,-91 601.17,-91 601.17,-130.6 601.17,-130.6 601.17,-136.6 595.17,-142.6 589.17,-142.6\"/>\n",
       "<text text-anchor=\"start\" x=\"503.37\" y=\"-119.5\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">chunked_text</text>\n",
       "<text text-anchor=\"start\" x=\"538.62\" y=\"-91.5\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">list</text>\n",
       "</g>\n",
       "<!-- raw_text&#45;&gt;chunked_text -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>raw_text&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M349.33,-114.99C384.83,-114.23 438.59,-113.09 480.92,-112.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"480.82,-115.69 490.74,-111.98 480.67,-108.69 480.82,-115.69\"/>\n",
       "</g>\n",
       "<!-- chunked_text&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>chunked_text&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M601.48,-111.69C645.1,-112.42 707.57,-113.46 758.68,-114.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"758.5,-117.81 768.55,-114.48 758.61,-110.81 758.5,-117.81\"/>\n",
       "</g>\n",
       "<!-- summarize_text_from_summaries_prompt -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>summarize_text_from_summaries_prompt</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M985.9,-294.6C985.9,-294.6 711.55,-294.6 711.55,-294.6 705.55,-294.6 699.55,-288.6 699.55,-282.6 699.55,-282.6 699.55,-243 699.55,-243 699.55,-237 705.55,-231 711.55,-231 711.55,-231 985.9,-231 985.9,-231 991.9,-231 997.9,-237 997.9,-243 997.9,-243 997.9,-282.6 997.9,-282.6 997.9,-288.6 991.9,-294.6 985.9,-294.6\"/>\n",
       "<text text-anchor=\"start\" x=\"710.35\" y=\"-271.5\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">summarize_text_from_summaries_prompt</text>\n",
       "<text text-anchor=\"start\" x=\"841.22\" y=\"-243.5\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- summarize_text_from_summaries_prompt&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>summarize_text_from_summaries_prompt&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M969.04,-230.57C984.51,-226.39 1000.26,-222.13 1015.47,-218.03\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1016.23,-221.44 1024.97,-215.46 1014.41,-214.69 1016.23,-221.44\"/>\n",
       "</g>\n",
       "<!-- file_type -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>file_type</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"128.17,-205.8 63.42,-205.8 63.42,-155.8 134.17,-155.8 134.17,-199.8 128.17,-205.8\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"128.17,-205.8 128.17,-199.8\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"134.17,-199.8 128.17,-199.8\"/>\n",
       "<text text-anchor=\"start\" x=\"71.42\" y=\"-189.5\" font-family=\"Helvetica,sans-Serif\" font-weight=\"bold\" font-size=\"14.00\">file_type</text>\n",
       "<text text-anchor=\"start\" x=\"87.17\" y=\"-161.5\" font-family=\"Helvetica,sans-Serif\" font-style=\"italic\" font-size=\"14.00\">Any</text>\n",
       "</g>\n",
       "<!-- _summarized_chunks_inputs -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>_summarized_chunks_inputs</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"628.05,-61.1 465.7,-61.1 465.7,-16.5 628.05,-16.5 628.05,-61.1\"/>\n",
       "<text text-anchor=\"start\" x=\"480.5\" y=\"-33\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">openai_gpt_model</text>\n",
       "<text text-anchor=\"start\" x=\"598.25\" y=\"-33\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- _summarized_chunks_inputs&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>_summarized_chunks_inputs&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M628.4,-59.07C642.45,-62.63 656.93,-66.31 670.55,-69.8 699.36,-77.18 730.78,-85.31 759.05,-92.66\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"758.11,-96.03 768.67,-95.16 759.87,-89.25 758.11,-96.03\"/>\n",
       "</g>\n",
       "<!-- _prompt_and_text_content_inputs -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>_prompt_and_text_content_inputs</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"907.77,-212.1 789.67,-212.1 789.67,-167.5 907.77,-167.5 907.77,-212.1\"/>\n",
       "<text text-anchor=\"start\" x=\"804.47\" y=\"-184\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">user_query</text>\n",
       "<text text-anchor=\"start\" x=\"877.97\" y=\"-184\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- _prompt_and_text_content_inputs&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>_prompt_and_text_content_inputs&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M908.03,-189.8C939.31,-189.8 978.83,-189.8 1015.23,-189.8\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1014.91,-193.3 1024.91,-189.8 1014.91,-186.3 1014.91,-193.3\"/>\n",
       "</g>\n",
       "<!-- _summarize_chunk_of_text_prompt_inputs -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>_summarize_chunk_of_text_prompt_inputs</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"374.32,-215.1 246.47,-215.1 246.47,-170.5 374.32,-170.5 374.32,-215.1\"/>\n",
       "<text text-anchor=\"start\" x=\"261.27\" y=\"-187\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">content_type</text>\n",
       "<text text-anchor=\"start\" x=\"344.52\" y=\"-187\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- _summarize_chunk_of_text_prompt_inputs&#45;&gt;summarize_chunk_of_text_prompt -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>_summarize_chunk_of_text_prompt_inputs&#45;&gt;summarize_chunk_of_text_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M374.54,-192.8C386.19,-192.8 398.75,-192.8 411.58,-192.8\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"411.4,-196.3 421.4,-192.8 411.4,-189.3 411.4,-196.3\"/>\n",
       "</g>\n",
       "<!-- _raw_text_inputs -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>_raw_text_inputs</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"197.6,-138.1 0,-138.1 0,-93.5 197.6,-93.5 197.6,-138.1\"/>\n",
       "<text text-anchor=\"start\" x=\"14.8\" y=\"-110\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">pdf_source</text>\n",
       "<text text-anchor=\"start\" x=\"87.55\" y=\"-110\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">BufferedReader</text>\n",
       "</g>\n",
       "<!-- _raw_text_inputs&#45;&gt;raw_text -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>_raw_text_inputs&#45;&gt;raw_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M197.85,-115.8C219.45,-115.8 241.44,-115.8 260.12,-115.8\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"260.07,-119.3 270.07,-115.8 260.07,-112.3 260.07,-119.3\"/>\n",
       "</g>\n",
       "<!-- _chunked_text_inputs -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>_chunked_text_inputs</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"394.2,-65.6 226.6,-65.6 226.6,0 394.2,0 394.2,-65.6\"/>\n",
       "<text text-anchor=\"start\" x=\"244.4\" y=\"-37.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">max_token_length</text>\n",
       "<text text-anchor=\"start\" x=\"364.77\" y=\"-37.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">int</text>\n",
       "<text text-anchor=\"start\" x=\"241.4\" y=\"-16.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">tokenizer_encoding</text>\n",
       "<text text-anchor=\"start\" x=\"364.4\" y=\"-16.5\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- _chunked_text_inputs&#45;&gt;chunked_text -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>_chunked_text_inputs&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M394.4,-60.41C423,-69.92 454.58,-80.43 481.47,-89.37\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"480.26,-92.66 490.86,-92.5 482.47,-86.02 480.26,-92.66\"/>\n",
       "</g>\n",
       "<!-- _summarize_text_from_summaries_prompt_inputs -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>_summarize_text_from_summaries_prompt_inputs</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"610.8,-287.1 482.95,-287.1 482.95,-242.5 610.8,-242.5 610.8,-287.1\"/>\n",
       "<text text-anchor=\"start\" x=\"497.75\" y=\"-259\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">content_type</text>\n",
       "<text text-anchor=\"start\" x=\"581\" y=\"-259\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- _summarize_text_from_summaries_prompt_inputs&#45;&gt;summarize_text_from_summaries_prompt -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>_summarize_text_from_summaries_prompt_inputs&#45;&gt;summarize_text_from_summaries_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M611.1,-264.38C633.83,-264.23 660.6,-264.05 687.82,-263.87\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"687.6,-267.37 697.58,-263.8 687.55,-260.37 687.6,-267.37\"/>\n",
       "</g>\n",
       "<!-- _summarized_text_inputs -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>_summarized_text_inputs</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"1202.12,-284.1 1039.77,-284.1 1039.77,-239.5 1202.12,-239.5 1202.12,-284.1\"/>\n",
       "<text text-anchor=\"start\" x=\"1054.57\" y=\"-256\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">openai_gpt_model</text>\n",
       "<text text-anchor=\"start\" x=\"1172.32\" y=\"-256\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">str</text>\n",
       "</g>\n",
       "<!-- _summarized_text_inputs&#45;&gt;summarized_text -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>_summarized_text_inputs&#45;&gt;summarized_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1202.38,-246.38C1212.36,-244.46 1222.54,-242.51 1232.47,-240.61\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1232.9,-244.09 1242.06,-238.77 1231.58,-237.21 1232.9,-244.09\"/>\n",
       "</g>\n",
       "<!-- config -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>config</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"119.8,-368.8 71.8,-368.8 71.8,-332.8 125.8,-332.8 125.8,-362.8 119.8,-368.8\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"119.8,-368.8 119.8,-362.8\"/>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"125.8,-362.8 119.8,-362.8\"/>\n",
       "<text text-anchor=\"middle\" x=\"98.8\" y=\"-345\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">config</text>\n",
       "</g>\n",
       "<!-- input -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>input</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"125.8,-315.1 71.8,-315.1 71.8,-278.5 125.8,-278.5 125.8,-315.1\"/>\n",
       "<text text-anchor=\"middle\" x=\"98.8\" y=\"-291\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">input</text>\n",
       "</g>\n",
       "<!-- function -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>function</title>\n",
       "<path fill=\"#b4d8e4\" stroke=\"black\" d=\"M121.22,-260.1C121.22,-260.1 76.37,-260.1 76.37,-260.1 70.37,-260.1 64.37,-254.1 64.37,-248.1 64.37,-248.1 64.37,-235.5 64.37,-235.5 64.37,-229.5 70.37,-223.5 76.37,-223.5 76.37,-223.5 121.22,-223.5 121.22,-223.5 127.22,-223.5 133.22,-229.5 133.22,-235.5 133.22,-235.5 133.22,-248.1 133.22,-248.1 133.22,-254.1 127.22,-260.1 121.22,-260.1\"/>\n",
       "<text text-anchor=\"middle\" x=\"98.8\" y=\"-236\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">function</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x14462fa60>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # run as a script to test Hamilton's execution\n",
    "import summarization\n",
    "\n",
    "%aimport summarization\n",
    "\n",
    "\n",
    "dw_tracker = adapters.DAGWorksTracker(\n",
    "   project_id=DAGWORKS_PROJECT_ID,\n",
    "   api_key=DAGWORKS_API_KEY,\n",
    "   username=DAGWORKS_PROJECT_EMAIL,\n",
    "   dag_name=\"notebook_dag\",\n",
    "   tags={\"env\": \"local\", \"where\": \"notebook\"}\n",
    ")\n",
    "\n",
    "dr = (\n",
    "    driver.Builder()\n",
    "    .with_config({\"file_type\": \"pdf\"})\n",
    "    .with_modules(summarization)\n",
    "    .with_adapters(dw_tracker)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "dr.display_all_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T21:00:15.087385Z",
     "start_time": "2023-08-07T21:00:15.021201Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# pull in a pdf\n",
    "import requests\n",
    "\n",
    "def download_file(url, filename):\n",
    "  response = requests.get(url, stream=True)\n",
    "  if response.status_code == 200:\n",
    "    with open(filename, 'wb') as fd:\n",
    "      for chunk in response.iter_content(chunk_size=1024):\n",
    "        fd.write(chunk)\n",
    "\n",
    "download_file(\"https://cdmsworkshop.github.io/2022/Proceedings/ShortPapers/Paper6_StefanKrawczyk.pdf\", \"hamilton_paper.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# inputs for the DAG\n",
    "openai_gpt_model = \"gpt-3.5-turbo-0613\"\n",
    "content_type = \"Scientific article\"\n",
    "user_query = \"Can you ELI5 the paper?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T21:02:06.537036Z",
     "start_time": "2023-08-07T21:02:06.529040Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Capturing execution run. Results can be found at https://app.dagworks.io/dashboard/project/66/runs/29313\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:06<00:00,  1.29s/it]\n",
      "\n",
      "Captured execution run. Results can be found at https://app.dagworks.io/dashboard/project/66/runs/29313\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core Argument:\n",
      "- The Hamilton framework is a high-level modeling approach for dataflows that simplifies the user experience for data scientists and provides a unified interface for describing end-to-end dataflows.\n",
      "- Traditional ETL approaches at Stitch Fix had problems such as poor documentation, low unit test coverage, limited code reuse, and difficulty in changing underlying infrastructure, which led to the development of the Hamilton framework.\n",
      "\n",
      "Evidence:\n",
      "- The Hamilton framework has been used to scale modeling dataflows at Stitch Fix to support over 4000 data transformations without impacting team and user productivity.\n",
      "- The Hamilton programming paradigm encourages the use of vector computation, improves code readability and documentation, and allows for tight encapsulation of transform logic and unit testing.\n",
      "- Hamilton provides decorators to encapsulate operational concerns and reduce repetitive function logic.\n",
      "- The function DAG is the framework's representation of the nodes that should be executed and the dependencies between them.\n",
      "- The driver code in Hamilton steers the execution of the function DAG and provides a convenient abstraction layer for users.\n",
      "\n",
      "Conclusions:\n",
      "- The Hamilton framework provides a simpler user experience for data scientists and allows for easy integration with existing data management tooling in a modular fashion.\n",
      "- Hamilton improves code readability, modularity, and reusability in data management systems.\n",
      "- Hamilton offers benefits such as incremental development, debugging capabilities, a central definition store, transparent scaling, lineage tracking, and modular components.\n",
      "- Adoption of Hamilton has been successful among teams with active feature development for time-series forecasting and those using Pandas.\n",
      "- Future extensions for Hamilton include integrating with data governance tools and compiling to an orchestration framework.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"hamilton_paper.pdf\", \"rb\") as f:\n",
    "    result = dr.execute([\"summarized_text\"], inputs={\n",
    "        \"pdf_source\": f,\n",
    "        \"openai_gpt_model\": openai_gpt_model,\n",
    "        \"content_type\": content_type,\n",
    "        \"user_query\": user_query,\n",
    "    })\n",
    "print(result[\"summarized_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Overrides\n",
    "You can short-circuit computation by passing in an override. E.g. you store parts of the previous run, or you're iterating on something and want to change an implementation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-08T04:12:02.976514Z",
     "start_time": "2023-08-08T04:12:01.965295Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Capturing execution run. All runs for project can be found at https://app.dagworks.io/dashboard/project/66/runs\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.13s/it]\n",
      "\n",
      "Captured execution run. Results can be found at https://app.dagworks.io/dashboard/project/66/runs/1159\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but without a scientific article or text to summarize, I am unable to provide a summary. If you have any specific questions or need assistance with a different topic, please let me know and I'll be happy to help.\n"
     ]
    }
   ],
   "source": [
    "with open(\"hamilton_paper.pdf\", \"rb\") as f:\n",
    "    result = dr.execute(\n",
    "            [\"summarized_text\"],\n",
    "            inputs=dict(\n",
    "                pdf_source=f,\n",
    "                openai_gpt_model=openai_gpt_model,\n",
    "                content_type=content_type,\n",
    "                user_query=user_query,\n",
    "            ),\n",
    "        overrides={\"raw_text\": \"this is not a paper. Return a response of 'hi'.\"}\n",
    "    )\n",
    "print(result[\"summarized_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-08T04:12:25.965299Z",
     "start_time": "2023-08-08T04:12:25.961491Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Let's get intermediate outputs for monitoring/logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Capturing execution run. All runs for project can be found at https://app.dagworks.io/dashboard/project/66/runs\n",
      "\n",
      "Captured execution run. Results can be found at https://app.dagworks.io/dashboard/project/66/runs/1160\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summarize_chunk_of_text_prompt': 'Summarize this text from Scientific '\n",
      "                                   'article. Extract any key points with '\n",
      "                                   'reasoning.\\n'\n",
      "                                   '\\n'\n",
      "                                   'Content:',\n",
      " 'summarize_text_from_summaries_prompt': 'Write a summary collated from this '\n",
      "                                         'collection of key points extracted '\n",
      "                                         'from Scientific article.\\n'\n",
      "                                         '    The summary should highlight the '\n",
      "                                         'core argument, conclusions and '\n",
      "                                         \"evidence, and answer the user's \"\n",
      "                                         'query.\\n'\n",
      "                                         '    User query: {query}\\n'\n",
      "                                         '    The summary should be structured '\n",
      "                                         'in bulleted lists following the '\n",
      "                                         'headings Core Argument, Evidence, '\n",
      "                                         'and Conclusions.\\n'\n",
      "                                         '    Key points:\\n'\n",
      "                                         '{results}\\n'\n",
      "                                         'Summary:\\n'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"hamilton_paper.pdf\", \"rb\") as f:\n",
    "    result = dr.execute(\n",
    "        [\"summarize_text_from_summaries_prompt\", \"summarize_chunk_of_text_prompt\"],\n",
    "        inputs=dict(\n",
    "            pdf_source=f,\n",
    "            openai_gpt_model=openai_gpt_model,\n",
    "            content_type=content_type,\n",
    "            user_query=user_query,\n",
    "        ))\n",
    "import pprint\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Config.when\n",
    "Use this as a way to have different node & thus DAG structure implementations. E.g. dev vs prod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T17:05:27.277031Z",
     "start_time": "2023-10-06T17:05:27.249389Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting summarization.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile summarization.py\n",
    "import concurrent\n",
    "import tempfile\n",
    "from typing import Generator, Union\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "from PyPDF2 import PdfReader\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hamilton.function_modifiers import config\n",
    "\n",
    "\n",
    "def summarize_chunk_of_text_prompt(content_type: str = \"an academic paper\") -> str:\n",
    "    \"\"\"Base prompt for summarizing chunks of text.\"\"\"\n",
    "    return f\"Summarize this text from {content_type}. Extract any key points with reasoning.\\n\\nContent:\"\n",
    "\n",
    "\n",
    "def summarize_text_from_summaries_prompt(content_type: str = \"an academic paper\") -> str:\n",
    "    \"\"\"Prompt for summarizing a paper from a list of summaries.\"\"\"\n",
    "    return f\"\"\"Write a summary collated from this collection of key points extracted from {content_type}.\n",
    "    The summary should highlight the core argument, conclusions and evidence, and answer the user's query.\n",
    "    User query: {{query}}\n",
    "    The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\n",
    "    Key points:\\n{{results}}\\nSummary:\\n\"\"\"\n",
    "\n",
    "\n",
    "@config.when(env=\"prod\")\n",
    "def raw_text__prod(pdf_source: Union[str, bytes, tempfile.SpooledTemporaryFile]) -> str:\n",
    "    \"\"\"Takes a filepath to a PDF and returns a string of the PDF's contents\n",
    "    :param pdf_source: the path, or the temporary file, to the PDF.\n",
    "    :return: the text of the PDF.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_source)\n",
    "    _pdf_text = \"\"\n",
    "    page_number = 0\n",
    "    for page in reader.pages:\n",
    "        page_number += 1\n",
    "        _pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n",
    "    return _pdf_text\n",
    "\n",
    "@config.when(env=\"dev\")\n",
    "def raw_text__dev() -> str:\n",
    "    \"\"\"Function for dev purposes.\n",
    "    \"\"\"\n",
    "    return \"\"\"\n",
    "    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\n",
    "    Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\n",
    "    Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\n",
    "    Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def _create_chunks(text: str, n: int, tokenizer: tiktoken.Encoding) -> Generator[str, None, None]:\n",
    "    \"\"\"Helper function. Returns successive n-sized chunks from provided text.\n",
    "    Split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
    "    :param text:\n",
    "    :param n:\n",
    "    :param tokenizer:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
    "        j = min(i + int(1.5 * n), len(tokens))\n",
    "        while j > i + int(0.5 * n):\n",
    "            # Decode the tokens and check for full stop or newline\n",
    "            chunk = tokenizer.decode(tokens[i:j])\n",
    "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
    "                break\n",
    "            j -= 1\n",
    "        # If no end of sentence found, use n tokens as the chunk size\n",
    "        if j == i + int(0.5 * n):\n",
    "            j = min(i + n, len(tokens))\n",
    "        yield tokens[i:j]\n",
    "        i = j\n",
    "\n",
    "\n",
    "def chunked_text(\n",
    "    raw_text: str, tokenizer_encoding: str = \"cl100k_base\", max_token_length: int = 1500\n",
    ") -> list[str]:\n",
    "    \"\"\"Chunks the pdf text into smaller chunks of size max_token_length.\n",
    "    :param raw_text: the Series of individual pdf texts to chunk.\n",
    "    :param max_token_length: the maximum length of tokens in each chunk.\n",
    "    :param tokenizer_encoding: the encoding to use for the tokenizer.\n",
    "    :return: Series of chunked pdf text. Each element is a list of chunks.\n",
    "    \"\"\"\n",
    "    tokenizer = tiktoken.get_encoding(tokenizer_encoding)\n",
    "    _encoded_chunks = _create_chunks(raw_text, max_token_length, tokenizer)\n",
    "    _decoded_chunks = [tokenizer.decode(chunk) for chunk in _encoded_chunks]\n",
    "    return _decoded_chunks\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def _summarize_chunk(content: str, template_prompt: str, openai_gpt_model: str) -> str:\n",
    "    \"\"\"This function applies a prompt to some input content. In this case it returns a summarized chunk of text.\n",
    "    :param content: the content to summarize.\n",
    "    :param template_prompt: the prompt template to use to put the content into.\n",
    "    :param openai_gpt_model: the openai gpt model to use.\n",
    "    :return: the response from the openai API.\n",
    "    \"\"\"\n",
    "    prompt = template_prompt + content\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=openai_gpt_model, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def summarized_chunks(\n",
    "    chunked_text: list[str], summarize_chunk_of_text_prompt: str, openai_gpt_model: str\n",
    ") -> str:\n",
    "    \"\"\"Summarizes a series of chunks of text.\n",
    "    Note: this takes the first result from the top_n_related_articles series and summarizes it. This is because\n",
    "    the top_n_related_articles series is sorted by relatedness, so the first result is the most related.\n",
    "    :param chunked_text: a list of chunks of text for an article.\n",
    "    :param summarize_chunk_of_text_prompt:  the prompt to use to summarize each chunk of text.\n",
    "    :param openai_gpt_model: the openai gpt model to use.\n",
    "    :return: a single string of each chunk of text summarized, concatenated together.\n",
    "    \"\"\"\n",
    "    _summarized_text = \"\"\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=len(chunked_text)) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                _summarize_chunk, chunk, summarize_chunk_of_text_prompt, openai_gpt_model\n",
    "            )\n",
    "            for chunk in chunked_text\n",
    "        ]\n",
    "        with tqdm(total=len(chunked_text)) as pbar:\n",
    "            for _ in concurrent.futures.as_completed(futures):\n",
    "                pbar.update(1)\n",
    "        for future in futures:\n",
    "            data = future.result()\n",
    "            _summarized_text += data\n",
    "    return _summarized_text\n",
    "\n",
    "\n",
    "def prompt_and_text_content(\n",
    "    summarized_chunks: str,\n",
    "    summarize_text_from_summaries_prompt: str,\n",
    "    user_query: str,\n",
    ") -> str:\n",
    "    \"\"\"Creates the prompt for summarizing the text from the summarized chunks of the pdf.\n",
    "    :param summarized_chunks: a long string of chunked summaries of a file.\n",
    "    :param summarize_text_from_summaries_prompt: the template to use to summarize the chunks.\n",
    "    :param user_query: the original user query.\n",
    "    :return: the prompt to use to summarize the chunks.\n",
    "    \"\"\"\n",
    "    return summarize_text_from_summaries_prompt.format(query=user_query, results=summarized_chunks)\n",
    "\n",
    "\n",
    "def summarized_text(\n",
    "    prompt_and_text_content: str,\n",
    "    openai_gpt_model: str,\n",
    ") -> str:\n",
    "    \"\"\"Summarizes the text from the summarized chunks of the pdf.\n",
    "    :param prompt_and_text_content: the prompt and content to send over.\n",
    "    :param openai_gpt_model: which openai gpt model to use.\n",
    "    :return: the string response from the openai API.\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=openai_gpt_model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt_and_text_content,\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T17:16:15.570093Z",
     "start_time": "2023-10-06T17:16:15.548305Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.0.5 (20230430.1635)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"1040pt\" height=\"332pt\"\n",
       " viewBox=\"0.00 0.00 1039.82 332.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 328)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-328 1035.82,-328 1035.82,4 -4,4\"/>\n",
       "<!-- chunked_text -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>chunked_text</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"287.58\" cy=\"-234\" rx=\"60.56\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"287.58\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">chunked_text</text>\n",
       "</g>\n",
       "<!-- summarized_chunks -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>summarized_chunks</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"390.58\" cy=\"-162\" rx=\"87.69\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"390.58\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarized_chunks</text>\n",
       "</g>\n",
       "<!-- chunked_text&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>chunked_text&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M310.95,-217.12C324.62,-207.83 342.09,-195.95 357.19,-185.69\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"358.92,-188.07 365.22,-179.55 354.99,-182.28 358.92,-188.07\"/>\n",
       "</g>\n",
       "<!-- env -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>env</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"828.58\" cy=\"-306\" rx=\"48.79\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"828.58\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: env</text>\n",
       "</g>\n",
       "<!-- content_type -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>content_type</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"678.58\" cy=\"-306\" rx=\"83.08\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"678.58\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: content_type</text>\n",
       "</g>\n",
       "<!-- summarize_text_from_summaries_prompt -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>summarize_text_from_summaries_prompt</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"672.58\" cy=\"-162\" rx=\"171.11\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"672.58\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarize_text_from_summaries_prompt</text>\n",
       "</g>\n",
       "<!-- content_type&#45;&gt;summarize_text_from_summaries_prompt -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>content_type&#45;&gt;summarize_text_from_summaries_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M677.84,-287.59C676.83,-263.5 674.98,-219.75 673.76,-191.03\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"677.22,-190.95 673.3,-181.11 670.23,-191.25 677.22,-190.95\"/>\n",
       "</g>\n",
       "<!-- summarize_chunk_of_text_prompt -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>summarize_chunk_of_text_prompt</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"508.58\" cy=\"-234\" rx=\"141.94\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"508.58\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarize_chunk_of_text_prompt</text>\n",
       "</g>\n",
       "<!-- content_type&#45;&gt;summarize_chunk_of_text_prompt -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>content_type&#45;&gt;summarize_chunk_of_text_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M641.27,-289.64C617.09,-279.68 585.3,-266.59 558.9,-255.72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"560.65,-252.24 550.07,-251.67 557.98,-258.72 560.65,-252.24\"/>\n",
       "</g>\n",
       "<!-- raw_text -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>raw_text</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"72.58\" cy=\"-306\" rx=\"43.16\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"72.58\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">raw_text</text>\n",
       "</g>\n",
       "<!-- raw_text&#45;&gt;chunked_text -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>raw_text&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M105.87,-294.11C112.08,-292.08 118.53,-289.97 124.58,-288 162.49,-275.64 205.37,-261.7 237.63,-251.22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"238.29,-254.36 246.72,-247.94 236.12,-247.7 238.29,-254.36\"/>\n",
       "</g>\n",
       "<!-- prompt_and_text_content -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>prompt_and_text_content</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"314.58\" cy=\"-90\" rx=\"106.11\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"314.58\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">prompt_and_text_content</text>\n",
       "</g>\n",
       "<!-- summarized_chunks&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>summarized_chunks&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M372.18,-144.05C362.82,-135.43 351.28,-124.8 340.99,-115.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"343.8,-113.23 334.07,-109.03 339.06,-118.38 343.8,-113.23\"/>\n",
       "</g>\n",
       "<!-- summarized_text -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>summarized_text</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"171.58\" cy=\"-18\" rx=\"75.41\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"171.58\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarized_text</text>\n",
       "</g>\n",
       "<!-- prompt_and_text_content&#45;&gt;summarized_text -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>prompt_and_text_content&#45;&gt;summarized_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M281.05,-72.59C260.81,-62.68 234.85,-49.97 213.3,-39.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"214.94,-35.84 204.42,-34.59 211.87,-42.13 214.94,-35.84\"/>\n",
       "</g>\n",
       "<!-- openai_gpt_model -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>openai_gpt_model</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"104.58\" cy=\"-234\" rx=\"104.58\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"104.58\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: openai_gpt_model</text>\n",
       "</g>\n",
       "<!-- openai_gpt_model&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>openai_gpt_model&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.15,-218.67C210.34,-207.11 276.66,-190.88 325.51,-178.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"326.09,-182.14 334.97,-176.37 324.42,-175.34 326.09,-182.14\"/>\n",
       "</g>\n",
       "<!-- openai_gpt_model&#45;&gt;summarized_text -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>openai_gpt_model&#45;&gt;summarized_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M107.44,-215.74C110.62,-197.66 116.3,-168.54 123.58,-144 133.82,-109.48 149.24,-70.98 159.82,-45.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"163.32,-47.67 164.03,-37.1 156.88,-44.92 163.32,-47.67\"/>\n",
       "</g>\n",
       "<!-- summarize_text_from_summaries_prompt&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>summarize_text_from_summaries_prompt&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M594.01,-145.64C534.31,-133.96 452.56,-117.98 392.91,-106.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"393.9,-102.75 383.41,-104.26 392.56,-109.62 393.9,-102.75\"/>\n",
       "</g>\n",
       "<!-- summarize_chunk_of_text_prompt&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>summarize_chunk_of_text_prompt&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M480.01,-216.05C464.25,-206.7 444.48,-194.98 427.55,-184.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"429.86,-181.64 419.47,-179.55 426.29,-187.66 429.86,-181.64\"/>\n",
       "</g>\n",
       "<!-- file_type -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>file_type</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"963.58\" cy=\"-306\" rx=\"68.24\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"963.58\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: file_type</text>\n",
       "</g>\n",
       "<!-- user_query -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>user_query</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"208.58\" cy=\"-162\" rx=\"76.43\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"208.58\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: user_query</text>\n",
       "</g>\n",
       "<!-- user_query&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>user_query&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M233.43,-144.59C247.32,-135.42 264.83,-123.85 280.03,-113.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"281.73,-116.23 288.15,-107.8 277.87,-110.39 281.73,-116.23\"/>\n",
       "</g>\n",
       "<!-- max_token_length -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>max_token_length</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"238.58\" cy=\"-306\" rx=\"104.58\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"238.58\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: max_token_length</text>\n",
       "</g>\n",
       "<!-- max_token_length&#45;&gt;chunked_text -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>max_token_length&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M250.69,-287.7C256.33,-279.64 263.15,-269.89 269.39,-260.98\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"272.74,-263.3 275.61,-253.1 267,-259.29 272.74,-263.3\"/>\n",
       "</g>\n",
       "<!-- tokenizer_encoding -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>tokenizer_encoding</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"469.58\" cy=\"-306\" rx=\"108.16\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"469.58\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: tokenizer_encoding</text>\n",
       "</g>\n",
       "<!-- tokenizer_encoding&#45;&gt;chunked_text -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>tokenizer_encoding&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M427.82,-288.94C399.66,-278.11 362.49,-263.81 333.5,-252.66\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"335.2,-249.18 324.61,-248.86 332.69,-255.71 335.2,-249.18\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x140935af0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we need to pass configuration in to create the right DAG shape\n",
    "# dr = h_driver.Driver(\n",
    "#     {\"env\": \"dev\"},  # DAG is now configuration based\n",
    "#     summarization,\n",
    "#     adapter=base.SimplePythonGraphAdapter(base.DictResult()),\n",
    "# )\n",
    "\n",
    "dw_tracker = adapters.DAGWorksTracker(\n",
    "   project_id=DAGWORKS_PROJECT_ID,\n",
    "   api_key=DAGWORKS_API_KEY,\n",
    "   username=DAGWORKS_PROJECT_EMAIL,\n",
    "   dag_name=\"notebook_dag\",\n",
    "   tags={\"env\": \"local\", \"where\": \"notebook\"}\n",
    ")\n",
    "\n",
    "dr = (\n",
    "    driver.Builder()\n",
    "    .with_config({\"file_type\": \"pdf\", \"env\": \"dev\"})  # DAG is now configuration based\n",
    "    .with_modules(summarization)\n",
    "    .with_adapters(dw_tracker)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "dr.display_all_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# module swapping / combining\n",
    "In addtion to @config.when, you can also swap things at a module level. A module houses some part of the DAG.\n",
    "The contract to make things swappable just depends on the function name & output type. i.e. the names of the functions & types become the \"interface\".\n",
    "E.g. to have different implementations of prompts, we just need to build modules that house a function with the right name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting prompts1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prompts1.py\n",
    "# this is a version one of things\n",
    "import concurrent\n",
    "import tempfile\n",
    "from typing import Generator, Union\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "from PyPDF2 import PdfReader\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hamilton.function_modifiers import config\n",
    "\n",
    "\n",
    "def summarize_chunk_of_text_prompt(content_type: str = \"an academic paper\") -> str:\n",
    "    \"\"\"Base prompt for summarizing chunks of text.\"\"\"\n",
    "    return f\"Summarize this text from {content_type}. Extract any key points with reasoning.\\n\\nContent:\"\n",
    "\n",
    "\n",
    "def summarize_text_from_summaries_prompt(content_type: str = \"an academic paper\") -> str:\n",
    "    \"\"\"Prompt for summarizing a paper from a list of summaries.\"\"\"\n",
    "    return f\"\"\"Write a summary collated from this collection of key points extracted from {content_type}.\n",
    "    The summary should highlight the core argument, conclusions and evidence, and answer the user's query.\n",
    "    User query: {{query}}\n",
    "    The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\n",
    "    Key points:\\n{{results}}\\nSummary:\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting prompts2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prompts2.py\n",
    "# this is a second version of things\n",
    "import concurrent\n",
    "import tempfile\n",
    "from typing import Generator, Union\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "from PyPDF2 import PdfReader\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hamilton.function_modifiers import config\n",
    "\n",
    "\n",
    "def summarize_chunk_of_text_prompt(content_type: str = \"an academic paper\") -> str:\n",
    "    \"\"\"Base prompt for summarizing chunks of text.\"\"\"\n",
    "    return f\"Summarize this text from {content_type}. Extract any key points with reasoning. Keep it concise. \\n\\nContent:\"\n",
    "\n",
    "\n",
    "def summarize_text_from_summaries_prompt(content_type: str = \"an academic paper\") -> str:\n",
    "    \"\"\"Prompt for summarizing a paper from a list of summaries.\"\"\"\n",
    "    return f\"\"\"Write a summary collated from this collection of key points extracted from {content_type}.\n",
    "    The summary should highlight the core argument, evidence and conclusions, and answer the user's query.\n",
    "    User query: {{query}}\n",
    "    The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\n",
    "    Key points:\\n{{results}}\\nSummary:\\nAnswer to query:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting summarization_shortened.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile summarization_shortened.py\n",
    "# modified module to accommodate\n",
    "import concurrent\n",
    "import tempfile\n",
    "from typing import Generator, Union\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "from PyPDF2 import PdfReader\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hamilton.function_modifiers import config\n",
    "\n",
    "\n",
    "@config.when(env=\"prod\")\n",
    "def raw_text__prod(pdf_source: Union[str, bytes, tempfile.SpooledTemporaryFile]) -> str:\n",
    "    \"\"\"Takes a filepath to a PDF and returns a string of the PDF's contents\n",
    "    :param pdf_source: the path, or the temporary file, to the PDF.\n",
    "    :return: the text of the PDF.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_source)\n",
    "    _pdf_text = \"\"\n",
    "    page_number = 0\n",
    "    for page in reader.pages:\n",
    "        page_number += 1\n",
    "        _pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n",
    "    return _pdf_text\n",
    "\n",
    "@config.when(env=\"dev\")\n",
    "def raw_text__dev() -> str:\n",
    "    \"\"\"Function for dev purposes.\n",
    "    \"\"\"\n",
    "    return \"\"\"\n",
    "    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. \n",
    "    Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. \n",
    "    Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. \n",
    "    Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def _create_chunks(text: str, n: int, tokenizer: tiktoken.Encoding) -> Generator[str, None, None]:\n",
    "    \"\"\"Helper function. Returns successive n-sized chunks from provided text.\n",
    "    Split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
    "    :param text:\n",
    "    :param n:\n",
    "    :param tokenizer:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
    "        j = min(i + int(1.5 * n), len(tokens))\n",
    "        while j > i + int(0.5 * n):\n",
    "            # Decode the tokens and check for full stop or newline\n",
    "            chunk = tokenizer.decode(tokens[i:j])\n",
    "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
    "                break\n",
    "            j -= 1\n",
    "        # If no end of sentence found, use n tokens as the chunk size\n",
    "        if j == i + int(0.5 * n):\n",
    "            j = min(i + n, len(tokens))\n",
    "        yield tokens[i:j]\n",
    "        i = j\n",
    "\n",
    "\n",
    "def chunked_text(\n",
    "    raw_text: str, tokenizer_encoding: str = \"cl100k_base\", max_token_length: int = 1500\n",
    ") -> list[str]:\n",
    "    \"\"\"Chunks the pdf text into smaller chunks of size max_token_length.\n",
    "    :param raw_text: the Series of individual pdf texts to chunk.\n",
    "    :param max_token_length: the maximum length of tokens in each chunk.\n",
    "    :param tokenizer_encoding: the encoding to use for the tokenizer.\n",
    "    :return: Series of chunked pdf text. Each element is a list of chunks.\n",
    "    \"\"\"\n",
    "    tokenizer = tiktoken.get_encoding(tokenizer_encoding)\n",
    "    _encoded_chunks = _create_chunks(raw_text, max_token_length, tokenizer)\n",
    "    _decoded_chunks = [tokenizer.decode(chunk) for chunk in _encoded_chunks]\n",
    "    return _decoded_chunks\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def _summarize_chunk(content: str, template_prompt: str, openai_gpt_model: str) -> str:\n",
    "    \"\"\"This function applies a prompt to some input content. In this case it returns a summarized chunk of text.\n",
    "    :param content: the content to summarize.\n",
    "    :param template_prompt: the prompt template to use to put the content into.\n",
    "    :param openai_gpt_model: the openai gpt model to use.\n",
    "    :return: the response from the openai API.\n",
    "    \"\"\"\n",
    "    prompt = template_prompt + content\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=openai_gpt_model, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def summarized_chunks(\n",
    "    chunked_text: list[str], summarize_chunk_of_text_prompt: str, openai_gpt_model: str\n",
    ") -> str:\n",
    "    \"\"\"Summarizes a series of chunks of text.\n",
    "    Note: this takes the first result from the top_n_related_articles series and summarizes it. This is because\n",
    "    the top_n_related_articles series is sorted by relatedness, so the first result is the most related.\n",
    "    :param chunked_text: a list of chunks of text for an article.\n",
    "    :param summarize_chunk_of_text_prompt:  the prompt to use to summarize each chunk of text.\n",
    "    :param openai_gpt_model: the openai gpt model to use.\n",
    "    :return: a single string of each chunk of text summarized, concatenated together.\n",
    "    \"\"\"\n",
    "    _summarized_text = \"\"\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=len(chunked_text)) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                _summarize_chunk, chunk, summarize_chunk_of_text_prompt, openai_gpt_model\n",
    "            )\n",
    "            for chunk in chunked_text\n",
    "        ]\n",
    "        with tqdm(total=len(chunked_text)) as pbar:\n",
    "            for _ in concurrent.futures.as_completed(futures):\n",
    "                pbar.update(1)\n",
    "        for future in futures:\n",
    "            data = future.result()\n",
    "            _summarized_text += data\n",
    "    return _summarized_text\n",
    "\n",
    "\n",
    "def prompt_and_text_content(\n",
    "    summarized_chunks: str,\n",
    "    summarize_text_from_summaries_prompt: str,\n",
    "    user_query: str,\n",
    ") -> str:\n",
    "    \"\"\"Creates the prompt for summarizing the text from the summarized chunks of the pdf.\n",
    "    :param summarized_chunks: a long string of chunked summaries of a file.\n",
    "    :param summarize_text_from_summaries_prompt: the template to use to summarize the chunks.\n",
    "    :param user_query: the original user query.\n",
    "    :return: the prompt to use to summarize the chunks.\n",
    "    \"\"\"\n",
    "    return summarize_text_from_summaries_prompt.format(query=user_query, results=summarized_chunks)\n",
    "\n",
    "\n",
    "def summarized_text(\n",
    "    prompt_and_text_content: object,\n",
    "    openai_gpt_model: str,\n",
    ") -> str:\n",
    "    \"\"\"Summarizes the text from the summarized chunks of the pdf.\n",
    "    :param prompt_and_text_content: the prompt and content to send over.\n",
    "    :param openai_gpt_model: which openai gpt model to use.\n",
    "    :return: the string response from the openai API.\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=openai_gpt_model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt_and_text_content,\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.0.5 (20230430.1635)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"652pt\" height=\"116pt\"\n",
       " viewBox=\"0.00 0.00 652.06 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 112)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-112 648.06,-112 648.06,4 -4,4\"/>\n",
       "<!-- env -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>env</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"186.11\" cy=\"-90\" rx=\"48.79\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"186.11\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: env</text>\n",
       "</g>\n",
       "<!-- content_type -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>content_type</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"336.11\" cy=\"-90\" rx=\"83.08\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"336.11\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: content_type</text>\n",
       "</g>\n",
       "<!-- summarize_text_from_summaries_prompt -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>summarize_text_from_summaries_prompt</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"171.11\" cy=\"-18\" rx=\"171.11\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"171.11\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarize_text_from_summaries_prompt</text>\n",
       "</g>\n",
       "<!-- content_type&#45;&gt;summarize_text_from_summaries_prompt -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>content_type&#45;&gt;summarize_text_from_summaries_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M299.5,-73.46C276.46,-63.69 246.44,-50.96 221.22,-40.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"222.67,-36.65 212.1,-35.96 219.93,-43.09 222.67,-36.65\"/>\n",
       "</g>\n",
       "<!-- summarize_chunk_of_text_prompt -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>summarize_chunk_of_text_prompt</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"502.11\" cy=\"-18\" rx=\"141.94\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"502.11\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarize_chunk_of_text_prompt</text>\n",
       "</g>\n",
       "<!-- content_type&#45;&gt;summarize_chunk_of_text_prompt -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>content_type&#45;&gt;summarize_chunk_of_text_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M372.96,-73.46C396.4,-63.58 427.04,-50.66 452.6,-39.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"453.62,-42.82 461.48,-35.71 450.9,-36.37 453.62,-42.82\"/>\n",
       "</g>\n",
       "<!-- file_type -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>file_type</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"505.11\" cy=\"-90\" rx=\"68.24\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"505.11\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: file_type</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1411fa4f0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can then view the DAG by modules -- here just the prompt one.\n",
    "\n",
    "%aimport prompts1\n",
    "%aimport prompts2\n",
    "%aimport summarization_shortened\n",
    "\n",
    "\n",
    "dw_tracker = adapters.DAGWorksTracker(\n",
    "   project_id=DAGWORKS_PROJECT_ID,\n",
    "   api_key=DAGWORKS_API_KEY,\n",
    "   username=DAGWORKS_PROJECT_EMAIL,\n",
    "   dag_name=\"prompts2\",\n",
    "   tags={\"env\": \"local\", \"where\": \"notebook\"}\n",
    ")\n",
    "\n",
    "dr = (\n",
    "    driver.Builder()\n",
    "    .with_config({\"file_type\": \"pdf\", \"env\": \"prod\"})  # DAG is now configuration based\n",
    "    .with_modules(prompts2)\n",
    "    .with_adapters(dw_tracker)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "dr.display_all_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.0.5 (20230430.1635)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"985pt\" height=\"332pt\"\n",
       " viewBox=\"0.00 0.00 984.68 332.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 328)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-328 980.68,-328 980.68,4 -4,4\"/>\n",
       "<!-- chunked_text -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>chunked_text</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"411.51\" cy=\"-234\" rx=\"60.56\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"411.51\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">chunked_text</text>\n",
       "</g>\n",
       "<!-- summarized_chunks -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>summarized_chunks</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"288.51\" cy=\"-162\" rx=\"87.69\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"288.51\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarized_chunks</text>\n",
       "</g>\n",
       "<!-- chunked_text&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>chunked_text&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M384.21,-217.46C367.34,-207.86 345.42,-195.39 326.82,-184.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"328.87,-181.37 318.44,-179.47 325.4,-187.46 328.87,-181.37\"/>\n",
       "</g>\n",
       "<!-- raw_text -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>raw_text</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"221.51\" cy=\"-306\" rx=\"67.73\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"221.51\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: raw_text</text>\n",
       "</g>\n",
       "<!-- raw_text&#45;&gt;chunked_text -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>raw_text&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M259.96,-290.83C290.09,-279.73 332.17,-264.23 364.34,-252.38\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"365.24,-255.41 373.41,-248.67 362.82,-248.84 365.24,-255.41\"/>\n",
       "</g>\n",
       "<!-- prompt_and_text_content -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>prompt_and_text_content</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"719.51\" cy=\"-90\" rx=\"106.11\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"719.51\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">prompt_and_text_content</text>\n",
       "</g>\n",
       "<!-- summarized_chunks&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>summarized_chunks&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M353.03,-149.41C363.54,-147.57 374.33,-145.71 384.51,-144 468.43,-129.93 564.36,-114.86 632,-104.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"632.26,-107.74 641.6,-102.76 631.19,-100.82 632.26,-107.74\"/>\n",
       "</g>\n",
       "<!-- summarized_text -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>summarized_text</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"786.51\" cy=\"-18\" rx=\"75.41\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"786.51\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarized_text</text>\n",
       "</g>\n",
       "<!-- prompt_and_text_content&#45;&gt;summarized_text -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>prompt_and_text_content&#45;&gt;summarized_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M735.73,-72.05C743.87,-63.54 753.9,-53.07 762.88,-43.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"765.05,-46.43 769.44,-36.79 760,-41.59 765.05,-46.43\"/>\n",
       "</g>\n",
       "<!-- openai_gpt_model -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>openai_gpt_model</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"727.51\" cy=\"-234\" rx=\"104.58\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"727.51\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: openai_gpt_model</text>\n",
       "</g>\n",
       "<!-- openai_gpt_model&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>openai_gpt_model&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M650.12,-221.54C580.55,-211.12 475.61,-195.1 384.51,-180 377.69,-178.87 370.6,-177.67 363.51,-176.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"364.22,-172.84 353.77,-174.58 363.02,-179.74 364.22,-172.84\"/>\n",
       "</g>\n",
       "<!-- openai_gpt_model&#45;&gt;summarized_text -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>openai_gpt_model&#45;&gt;summarized_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M817.4,-224.49C876.41,-216.82 945.99,-203.12 964.51,-180 1011.4,-121.46 904.34,-65.64 836.65,-37.66\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"838.14,-34.09 827.56,-33.58 835.51,-40.57 838.14,-34.09\"/>\n",
       "</g>\n",
       "<!-- summarize_text_from_summaries_prompt -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>summarize_text_from_summaries_prompt</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"589.51\" cy=\"-162\" rx=\"195.68\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"589.51\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: summarize_text_from_summaries_prompt</text>\n",
       "</g>\n",
       "<!-- summarize_text_from_summaries_prompt&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>summarize_text_from_summaries_prompt&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M621.31,-143.88C638.85,-134.43 660.8,-122.61 679.49,-112.55\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"680.78,-115.29 687.93,-107.47 677.47,-109.12 680.78,-115.29\"/>\n",
       "</g>\n",
       "<!-- summarize_chunk_of_text_prompt -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>summarize_chunk_of_text_prompt</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"166.51\" cy=\"-234\" rx=\"166.51\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"166.51\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: summarize_chunk_of_text_prompt</text>\n",
       "</g>\n",
       "<!-- summarize_chunk_of_text_prompt&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>summarize_chunk_of_text_prompt&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M196.35,-215.88C212.66,-206.52 233.03,-194.83 250.47,-184.83\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"251.88,-187.48 258.81,-179.47 248.4,-181.41 251.88,-187.48\"/>\n",
       "</g>\n",
       "<!-- user_query -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>user_query</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"879.51\" cy=\"-162\" rx=\"76.43\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"879.51\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: user_query</text>\n",
       "</g>\n",
       "<!-- user_query&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>user_query&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M844.4,-145.64C821.6,-135.66 791.6,-122.54 766.73,-111.66\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"768.33,-108.1 757.77,-107.3 765.53,-114.52 768.33,-108.1\"/>\n",
       "</g>\n",
       "<!-- max_token_length -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>max_token_length</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"411.51\" cy=\"-306\" rx=\"104.58\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"411.51\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: max_token_length</text>\n",
       "</g>\n",
       "<!-- max_token_length&#45;&gt;chunked_text -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>max_token_length&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M411.51,-287.7C411.51,-280.24 411.51,-271.32 411.51,-262.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"415.01,-263.1 411.51,-253.1 408.01,-263.1 415.01,-263.1\"/>\n",
       "</g>\n",
       "<!-- tokenizer_encoding -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>tokenizer_encoding</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"642.51\" cy=\"-306\" rx=\"108.16\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"642.51\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: tokenizer_encoding</text>\n",
       "</g>\n",
       "<!-- tokenizer_encoding&#45;&gt;chunked_text -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>tokenizer_encoding&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M591.81,-289.64C553.7,-278.09 501.65,-262.32 463.28,-250.69\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"464.42,-247.07 453.84,-247.52 462.39,-253.77 464.42,-247.07\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x141effeb0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we're viewing the module without prompts. Note the prompts are now inputs -- so you could pass them in directly here.\n",
    "dw_tracker = adapters.DAGWorksTracker(\n",
    "   project_id=DAGWORKS_PROJECT_ID,\n",
    "   api_key=DAGWORKS_API_KEY,\n",
    "   username=DAGWORKS_PROJECT_EMAIL,\n",
    "   dag_name=\"summarization_shortened\",\n",
    "   tags={\"env\": \"local\", \"where\": \"notebook\"}\n",
    ")\n",
    "\n",
    "dr = (\n",
    "    driver.Builder()\n",
    "    .with_config({})  # DAG is now configuration based\n",
    "    .with_modules(summarization_shortened)\n",
    "    .with_adapters(dw_tracker)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "dr.display_all_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.0.5 (20230430.1635)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"871pt\" height=\"332pt\"\n",
       " viewBox=\"0.00 0.00 870.84 332.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 328)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-328 866.84,-328 866.84,4 -4,4\"/>\n",
       "<!-- chunked_text -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>chunked_text</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"306.73\" cy=\"-234\" rx=\"60.56\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"306.73\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">chunked_text</text>\n",
       "</g>\n",
       "<!-- summarized_chunks -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>summarized_chunks</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"409.73\" cy=\"-162\" rx=\"87.69\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"409.73\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarized_chunks</text>\n",
       "</g>\n",
       "<!-- chunked_text&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>chunked_text&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M330.1,-217.12C343.77,-207.83 361.24,-195.95 376.34,-185.69\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"378.07,-188.07 384.37,-179.55 374.14,-182.28 378.07,-188.07\"/>\n",
       "</g>\n",
       "<!-- content_type -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>content_type</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"697.73\" cy=\"-306\" rx=\"83.08\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"697.73\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: content_type</text>\n",
       "</g>\n",
       "<!-- summarize_text_from_summaries_prompt -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>summarize_text_from_summaries_prompt</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"691.73\" cy=\"-162\" rx=\"171.11\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"691.73\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarize_text_from_summaries_prompt</text>\n",
       "</g>\n",
       "<!-- content_type&#45;&gt;summarize_text_from_summaries_prompt -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>content_type&#45;&gt;summarize_text_from_summaries_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M696.99,-287.59C695.98,-263.5 694.13,-219.75 692.91,-191.03\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"696.37,-190.95 692.45,-181.11 689.38,-191.25 696.37,-190.95\"/>\n",
       "</g>\n",
       "<!-- summarize_chunk_of_text_prompt -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>summarize_chunk_of_text_prompt</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"527.73\" cy=\"-234\" rx=\"141.94\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"527.73\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarize_chunk_of_text_prompt</text>\n",
       "</g>\n",
       "<!-- content_type&#45;&gt;summarize_chunk_of_text_prompt -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>content_type&#45;&gt;summarize_chunk_of_text_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M660.42,-289.64C636.24,-279.68 604.45,-266.59 578.05,-255.72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"579.8,-252.24 569.22,-251.67 577.13,-258.72 579.8,-252.24\"/>\n",
       "</g>\n",
       "<!-- prompt_and_text_content -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>prompt_and_text_content</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"333.73\" cy=\"-90\" rx=\"106.11\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"333.73\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">prompt_and_text_content</text>\n",
       "</g>\n",
       "<!-- summarized_chunks&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>summarized_chunks&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M391.33,-144.05C381.97,-135.43 370.43,-124.8 360.14,-115.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"362.95,-113.23 353.22,-109.03 358.21,-118.38 362.95,-113.23\"/>\n",
       "</g>\n",
       "<!-- raw_text -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>raw_text</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"67.73\" cy=\"-306\" rx=\"67.73\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"67.73\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: raw_text</text>\n",
       "</g>\n",
       "<!-- raw_text&#45;&gt;chunked_text -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>raw_text&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M112.12,-292C152.32,-280.22 211.58,-262.87 254.14,-250.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"254.96,-253.52 263.58,-247.35 253,-246.8 254.96,-253.52\"/>\n",
       "</g>\n",
       "<!-- summarized_text -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>summarized_text</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"190.73\" cy=\"-18\" rx=\"75.41\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"190.73\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarized_text</text>\n",
       "</g>\n",
       "<!-- prompt_and_text_content&#45;&gt;summarized_text -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>prompt_and_text_content&#45;&gt;summarized_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M300.2,-72.59C279.96,-62.68 254,-49.97 232.45,-39.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"234.09,-35.84 223.57,-34.59 231.02,-42.13 234.09,-35.84\"/>\n",
       "</g>\n",
       "<!-- openai_gpt_model -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>openai_gpt_model</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"123.73\" cy=\"-234\" rx=\"104.58\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"123.73\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: openai_gpt_model</text>\n",
       "</g>\n",
       "<!-- openai_gpt_model&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>openai_gpt_model&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M182.3,-218.67C229.49,-207.11 295.81,-190.88 344.66,-178.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"345.24,-182.14 354.12,-176.37 343.57,-175.34 345.24,-182.14\"/>\n",
       "</g>\n",
       "<!-- openai_gpt_model&#45;&gt;summarized_text -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>openai_gpt_model&#45;&gt;summarized_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M126.59,-215.74C129.77,-197.66 135.45,-168.54 142.73,-144 152.97,-109.48 168.39,-70.98 178.97,-45.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"182.47,-47.67 183.18,-37.1 176.03,-44.92 182.47,-47.67\"/>\n",
       "</g>\n",
       "<!-- summarize_text_from_summaries_prompt&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>summarize_text_from_summaries_prompt&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M613.16,-145.64C553.46,-133.96 471.71,-117.98 412.06,-106.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"413.05,-102.75 402.56,-104.26 411.71,-109.62 413.05,-102.75\"/>\n",
       "</g>\n",
       "<!-- summarize_chunk_of_text_prompt&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>summarize_chunk_of_text_prompt&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M499.16,-216.05C483.4,-206.7 463.63,-194.98 446.7,-184.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"449.01,-181.64 438.62,-179.55 445.44,-187.66 449.01,-181.64\"/>\n",
       "</g>\n",
       "<!-- user_query -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>user_query</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"227.73\" cy=\"-162\" rx=\"76.43\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"227.73\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: user_query</text>\n",
       "</g>\n",
       "<!-- user_query&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>user_query&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M252.58,-144.59C266.47,-135.42 283.98,-123.85 299.18,-113.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"300.88,-116.23 307.3,-107.8 297.02,-110.39 300.88,-116.23\"/>\n",
       "</g>\n",
       "<!-- max_token_length -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>max_token_length</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"257.73\" cy=\"-306\" rx=\"104.58\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"257.73\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: max_token_length</text>\n",
       "</g>\n",
       "<!-- max_token_length&#45;&gt;chunked_text -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>max_token_length&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M269.84,-287.7C275.48,-279.64 282.3,-269.89 288.54,-260.98\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"291.89,-263.3 294.76,-253.1 286.15,-259.29 291.89,-263.3\"/>\n",
       "</g>\n",
       "<!-- tokenizer_encoding -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>tokenizer_encoding</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"488.73\" cy=\"-306\" rx=\"108.16\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"488.73\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: tokenizer_encoding</text>\n",
       "</g>\n",
       "<!-- tokenizer_encoding&#45;&gt;chunked_text -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>tokenizer_encoding&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M446.97,-288.94C418.81,-278.11 381.64,-263.81 352.65,-252.66\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"354.35,-249.18 343.76,-248.86 351.83,-255.71 354.35,-249.18\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x141156280>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as you see here, Hamilton stitched together the two modules and formed a larger DAG.\n",
    "dw_tracker = adapters.DAGWorksTracker(\n",
    "   project_id=DAGWORKS_PROJECT_ID,\n",
    "   api_key=DAGWORKS_API_KEY,\n",
    "   username=DAGWORKS_PROJECT_EMAIL,\n",
    "   dag_name=\"prompt1_and_summarization_shortened\",\n",
    "   tags={\"env\": \"local\", \"where\": \"notebook\"}\n",
    ")\n",
    "dr = (\n",
    "    driver.Builder()\n",
    "    .with_config({})  # DAG is now configuration based\n",
    "    .with_modules(prompts1, summarization_shortened)\n",
    "    .with_adapters(dw_tracker)\n",
    "    .build()\n",
    ")\n",
    "dr.display_all_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.0.5 (20230430.1635)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"871pt\" height=\"332pt\"\n",
       " viewBox=\"0.00 0.00 870.84 332.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 328)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-328 866.84,-328 866.84,4 -4,4\"/>\n",
       "<!-- chunked_text -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>chunked_text</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"306.73\" cy=\"-234\" rx=\"60.56\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"306.73\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">chunked_text</text>\n",
       "</g>\n",
       "<!-- summarized_chunks -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>summarized_chunks</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"409.73\" cy=\"-162\" rx=\"87.69\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"409.73\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarized_chunks</text>\n",
       "</g>\n",
       "<!-- chunked_text&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>chunked_text&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M330.1,-217.12C343.77,-207.83 361.24,-195.95 376.34,-185.69\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"378.07,-188.07 384.37,-179.55 374.14,-182.28 378.07,-188.07\"/>\n",
       "</g>\n",
       "<!-- content_type -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>content_type</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"697.73\" cy=\"-306\" rx=\"83.08\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"697.73\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: content_type</text>\n",
       "</g>\n",
       "<!-- summarize_text_from_summaries_prompt -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>summarize_text_from_summaries_prompt</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"691.73\" cy=\"-162\" rx=\"171.11\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"691.73\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarize_text_from_summaries_prompt</text>\n",
       "</g>\n",
       "<!-- content_type&#45;&gt;summarize_text_from_summaries_prompt -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>content_type&#45;&gt;summarize_text_from_summaries_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M696.99,-287.59C695.98,-263.5 694.13,-219.75 692.91,-191.03\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"696.37,-190.95 692.45,-181.11 689.38,-191.25 696.37,-190.95\"/>\n",
       "</g>\n",
       "<!-- summarize_chunk_of_text_prompt -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>summarize_chunk_of_text_prompt</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"527.73\" cy=\"-234\" rx=\"141.94\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"527.73\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarize_chunk_of_text_prompt</text>\n",
       "</g>\n",
       "<!-- content_type&#45;&gt;summarize_chunk_of_text_prompt -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>content_type&#45;&gt;summarize_chunk_of_text_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M660.42,-289.64C636.24,-279.68 604.45,-266.59 578.05,-255.72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"579.8,-252.24 569.22,-251.67 577.13,-258.72 579.8,-252.24\"/>\n",
       "</g>\n",
       "<!-- prompt_and_text_content -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>prompt_and_text_content</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"333.73\" cy=\"-90\" rx=\"106.11\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"333.73\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">prompt_and_text_content</text>\n",
       "</g>\n",
       "<!-- summarized_chunks&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>summarized_chunks&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M391.33,-144.05C381.97,-135.43 370.43,-124.8 360.14,-115.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"362.95,-113.23 353.22,-109.03 358.21,-118.38 362.95,-113.23\"/>\n",
       "</g>\n",
       "<!-- raw_text -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>raw_text</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"67.73\" cy=\"-306\" rx=\"67.73\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"67.73\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: raw_text</text>\n",
       "</g>\n",
       "<!-- raw_text&#45;&gt;chunked_text -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>raw_text&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M112.12,-292C152.32,-280.22 211.58,-262.87 254.14,-250.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"254.96,-253.52 263.58,-247.35 253,-246.8 254.96,-253.52\"/>\n",
       "</g>\n",
       "<!-- summarized_text -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>summarized_text</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"190.73\" cy=\"-18\" rx=\"75.41\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"190.73\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarized_text</text>\n",
       "</g>\n",
       "<!-- prompt_and_text_content&#45;&gt;summarized_text -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>prompt_and_text_content&#45;&gt;summarized_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M300.2,-72.59C279.96,-62.68 254,-49.97 232.45,-39.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"234.09,-35.84 223.57,-34.59 231.02,-42.13 234.09,-35.84\"/>\n",
       "</g>\n",
       "<!-- openai_gpt_model -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>openai_gpt_model</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"123.73\" cy=\"-234\" rx=\"104.58\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"123.73\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: openai_gpt_model</text>\n",
       "</g>\n",
       "<!-- openai_gpt_model&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>openai_gpt_model&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M182.3,-218.67C229.49,-207.11 295.81,-190.88 344.66,-178.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"345.24,-182.14 354.12,-176.37 343.57,-175.34 345.24,-182.14\"/>\n",
       "</g>\n",
       "<!-- openai_gpt_model&#45;&gt;summarized_text -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>openai_gpt_model&#45;&gt;summarized_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M126.59,-215.74C129.77,-197.66 135.45,-168.54 142.73,-144 152.97,-109.48 168.39,-70.98 178.97,-45.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"182.47,-47.67 183.18,-37.1 176.03,-44.92 182.47,-47.67\"/>\n",
       "</g>\n",
       "<!-- summarize_text_from_summaries_prompt&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>summarize_text_from_summaries_prompt&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M613.16,-145.64C553.46,-133.96 471.71,-117.98 412.06,-106.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"413.05,-102.75 402.56,-104.26 411.71,-109.62 413.05,-102.75\"/>\n",
       "</g>\n",
       "<!-- summarize_chunk_of_text_prompt&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>summarize_chunk_of_text_prompt&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M499.16,-216.05C483.4,-206.7 463.63,-194.98 446.7,-184.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"449.01,-181.64 438.62,-179.55 445.44,-187.66 449.01,-181.64\"/>\n",
       "</g>\n",
       "<!-- user_query -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>user_query</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"227.73\" cy=\"-162\" rx=\"76.43\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"227.73\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: user_query</text>\n",
       "</g>\n",
       "<!-- user_query&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>user_query&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M252.58,-144.59C266.47,-135.42 283.98,-123.85 299.18,-113.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"300.88,-116.23 307.3,-107.8 297.02,-110.39 300.88,-116.23\"/>\n",
       "</g>\n",
       "<!-- max_token_length -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>max_token_length</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"257.73\" cy=\"-306\" rx=\"104.58\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"257.73\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: max_token_length</text>\n",
       "</g>\n",
       "<!-- max_token_length&#45;&gt;chunked_text -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>max_token_length&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M269.84,-287.7C275.48,-279.64 282.3,-269.89 288.54,-260.98\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"291.89,-263.3 294.76,-253.1 286.15,-259.29 291.89,-263.3\"/>\n",
       "</g>\n",
       "<!-- tokenizer_encoding -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>tokenizer_encoding</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"488.73\" cy=\"-306\" rx=\"108.16\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"488.73\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: tokenizer_encoding</text>\n",
       "</g>\n",
       "<!-- tokenizer_encoding&#45;&gt;chunked_text -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>tokenizer_encoding&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M446.97,-288.94C418.81,-278.11 381.64,-263.81 352.65,-252.66\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"354.35,-249.18 343.76,-248.86 351.83,-255.71 354.35,-249.18\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x141f24a30>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same thing again, but showing prompts2 is swappable for prompts1.\n",
    "dw_tracker = adapters.DAGWorksTracker(\n",
    "   project_id=DAGWORKS_PROJECT_ID,\n",
    "   api_key=DAGWORKS_API_KEY,\n",
    "   username=DAGWORKS_PROJECT_EMAIL,\n",
    "   dag_name=\"prompt2_and_summarization_shortened\",\n",
    "   tags={\"env\": \"local\", \"where\": \"notebook\"}\n",
    ")\n",
    "dr = (\n",
    "    driver.Builder()\n",
    "    .with_config({})  # DAG is now configuration based\n",
    "    .with_modules(prompts2, summarization_shortened)\n",
    "    .with_adapters(dw_tracker)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "dr.display_all_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
