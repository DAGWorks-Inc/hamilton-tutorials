{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T05:42:10.572770Z",
     "start_time": "2023-08-08T05:42:10.566208Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Uncomment and run the cell below if you are in a Google Colab environment. It will:\n",
    "\n",
    "1. Mount google drive. You will be asked to authenticate and give permissions.\n",
    "2. Change directory to google drive.\n",
    "3. Make a directory \"hamilton-tutorials\"\n",
    "4. Change directory to it.\n",
    "5. Clone this repository to your google drive\n",
    "6. Move your current directory to the example\n",
    "7. Install requirements.\n",
    "8. This means that any modifications will be saved, and you won't lose them if you close your browser."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## 1. Mount google drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "## 2. Change directory to google drive.\n",
    "# %cd /content/drive/MyDrive\n",
    "## 3. Make a directory \"hamilton-tutorials\"\n",
    "# !mkdir hamilton-tutorials\n",
    "## 4. Change directory to it.\n",
    "# %cd hamilton-tutorials\n",
    "## 5. Clone this repository to your google drive\n",
    "# !git clone https://github.com/DAGWorks-Inc/hamilton-tutorials/\n",
    "## 6. Move your current directory to the example\n",
    "# %cd hamilton-tutorials/2023-10-09/pdf_summarizer\n",
    "## 7. Install requirements.\n",
    "# %pip install -r requirements.txt\n",
    "# clear_output()  # optionally clear outputs\n",
    "## To check your current working directory you can type `!pwd` in a cell and run it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# set your openai API key\n",
    "# import openai\n",
    "# openai.api_key = \"YOUR_KEY_HERE\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T04:08:44.701939Z",
     "start_time": "2023-08-08T04:08:44.695171Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting summarization.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile summarization.py\n",
    "\n",
    "import io\n",
    "import concurrent\n",
    "from typing import Generator\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "from PyPDF2 import PdfReader\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hamilton.function_modifiers import config\n",
    "\n",
    "\n",
    "def summarize_chunk_of_text_prompt(content_type: str = \"an academic paper\") -> str:\n",
    "    \"\"\"Base prompt for summarizing chunks of text.\"\"\"\n",
    "    return f\"Summarize this text from {content_type}. Extract any key points with reasoning.\\n\\nContent:\"\n",
    "\n",
    "\n",
    "def summarize_text_from_summaries_prompt(content_type: str = \"an academic paper\") -> str:\n",
    "    \"\"\"Prompt for summarizing a paper from a list of summaries.\"\"\"\n",
    "    return f\"\"\"Write a summary collated from this collection of key points extracted from {content_type}.\n",
    "    The summary should highlight the core argument, conclusions and evidence, and answer the user's query.\n",
    "    User query: {{query}}\n",
    "    The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\n",
    "    Key points:\\n{{results}}\\nSummary:\\n\"\"\"\n",
    "\n",
    "\n",
    "@config.when(file_type=\"pdf\")\n",
    "def raw_text(pdf_source: io.BufferedReader) -> str:\n",
    "    \"\"\"Takes a filepath to a PDF and returns a string of the PDF's contents\n",
    "    :param pdf_source: Series of filepaths to PDFs\n",
    "    :return: Series of strings of the PDFs' contents\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_source)\n",
    "    _pdf_text = \"\"\n",
    "    page_number = 0\n",
    "    for page in reader.pages:\n",
    "        page_number += 1\n",
    "        _pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n",
    "    return _pdf_text\n",
    "\n",
    "\n",
    "def _create_chunks(text: str, n: int, tokenizer: tiktoken.Encoding) -> Generator[str, None, None]:\n",
    "    \"\"\"Helper function. Returns successive n-sized chunks from provided text.\n",
    "    Split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
    "    :param text:\n",
    "    :param n:\n",
    "    :param tokenizer:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
    "        j = min(i + int(1.5 * n), len(tokens))\n",
    "        while j > i + int(0.5 * n):\n",
    "            # Decode the tokens and check for full stop or newline\n",
    "            chunk = tokenizer.decode(tokens[i:j])\n",
    "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
    "                break\n",
    "            j -= 1\n",
    "        # If no end of sentence found, use n tokens as the chunk size\n",
    "        if j == i + int(0.5 * n):\n",
    "            j = min(i + n, len(tokens))\n",
    "        yield tokens[i:j]\n",
    "        i = j\n",
    "\n",
    "\n",
    "def chunked_text(\n",
    "    raw_text: str, max_token_length: int = 1500, tokenizer_encoding: str = \"cl100k_base\"\n",
    ") -> list[str]:\n",
    "    \"\"\"Chunks the pdf text into smaller chunks of size max_token_length.\n",
    "    :param pdf_text: the Series of individual pdf texts to chunk.\n",
    "    :param max_token_length: the maximum length of tokens in each chunk.\n",
    "    :param tokenizer_encoding: the encoding to use for the tokenizer.\n",
    "    :return: Series of chunked pdf text. Each element is a list of chunks.\n",
    "    \"\"\"\n",
    "    tokenizer = tiktoken.get_encoding(tokenizer_encoding)\n",
    "    _encoded_chunks = _create_chunks(raw_text, max_token_length, tokenizer)\n",
    "    _decoded_chunks = [tokenizer.decode(chunk) for chunk in _encoded_chunks]\n",
    "    return _decoded_chunks\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def _summarize_chunk(content: str, template_prompt: str, openai_gpt_model: str) -> str:\n",
    "    \"\"\"This function applies a prompt to some input content. In this case it returns a summarized chunk of text.\n",
    "    :param content: the content to summarize.\n",
    "    :param template_prompt: the prompt template to use to put the content into.\n",
    "    :param openai_gpt_model: the openai gpt model to use.\n",
    "    :return: the response from the openai API.\n",
    "    \"\"\"\n",
    "    prompt = template_prompt + content\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=openai_gpt_model, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def summarized_chunks(\n",
    "    chunked_text: list[str], summarize_chunk_of_text_prompt: str, openai_gpt_model: str\n",
    ") -> str:\n",
    "    \"\"\"Summarizes a series of chunks of text.\n",
    "    Note: this takes the first result from the top_n_related_articles series and summarizes it. This is because\n",
    "    the top_n_related_articles series is sorted by relatedness, so the first result is the most related.\n",
    "    :param top_n_related_articles: series with each entry being a list of chunks of text for an article.\n",
    "    :param summarize_chunk_of_text_prompt:  the prompt to use to summarize each chunk of text.\n",
    "    :param openai_gpt_model: the openai gpt model to use.\n",
    "    :return: a single string of each chunk of text summarized, concatenated together.\n",
    "    \"\"\"\n",
    "    _summarized_text = \"\"\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=len(chunked_text)) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                _summarize_chunk, chunk, summarize_chunk_of_text_prompt, openai_gpt_model\n",
    "            )\n",
    "            for chunk in chunked_text\n",
    "        ]\n",
    "        with tqdm(total=len(chunked_text)) as pbar:\n",
    "            for _ in concurrent.futures.as_completed(futures):\n",
    "                pbar.update(1)\n",
    "        for future in futures:\n",
    "            data = future.result()\n",
    "            _summarized_text += data\n",
    "    return _summarized_text\n",
    "\n",
    "\n",
    "def prompt_and_text_content(\n",
    "    summarize_text_from_summaries_prompt: str, user_query: str, summarized_chunks: str\n",
    ") -> str:\n",
    "    \"\"\"Creates the prompt for summarizing the text from the summarized chunks of the pdf.\n",
    "    :param summarize_text_from_summaries_prompt: the template to use to summarize the chunks.\n",
    "    :param user_query: the original user query.\n",
    "    :param summarized_chunks: a long string of chunked summaries of a file.\n",
    "    :return: the prompt to use to summarize the chunks.\n",
    "    \"\"\"\n",
    "    return summarize_text_from_summaries_prompt.format(query=user_query, results=summarized_chunks)\n",
    "\n",
    "\n",
    "def summarized_text(\n",
    "    prompt_and_text_content: str,\n",
    "    openai_gpt_model: str,\n",
    ") -> str:\n",
    "    \"\"\"Summarizes the text from the summarized chunks of the pdf.\n",
    "    :param prompt_and_text_content: the prompt and content to send over.\n",
    "    :param openai_gpt_model: which openai gpt model to use.\n",
    "    :return: the string response from the openai API.\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=openai_gpt_model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt_and_text_content,\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T21:00:12.871059Z",
     "start_time": "2023-08-07T21:00:12.866449Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 8.0.5 (20230430.1635)\n -->\n<!-- Pages: 1 -->\n<svg width=\"915pt\" height=\"404pt\"\n viewBox=\"0.00 0.00 915.01 404.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 400)\">\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-400 911.01,-400 911.01,4 -4,4\"/>\n<!-- summarize_text_from_summaries_prompt -->\n<g id=\"node1\" class=\"node\">\n<title>summarize_text_from_summaries_prompt</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"564.58\" cy=\"-162\" rx=\"171.11\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"564.58\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarize_text_from_summaries_prompt</text>\n</g>\n<!-- prompt_and_text_content -->\n<g id=\"node12\" class=\"node\">\n<title>prompt_and_text_content</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"401.58\" cy=\"-90\" rx=\"106.11\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"401.58\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">prompt_and_text_content</text>\n</g>\n<!-- summarize_text_from_summaries_prompt&#45;&gt;prompt_and_text_content -->\n<g id=\"edge12\" class=\"edge\">\n<title>summarize_text_from_summaries_prompt&#45;&gt;prompt_and_text_content</title>\n<path fill=\"none\" stroke=\"black\" d=\"M525.12,-144.05C502.29,-134.25 473.38,-121.84 449.28,-111.48\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"450.84,-107.92 440.27,-107.19 448.08,-114.35 450.84,-107.92\"/>\n</g>\n<!-- pdf_source -->\n<g id=\"node2\" class=\"node\">\n<title>pdf_source</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"287.58\" cy=\"-378\" rx=\"76.43\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"287.58\" y=\"-372.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: pdf_source</text>\n</g>\n<!-- raw_text -->\n<g id=\"node8\" class=\"node\">\n<title>raw_text</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"287.58\" cy=\"-306\" rx=\"43.16\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"287.58\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">raw_text</text>\n</g>\n<!-- pdf_source&#45;&gt;raw_text -->\n<g id=\"edge6\" class=\"edge\">\n<title>pdf_source&#45;&gt;raw_text</title>\n<path fill=\"none\" stroke=\"black\" d=\"M287.58,-359.7C287.58,-352.24 287.58,-343.32 287.58,-334.97\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"291.08,-335.1 287.58,-325.1 284.08,-335.1 291.08,-335.1\"/>\n</g>\n<!-- tokenizer_encoding -->\n<g id=\"node3\" class=\"node\">\n<title>tokenizer_encoding</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"118.58\" cy=\"-306\" rx=\"108.16\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"118.58\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: tokenizer_encoding</text>\n</g>\n<!-- chunked_text -->\n<g id=\"node6\" class=\"node\">\n<title>chunked_text</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"287.58\" cy=\"-234\" rx=\"60.56\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"287.58\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">chunked_text</text>\n</g>\n<!-- tokenizer_encoding&#45;&gt;chunked_text -->\n<g id=\"edge4\" class=\"edge\">\n<title>tokenizer_encoding&#45;&gt;chunked_text</title>\n<path fill=\"none\" stroke=\"black\" d=\"M157.78,-288.76C183.26,-278.21 216.54,-264.42 243.02,-253.46\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"244.28,-256.31 252.18,-249.25 241.6,-249.84 244.28,-256.31\"/>\n</g>\n<!-- content_type -->\n<g id=\"node4\" class=\"node\">\n<title>content_type</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"669.58\" cy=\"-306\" rx=\"83.08\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"669.58\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: content_type</text>\n</g>\n<!-- content_type&#45;&gt;summarize_text_from_summaries_prompt -->\n<g id=\"edge1\" class=\"edge\">\n<title>content_type&#45;&gt;summarize_text_from_summaries_prompt</title>\n<path fill=\"none\" stroke=\"black\" d=\"M672.04,-287.7C673.83,-268.53 673.87,-237.49 659.58,-216 650.74,-202.71 637.39,-192.42 623.46,-184.6\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"625.27,-181.07 614.78,-179.57 622.04,-187.28 625.27,-181.07\"/>\n</g>\n<!-- summarize_chunk_of_text_prompt -->\n<g id=\"node7\" class=\"node\">\n<title>summarize_chunk_of_text_prompt</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"508.58\" cy=\"-234\" rx=\"141.94\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"508.58\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarize_chunk_of_text_prompt</text>\n</g>\n<!-- content_type&#45;&gt;summarize_chunk_of_text_prompt -->\n<g id=\"edge5\" class=\"edge\">\n<title>content_type&#45;&gt;summarize_chunk_of_text_prompt</title>\n<path fill=\"none\" stroke=\"black\" d=\"M633.85,-289.46C611.21,-279.62 581.65,-266.77 556.93,-256.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"558.59,-252.49 548.02,-251.71 555.8,-258.91 558.59,-252.49\"/>\n</g>\n<!-- file_type -->\n<g id=\"node5\" class=\"node\">\n<title>file_type</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"450.58\" cy=\"-378\" rx=\"68.24\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"450.58\" y=\"-372.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: file_type</text>\n</g>\n<!-- summarized_chunks -->\n<g id=\"node11\" class=\"node\">\n<title>summarized_chunks</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"287.58\" cy=\"-162\" rx=\"87.69\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"287.58\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarized_chunks</text>\n</g>\n<!-- chunked_text&#45;&gt;summarized_chunks -->\n<g id=\"edge9\" class=\"edge\">\n<title>chunked_text&#45;&gt;summarized_chunks</title>\n<path fill=\"none\" stroke=\"black\" d=\"M287.58,-215.7C287.58,-208.24 287.58,-199.32 287.58,-190.97\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"291.08,-191.1 287.58,-181.1 284.08,-191.1 291.08,-191.1\"/>\n</g>\n<!-- summarize_chunk_of_text_prompt&#45;&gt;summarized_chunks -->\n<g id=\"edge10\" class=\"edge\">\n<title>summarize_chunk_of_text_prompt&#45;&gt;summarized_chunks</title>\n<path fill=\"none\" stroke=\"black\" d=\"M457.32,-216.76C423.43,-206.03 378.99,-191.95 344.08,-180.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"345.63,-177.4 335.04,-177.72 343.52,-184.07 345.63,-177.4\"/>\n</g>\n<!-- raw_text&#45;&gt;chunked_text -->\n<g id=\"edge2\" class=\"edge\">\n<title>raw_text&#45;&gt;chunked_text</title>\n<path fill=\"none\" stroke=\"black\" d=\"M287.58,-287.7C287.58,-280.24 287.58,-271.32 287.58,-262.97\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"291.08,-263.1 287.58,-253.1 284.08,-263.1 291.08,-263.1\"/>\n</g>\n<!-- summarized_text -->\n<g id=\"node9\" class=\"node\">\n<title>summarized_text</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"205.58\" cy=\"-18\" rx=\"75.41\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"205.58\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarized_text</text>\n</g>\n<!-- max_token_length -->\n<g id=\"node10\" class=\"node\">\n<title>max_token_length</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"453.58\" cy=\"-306\" rx=\"104.58\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"453.58\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: max_token_length</text>\n</g>\n<!-- max_token_length&#45;&gt;chunked_text -->\n<g id=\"edge3\" class=\"edge\">\n<title>max_token_length&#45;&gt;chunked_text</title>\n<path fill=\"none\" stroke=\"black\" d=\"M415.5,-288.94C390.48,-278.39 357.68,-264.56 331.56,-253.55\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"333.12,-249.98 322.54,-249.32 330.4,-256.43 333.12,-249.98\"/>\n</g>\n<!-- summarized_chunks&#45;&gt;prompt_and_text_content -->\n<g id=\"edge14\" class=\"edge\">\n<title>summarized_chunks&#45;&gt;prompt_and_text_content</title>\n<path fill=\"none\" stroke=\"black\" d=\"M314.6,-144.41C329.68,-135.15 348.67,-123.49 365.06,-113.42\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"366.67,-115.93 373.36,-107.71 363.01,-109.96 366.67,-115.93\"/>\n</g>\n<!-- prompt_and_text_content&#45;&gt;summarized_text -->\n<g id=\"edge7\" class=\"edge\">\n<title>prompt_and_text_content&#45;&gt;summarized_text</title>\n<path fill=\"none\" stroke=\"black\" d=\"M357.59,-73.29C327.68,-62.61 288,-48.44 256.68,-37.25\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"258,-33.65 247.41,-33.58 255.65,-40.24 258,-33.65\"/>\n</g>\n<!-- user_query -->\n<g id=\"node13\" class=\"node\">\n<title>user_query</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"830.58\" cy=\"-162\" rx=\"76.43\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"830.58\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: user_query</text>\n</g>\n<!-- user_query&#45;&gt;prompt_and_text_content -->\n<g id=\"edge13\" class=\"edge\">\n<title>user_query&#45;&gt;prompt_and_text_content</title>\n<path fill=\"none\" stroke=\"black\" d=\"M774.21,-149.51C764.34,-147.6 754.17,-145.69 744.58,-144 659.1,-128.95 561.22,-114.06 491.96,-103.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"492.49,-100.31 482.09,-102.32 491.48,-107.23 492.49,-100.31\"/>\n</g>\n<!-- openai_gpt_model -->\n<g id=\"node14\" class=\"node\">\n<title>openai_gpt_model</title>\n<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"104.58\" cy=\"-234\" rx=\"104.58\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"104.58\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: openai_gpt_model</text>\n</g>\n<!-- openai_gpt_model&#45;&gt;summarized_text -->\n<g id=\"edge8\" class=\"edge\">\n<title>openai_gpt_model&#45;&gt;summarized_text</title>\n<path fill=\"none\" stroke=\"black\" d=\"M112.67,-215.85C130.18,-178.75 171.69,-90.81 192.79,-46.1\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"196.34,-47.77 197.45,-37.23 190.01,-44.78 196.34,-47.77\"/>\n</g>\n<!-- openai_gpt_model&#45;&gt;summarized_chunks -->\n<g id=\"edge11\" class=\"edge\">\n<title>openai_gpt_model&#45;&gt;summarized_chunks</title>\n<path fill=\"none\" stroke=\"black\" d=\"M146.1,-217.12C173.15,-206.77 208.61,-193.21 237.26,-182.25\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"238.39,-185.18 246.48,-178.34 235.89,-178.64 238.39,-185.18\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": "<graphviz.graphs.Digraph at 0x12c189450>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # run as a script to test Hamilton's execution\n",
    "import summarization\n",
    "import importlib\n",
    "importlib.reload(summarization)\n",
    "\n",
    "from hamilton import base, driver\n",
    "\n",
    "dr = driver.Driver(\n",
    "    {\"file_type\": \"pdf\"},\n",
    "    summarization,\n",
    "    adapter=base.SimplePythonGraphAdapter(base.DictResult()),\n",
    ")\n",
    "dr.display_all_functions()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T21:00:13.895749Z",
     "start_time": "2023-08-07T21:00:13.601726Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# pull in a pdf\n",
    "import requests\n",
    "\n",
    "def download_file(url, filename):\n",
    "  response = requests.get(url, stream=True)\n",
    "  if response.status_code == 200:\n",
    "    with open(filename, 'wb') as fd:\n",
    "      for chunk in response.iter_content(chunk_size=1024):\n",
    "        fd.write(chunk)\n",
    "\n",
    "download_file(\"https://cdmsworkshop.github.io/2022/Proceedings/ShortPapers/Paper6_StefanKrawczyk.pdf\", \"hamilton_paper.pdf\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T21:00:15.087385Z",
     "start_time": "2023-08-07T21:00:15.021201Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core Argument:\n",
      "- The Hamilton framework is a high-level modeling approach for dataflows that simplifies the user experience for data scientists and provides a unified interface for describing end-to-end dataflows.\n",
      "- Traditional ETL approaches at Stitch Fix had problems such as poorly documented code, low unit test coverage, limited code reuse, and difficulty in changing underlying infrastructure, which led to the development of the Hamilton framework.\n",
      "\n",
      "Evidence:\n",
      "- The Hamilton framework has been used to scale modeling dataflows at Stitch Fix to support over 4000 data transformations without impacting team and user productivity.\n",
      "- The Hamilton programming paradigm encourages the use of vector computation, improves code readability and documentation, and allows for easy unit testing.\n",
      "- Hamilton provides decorators to encapsulate operational concerns and reduce repetitive function logic.\n",
      "- The function DAG is the framework's representation of the nodes that should be executed and the dependencies between them.\n",
      "- The driver code in Hamilton steers the execution of the function DAG and provides a convenient abstraction layer for users.\n",
      "\n",
      "Conclusions:\n",
      "- The Hamilton framework provides a simpler user experience for data scientists and allows for easy integration with existing data management tooling in a modular fashion.\n",
      "- Hamilton improves code readability, modularity, and reusability in data management systems.\n",
      "- Hamilton offers benefits such as incremental development, debugging capabilities, transparent scaling, lineage tracking, and modular components.\n",
      "- Adoption of Hamilton has been successful among teams with active feature development for time-series forecasting and those using Pandas.\n",
      "- Future extensions for Hamilton include integrating with data governance tools and compiling to an orchestration framework.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"hamilton_paper.pdf\", \"rb\") as f:\n",
    "    result = dr.execute([\"summarized_text\"], inputs={\n",
    "        \"pdf_source\": f,\n",
    "        \"openai_gpt_model\": \"gpt-3.5-turbo-0613\",\n",
    "        \"content_type\": \"Scientific article\",\n",
    "        \"user_query\": \"Can you ELI5 the paper?\"\n",
    "    })\n",
    "print(result[\"summarized_text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T21:02:06.537036Z",
     "start_time": "2023-08-07T21:02:06.529040Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run and see on DAGWorks Platform\n",
    "This is what you'd change to see runs on the DAGWorks platform instead."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from dagworks import driver as dw_driver\n",
    "from hamilton import base\n",
    "import summarization\n",
    "# import openai # set this if you didn't set it above already\n",
    "# openai.api_key = \"YOUR API KEY HERE\"\n",
    "# Get these when you create an account and set up a project.\n",
    "DAGWORKS_API_KEY = \"YOUR API KEY HERE\"\n",
    "DAGWORKS_PROJECT_ID = 66  # Change this to be your project ID\n",
    "DAGWORKS_PROJECT_EMAIL = \"stefan@dagworks.io\"  # Change this to your email\n",
    "dwdr = dw_driver.Driver(\n",
    "    {\"file_type\": \"pdf\"},\n",
    "    summarization,  # python module containing function logic\n",
    "    adapter=base.DefaultAdapter(),\n",
    "    project_id=DAGWORKS_PROJECT_ID,\n",
    "    api_key=DAGWORKS_API_KEY,\n",
    "    username=DAGWORKS_PROJECT_EMAIL,\n",
    "    dag_name=\"pdf_summarizer\",\n",
    "    tags={\"env\": \"local\", \"origin\": \"notebook\"}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T04:12:02.976514Z",
     "start_time": "2023-08-08T04:12:01.965295Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Capturing execution run. All runs for project can be found at https://app.dagworks.io/dashboard/project/66/runs\n",
      "100%|██████████| 5/5 [00:08<00:00,  1.76s/it]\n",
      "\n",
      "Captured execution run. Results can be found at https://app.dagworks.io/dashboard/project/66/runs/935\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core Argument:\n",
      "- The Hamilton framework is a high-level modeling approach for dataflows that simplifies the user experience for data scientists and provides a unified interface for describing end-to-end dataflows.\n",
      "- Traditional ETL approaches at Stitch Fix had problems such as poor documentation, low unit test coverage, limited code reuse, and difficulty in changing underlying infrastructure, which led to the development of the Hamilton framework.\n",
      "\n",
      "Evidence:\n",
      "- The Hamilton framework has been used to scale modeling dataflows at Stitch Fix to support over 4000 data transformations without impacting team and user productivity.\n",
      "- The Hamilton programming paradigm encourages the use of vector computation, improves code readability and documentation, and allows for tight encapsulation of transform logic and unit testing.\n",
      "- Hamilton provides decorators to encapsulate operational concerns and reduce repetitive function logic.\n",
      "- The function DAG is the framework's representation of the nodes that should be executed and the dependencies between them.\n",
      "- The driver code in Hamilton steers the execution of the function DAG and provides a convenient abstraction layer for users.\n",
      "\n",
      "Conclusions:\n",
      "- The Hamilton framework provides a simpler user experience for data scientists and allows for easy integration with existing data management tooling in a modular fashion.\n",
      "- Hamilton improves code readability, modularity, and reusability in data management systems.\n",
      "- Hamilton offers benefits such as incremental development, debugging capabilities, a central definition store, transparent scaling, lineage tracking, and modular components.\n",
      "- Adoption of Hamilton has been successful among teams with active feature development for time-series forecasting and those using Pandas.\n",
      "- Future extensions for Hamilton include integrating with data governance tools and compiling to an orchestration framework.\n"
     ]
    }
   ],
   "source": [
    "with open(\"hamilton_paper.pdf\", \"rb\") as f:\n",
    "    result = dwdr.execute([\"summarized_text\"], inputs={\n",
    "        \"pdf_source\": f,\n",
    "        \"openai_gpt_model\": \"gpt-3.5-turbo-0613\",\n",
    "        \"content_type\": \"Scientific article\",\n",
    "        \"user_query\": \"Can you ELI5 the paper?\"\n",
    "    })\n",
    "print(result[\"summarized_text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T04:12:25.959572Z",
     "start_time": "2023-08-08T04:12:05.371077Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-08T04:12:25.965299Z",
     "start_time": "2023-08-08T04:12:25.961491Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
