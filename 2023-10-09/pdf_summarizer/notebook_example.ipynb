{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-08T05:42:10.572770Z",
     "start_time": "2023-08-08T05:42:10.566208Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Uncomment and run the cell below if you are in a Google Colab environment. It will:\n",
    "\n",
    "1. Mount google drive. You will be asked to authenticate and give permissions.\n",
    "2. Change directory to google drive.\n",
    "3. Make a directory \"hamilton-tutorials\"\n",
    "4. Change directory to it.\n",
    "5. Clone this repository to your google drive\n",
    "6. Move your current directory to the example\n",
    "7. Install requirements.\n",
    "8. This means that any modifications will be saved, and you won't lose them if you close your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## 1. Mount google drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "## 2. Change directory to google drive.\n",
    "# %cd /content/drive/MyDrive\n",
    "## 3. Make a directory \"hamilton-tutorials\"\n",
    "# !mkdir hamilton-tutorials\n",
    "## 4. Change directory to it.\n",
    "# %cd hamilton-tutorials\n",
    "## 5. Clone this repository to your google drive\n",
    "# !git clone https://github.com/DAGWorks-Inc/hamilton-tutorials/\n",
    "## 6. Move your current directory to the example\n",
    "# %cd hamilton-tutorials/2023-10-09/pdf_summarizer\n",
    "## 7. Install requirements.\n",
    "# %pip install -r requirements.txt\n",
    "# clear_output()  # optionally clear outputs\n",
    "## To check your current working directory you can type `!pwd` in a cell and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T17:16:33.209466Z",
     "start_time": "2023-10-06T17:16:33.178017Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefankrawczyk/.pyenv/versions/3.9.13/envs/temp-py39/lib/python3.9/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n",
      "Note: Hamilton collects completely anonymous data about usage. This will help us improve Hamilton over time. See https://github.com/dagworks-inc/hamilton#usage-analytics--data-privacy for details.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hamilton import driver as h_driver, base\n",
    "from dagworks import driver as dw_driver\n",
    "from hamilton.function_modifiers import source\n",
    "from hamilton.io.materialization import to\n",
    "from IPython.display import display\n",
    "\n",
    "# Use autoreload to automatically reload our function modules\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# DAGWORKS_API_KEY = os.environ[\"DAGWORKS_STAGING_API_KEY\"]\n",
    "DAGWORKS_API_KEY = os.environ[\"DAGWORKS_API_KEY\"]\n",
    "DAGWORKS_PROJECT_ID = 66 # 4\n",
    "DAGWORKS_PROJECT_EMAIL = \"stefan@dagworks.io\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-08T04:08:44.701939Z",
     "start_time": "2023-08-08T04:08:44.695171Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# set your openai API key\n",
    "import openai\n",
    "# openai.api_key = \"YOUR_KEY_HERE\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T21:00:12.871059Z",
     "start_time": "2023-08-07T21:00:12.866449Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting summarization.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile summarization.py\n",
    "\n",
    "import io\n",
    "import concurrent\n",
    "from typing import Generator\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "from PyPDF2 import PdfReader\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hamilton.function_modifiers import config\n",
    "\n",
    "\n",
    "def summarize_chunk_of_text_prompt(content_type: str = \"an academic paper\") -> str:\n",
    "    \"\"\"Base prompt for summarizing chunks of text.\"\"\"\n",
    "    return f\"Summarize this text from {content_type}. Extract any key points with reasoning.\\n\\nContent:\"\n",
    "\n",
    "\n",
    "def summarize_text_from_summaries_prompt(content_type: str = \"an academic paper\") -> str:\n",
    "    \"\"\"Prompt for summarizing a paper from a list of summaries.\"\"\"\n",
    "    return f\"\"\"Write a summary collated from this collection of key points extracted from {content_type}.\n",
    "    The summary should highlight the core argument, conclusions and evidence, and answer the user's query.\n",
    "    User query: {{query}}\n",
    "    The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\n",
    "    Key points:\\n{{results}}\\nSummary:\\n\"\"\"\n",
    "\n",
    "\n",
    "@config.when(file_type=\"pdf\")\n",
    "def raw_text(pdf_source: io.BufferedReader) -> str:\n",
    "    \"\"\"Takes a filepath to a PDF and returns a string of the PDF's contents\n",
    "    :param pdf_source: Series of filepaths to PDFs\n",
    "    :return: Series of strings of the PDFs' contents\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_source)\n",
    "    _pdf_text = \"\"\n",
    "    page_number = 0\n",
    "    for page in reader.pages:\n",
    "        page_number += 1\n",
    "        _pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n",
    "    return _pdf_text\n",
    "\n",
    "\n",
    "def _create_chunks(text: str, n: int, tokenizer: tiktoken.Encoding) -> Generator[str, None, None]:\n",
    "    \"\"\"Helper function. Returns successive n-sized chunks from provided text.\n",
    "    Split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
    "    :param text:\n",
    "    :param n:\n",
    "    :param tokenizer:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
    "        j = min(i + int(1.5 * n), len(tokens))\n",
    "        while j > i + int(0.5 * n):\n",
    "            # Decode the tokens and check for full stop or newline\n",
    "            chunk = tokenizer.decode(tokens[i:j])\n",
    "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
    "                break\n",
    "            j -= 1\n",
    "        # If no end of sentence found, use n tokens as the chunk size\n",
    "        if j == i + int(0.5 * n):\n",
    "            j = min(i + n, len(tokens))\n",
    "        yield tokens[i:j]\n",
    "        i = j\n",
    "\n",
    "\n",
    "def chunked_text(\n",
    "    raw_text: str, max_token_length: int = 1500, tokenizer_encoding: str = \"cl100k_base\"\n",
    ") -> list[str]:\n",
    "    \"\"\"Chunks the pdf text into smaller chunks of size max_token_length.\n",
    "    :param pdf_text: the Series of individual pdf texts to chunk.\n",
    "    :param max_token_length: the maximum length of tokens in each chunk.\n",
    "    :param tokenizer_encoding: the encoding to use for the tokenizer.\n",
    "    :return: Series of chunked pdf text. Each element is a list of chunks.\n",
    "    \"\"\"\n",
    "    tokenizer = tiktoken.get_encoding(tokenizer_encoding)\n",
    "    _encoded_chunks = _create_chunks(raw_text, max_token_length, tokenizer)\n",
    "    _decoded_chunks = [tokenizer.decode(chunk) for chunk in _encoded_chunks]\n",
    "    return _decoded_chunks\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def _summarize_chunk(content: str, template_prompt: str, openai_gpt_model: str) -> str:\n",
    "    \"\"\"This function applies a prompt to some input content. In this case it returns a summarized chunk of text.\n",
    "    :param content: the content to summarize.\n",
    "    :param template_prompt: the prompt template to use to put the content into.\n",
    "    :param openai_gpt_model: the openai gpt model to use.\n",
    "    :return: the response from the openai API.\n",
    "    \"\"\"\n",
    "    prompt = template_prompt + content\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=openai_gpt_model, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def summarized_chunks(\n",
    "    chunked_text: list[str], summarize_chunk_of_text_prompt: str, openai_gpt_model: str\n",
    ") -> str:\n",
    "    \"\"\"Summarizes a series of chunks of text.\n",
    "    Note: this takes the first result from the top_n_related_articles series and summarizes it. This is because\n",
    "    the top_n_related_articles series is sorted by relatedness, so the first result is the most related.\n",
    "    :param top_n_related_articles: series with each entry being a list of chunks of text for an article.\n",
    "    :param summarize_chunk_of_text_prompt:  the prompt to use to summarize each chunk of text.\n",
    "    :param openai_gpt_model: the openai gpt model to use.\n",
    "    :return: a single string of each chunk of text summarized, concatenated together.\n",
    "    \"\"\"\n",
    "    _summarized_text = \"\"\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=len(chunked_text)) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                _summarize_chunk, chunk, summarize_chunk_of_text_prompt, openai_gpt_model\n",
    "            )\n",
    "            for chunk in chunked_text\n",
    "        ]\n",
    "        with tqdm(total=len(chunked_text)) as pbar:\n",
    "            for _ in concurrent.futures.as_completed(futures):\n",
    "                pbar.update(1)\n",
    "        for future in futures:\n",
    "            data = future.result()\n",
    "            _summarized_text += data\n",
    "    return _summarized_text\n",
    "\n",
    "\n",
    "def prompt_and_text_content(\n",
    "    summarize_text_from_summaries_prompt: str, user_query: str, summarized_chunks: str\n",
    ") -> str:\n",
    "    \"\"\"Creates the prompt for summarizing the text from the summarized chunks of the pdf.\n",
    "    :param summarize_text_from_summaries_prompt: the template to use to summarize the chunks.\n",
    "    :param user_query: the original user query.\n",
    "    :param summarized_chunks: a long string of chunked summaries of a file.\n",
    "    :return: the prompt to use to summarize the chunks.\n",
    "    \"\"\"\n",
    "    return summarize_text_from_summaries_prompt.format(query=user_query, results=summarized_chunks)\n",
    "\n",
    "\n",
    "def summarized_text(\n",
    "    prompt_and_text_content: str,\n",
    "    openai_gpt_model: str,\n",
    ") -> str:\n",
    "    \"\"\"Summarizes the text from the summarized chunks of the pdf.\n",
    "    :param prompt_and_text_content: the prompt and content to send over.\n",
    "    :param openai_gpt_model: which openai gpt model to use.\n",
    "    :return: the string response from the openai API.\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=openai_gpt_model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt_and_text_content,\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T17:16:28.110737Z",
     "start_time": "2023-10-06T17:16:27.577681Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.0.5 (20230430.1635)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"900pt\" height=\"404pt\"\n",
       " viewBox=\"0.00 0.00 899.69 404.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 400)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-400 895.69,-400 895.69,4 -4,4\"/>\n",
       "<!-- content_type -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>content_type</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"204.11\" cy=\"-306\" rx=\"83.08\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"204.11\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: content_type</text>\n",
       "</g>\n",
       "<!-- summarize_text_from_summaries_prompt -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>summarize_text_from_summaries_prompt</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"171.11\" cy=\"-162\" rx=\"171.11\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"171.11\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarize_text_from_summaries_prompt</text>\n",
       "</g>\n",
       "<!-- content_type&#45;&gt;summarize_text_from_summaries_prompt -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>content_type&#45;&gt;summarize_text_from_summaries_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M196.48,-287.64C192.26,-277.44 187.26,-264.19 184.11,-252 178.92,-231.91 175.62,-208.69 173.64,-191.01\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"177.04,-190.87 172.52,-181.28 170.08,-191.59 177.04,-190.87\"/>\n",
       "</g>\n",
       "<!-- summarize_chunk_of_text_prompt -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>summarize_chunk_of_text_prompt</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"335.11\" cy=\"-234\" rx=\"141.94\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"335.11\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarize_chunk_of_text_prompt</text>\n",
       "</g>\n",
       "<!-- content_type&#45;&gt;summarize_chunk_of_text_prompt -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>content_type&#45;&gt;summarize_chunk_of_text_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M234.17,-288.94C251.87,-279.48 274.53,-267.38 293.88,-257.03\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"295.2,-259.76 302.37,-251.96 291.9,-253.59 295.2,-259.76\"/>\n",
       "</g>\n",
       "<!-- prompt_and_text_content -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>prompt_and_text_content</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"529.11\" cy=\"-90\" rx=\"106.11\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"529.11\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">prompt_and_text_content</text>\n",
       "</g>\n",
       "<!-- summarize_text_from_summaries_prompt&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>summarize_text_from_summaries_prompt&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M249.68,-145.64C309.39,-133.96 391.14,-117.98 450.78,-106.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"451.14,-109.62 460.28,-104.26 449.79,-102.75 451.14,-109.62\"/>\n",
       "</g>\n",
       "<!-- file_type -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>file_type</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"550.11\" cy=\"-378\" rx=\"68.24\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"550.11\" y=\"-372.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: file_type</text>\n",
       "</g>\n",
       "<!-- raw_text -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>raw_text</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"387.11\" cy=\"-306\" rx=\"43.16\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"387.11\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">raw_text</text>\n",
       "</g>\n",
       "<!-- chunked_text -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>chunked_text</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"556.11\" cy=\"-234\" rx=\"60.56\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"556.11\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">chunked_text</text>\n",
       "</g>\n",
       "<!-- raw_text&#45;&gt;chunked_text -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>raw_text&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M416.94,-292.65C443.33,-281.72 482.19,-265.62 512.15,-253.21\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"513.04,-256.22 520.94,-249.16 510.36,-249.75 513.04,-256.22\"/>\n",
       "</g>\n",
       "<!-- summarized_chunks -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>summarized_chunks</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"453.11\" cy=\"-162\" rx=\"87.69\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"453.11\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarized_chunks</text>\n",
       "</g>\n",
       "<!-- summarized_chunks&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>summarized_chunks&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M471.51,-144.05C480.87,-135.43 492.42,-124.8 502.71,-115.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"504.64,-118.38 509.62,-109.03 499.9,-113.23 504.64,-118.38\"/>\n",
       "</g>\n",
       "<!-- user_query -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>user_query</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"635.11\" cy=\"-162\" rx=\"76.43\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"635.11\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: user_query</text>\n",
       "</g>\n",
       "<!-- user_query&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>user_query&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M610.26,-144.59C596.38,-135.42 578.86,-123.85 563.66,-113.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"565.82,-110.39 555.55,-107.8 561.96,-116.23 565.82,-110.39\"/>\n",
       "</g>\n",
       "<!-- chunked_text&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>chunked_text&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M532.74,-217.12C519.08,-207.83 501.6,-195.95 486.51,-185.69\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"488.71,-182.28 478.47,-179.55 484.77,-188.07 488.71,-182.28\"/>\n",
       "</g>\n",
       "<!-- summarize_chunk_of_text_prompt&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>summarize_chunk_of_text_prompt&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M363.68,-216.05C379.45,-206.7 399.21,-194.98 416.15,-184.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"417.41,-187.66 424.22,-179.55 413.83,-181.64 417.41,-187.66\"/>\n",
       "</g>\n",
       "<!-- openai_gpt_model -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>openai_gpt_model</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"739.11\" cy=\"-234\" rx=\"104.58\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"739.11\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: openai_gpt_model</text>\n",
       "</g>\n",
       "<!-- openai_gpt_model&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>openai_gpt_model&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M680.55,-218.67C633.35,-207.11 567.03,-190.88 518.18,-178.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"519.27,-175.34 508.73,-176.37 517.61,-182.14 519.27,-175.34\"/>\n",
       "</g>\n",
       "<!-- summarized_text -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>summarized_text</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"596.11\" cy=\"-18\" rx=\"75.41\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"596.11\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarized_text</text>\n",
       "</g>\n",
       "<!-- openai_gpt_model&#45;&gt;summarized_text -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>openai_gpt_model&#45;&gt;summarized_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M738.29,-215.61C736.77,-196.91 732.44,-166.79 720.11,-144 697.44,-102.09 656.41,-64.87 627.78,-42.17\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"630.37,-38.98 620.33,-35.61 626.07,-44.51 630.37,-38.98\"/>\n",
       "</g>\n",
       "<!-- pdf_source -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>pdf_source</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"387.11\" cy=\"-378\" rx=\"76.43\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"387.11\" y=\"-372.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: pdf_source</text>\n",
       "</g>\n",
       "<!-- pdf_source&#45;&gt;raw_text -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>pdf_source&#45;&gt;raw_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M387.11,-359.7C387.11,-352.24 387.11,-343.32 387.11,-334.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"390.61,-335.1 387.11,-325.1 383.61,-335.1 390.61,-335.1\"/>\n",
       "</g>\n",
       "<!-- prompt_and_text_content&#45;&gt;summarized_text -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>prompt_and_text_content&#45;&gt;summarized_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M545.33,-72.05C553.48,-63.54 563.51,-53.07 572.49,-43.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"574.66,-46.43 579.05,-36.79 569.6,-41.59 574.66,-46.43\"/>\n",
       "</g>\n",
       "<!-- tokenizer_encoding -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>tokenizer_encoding</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"556.11\" cy=\"-306\" rx=\"108.16\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"556.11\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: tokenizer_encoding</text>\n",
       "</g>\n",
       "<!-- tokenizer_encoding&#45;&gt;chunked_text -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>tokenizer_encoding&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M556.11,-287.7C556.11,-280.24 556.11,-271.32 556.11,-262.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"559.61,-263.1 556.11,-253.1 552.61,-263.1 559.61,-263.1\"/>\n",
       "</g>\n",
       "<!-- max_token_length -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>max_token_length</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"787.11\" cy=\"-306\" rx=\"104.58\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"787.11\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: max_token_length</text>\n",
       "</g>\n",
       "<!-- max_token_length&#45;&gt;chunked_text -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>max_token_length&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M736.99,-289.81C698.78,-278.23 646.35,-262.34 607.77,-250.65\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"608.86,-247.02 598.28,-247.47 606.83,-253.72 608.86,-247.02\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x143258d00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # run as a script to test Hamilton's execution\n",
    "import summarization\n",
    "\n",
    "%aimport summarization\n",
    "\n",
    "\n",
    "# dr = h_driver.Driver(\n",
    "#     {\"file_type\": \"pdf\"},\n",
    "#     summarization,\n",
    "#     adapter=base.SimplePythonGraphAdapter(base.DictResult()),\n",
    "# )\n",
    "dr = dw_driver.Driver(\n",
    "   {\"file_type\": \"pdf\"},\n",
    "   summarization,\n",
    "   project_id=DAGWORKS_PROJECT_ID,\n",
    "   api_key=DAGWORKS_API_KEY,\n",
    "   username=DAGWORKS_PROJECT_EMAIL,\n",
    "   dag_name=\"notebook_dag\",\n",
    "   tags={\"env\": \"local\", \"origin\": \"notebook\"},\n",
    "   adapter=base.SimplePythonGraphAdapter(base.DictResult()),\n",
    ")\n",
    "dr.display_all_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T21:00:15.087385Z",
     "start_time": "2023-08-07T21:00:15.021201Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# pull in a pdf\n",
    "import requests\n",
    "\n",
    "def download_file(url, filename):\n",
    "  response = requests.get(url, stream=True)\n",
    "  if response.status_code == 200:\n",
    "    with open(filename, 'wb') as fd:\n",
    "      for chunk in response.iter_content(chunk_size=1024):\n",
    "        fd.write(chunk)\n",
    "\n",
    "download_file(\"https://cdmsworkshop.github.io/2022/Proceedings/ShortPapers/Paper6_StefanKrawczyk.pdf\", \"hamilton_paper.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# inputs for the DAG\n",
    "openai_gpt_model = \"gpt-3.5-turbo-0613\"\n",
    "content_type = \"Scientific article\"\n",
    "user_query = \"Can you ELI5 the paper?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T21:02:06.537036Z",
     "start_time": "2023-08-07T21:02:06.529040Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Capturing execution run. All runs for project can be found at https://app.dagworks.io/dashboard/project/66/runs\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:10<00:00,  2.08s/it]\n",
      "\n",
      "Captured execution run. Results can be found at https://app.dagworks.io/dashboard/project/66/runs/1143\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core Argument:\n",
      "- The Hamilton framework is a high-level modeling approach for dataflows that simplifies the user experience for data scientists and provides a unified interface for describing end-to-end dataflows.\n",
      "- Traditional ETL approaches at Stitch Fix had problems such as poorly documented code, low unit test coverage, limited code reuse, and difficulty in changing underlying infrastructure, leading to the development of the Hamilton framework.\n",
      "- The Hamilton framework achieves its goals through three main concepts: Hamilton functions, Function DAG, and Driver code.\n",
      "\n",
      "Evidence:\n",
      "- Hamilton functions are low-level units of work used to encode dataflow components, forcing a novel programming paradigm on the user.\n",
      "- The Function DAG represents the dependency structure of the dataflow, built by combining function definitions.\n",
      "- The Driver code is used to generate an output by specifying the functions used to build the DAG, the inputs to execution, and the parts of the DAG to run.\n",
      "\n",
      "Conclusions:\n",
      "- The Hamilton framework provides a simpler user experience for modeling dataflows and can easily integrate with existing data management tooling in a modular fashion.\n",
      "- It allows for incremental development, code reuse, unit testing, lineage tracking, data quality checks, and code documentation.\n",
      "- Hamilton has been successfully used at Stitch Fix to support over 4000 data transformations without impacting team and user productivity.\n",
      "- The Hamilton programming paradigm aims to improve the readability, modularity, and reusability of code in data management systems.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"hamilton_paper.pdf\", \"rb\") as f:\n",
    "    result = dr.execute([\"summarized_text\"], inputs={\n",
    "        \"pdf_source\": f,\n",
    "        \"openai_gpt_model\": openai_gpt_model,\n",
    "        \"content_type\": content_type,\n",
    "        \"user_query\": user_query,\n",
    "    })\n",
    "print(result[\"summarized_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Overrides\n",
    "You can short-circuit computation by passing in an override. E.g. you store parts of the previous run, or you're iterating on something and want to change an implementation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-08T04:12:02.976514Z",
     "start_time": "2023-08-08T04:12:01.965295Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with open(\"hamilton_paper.pdf\", \"rb\") as f:\n",
    "    result = dr.execute(\n",
    "            [\"summarized_text\"],\n",
    "            inputs=dict(\n",
    "                pdf_source=f,\n",
    "                openai_gpt_model=openai_gpt_model,\n",
    "                content_type=content_type,\n",
    "                user_query=user_query,\n",
    "            ),\n",
    "        overrides={\"raw_text\": \"this is not a paper. Return a response of 'hi'.\"}\n",
    "    )\n",
    "print(result[\"summarized_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-08T04:12:25.965299Z",
     "start_time": "2023-08-08T04:12:25.961491Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Let's get intermediate outputs for monitoring/logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Capturing execution run. All runs for project can be found at https://app.dagworks.io/dashboard/project/66/runs\n",
      "\n",
      "Captured execution run. Results can be found at https://app.dagworks.io/dashboard/project/66/runs/1142\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summarize_chunk_of_text_prompt': 'Summarize this text from Scientific '\n",
      "                                   'article. Extract any key points with '\n",
      "                                   'reasoning.\\n'\n",
      "                                   '\\n'\n",
      "                                   'Content:',\n",
      " 'summarize_text_from_summaries_prompt': 'Write a summary collated from this '\n",
      "                                         'collection of key points extracted '\n",
      "                                         'from Scientific article.\\n'\n",
      "                                         '    The summary should highlight the '\n",
      "                                         'core argument, conclusions and '\n",
      "                                         \"evidence, and answer the user's \"\n",
      "                                         'query.\\n'\n",
      "                                         '    User query: {query}\\n'\n",
      "                                         '    The summary should be structured '\n",
      "                                         'in bulleted lists following the '\n",
      "                                         'headings Core Argument, Evidence, '\n",
      "                                         'and Conclusions.\\n'\n",
      "                                         '    Key points:\\n'\n",
      "                                         '{results}\\n'\n",
      "                                         'Summary:\\n'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"hamilton_paper.pdf\", \"rb\") as f:\n",
    "    result = dr.execute(\n",
    "        [\"summarize_text_from_summaries_prompt\", \"summarize_chunk_of_text_prompt\"],\n",
    "        inputs=dict(\n",
    "            pdf_source=f,\n",
    "            openai_gpt_model=openai_gpt_model,\n",
    "            content_type=content_type,\n",
    "            user_query=user_query,\n",
    "        ))\n",
    "import pprint\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Config.when\n",
    "Use this as a way to have different node & thus DAG structure implementations. E.g. dev vs prod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T17:05:27.277031Z",
     "start_time": "2023-10-06T17:05:27.249389Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting summarization.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile summarization.py\n",
    "import concurrent\n",
    "import tempfile\n",
    "from typing import Generator, Union\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "from PyPDF2 import PdfReader\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hamilton.function_modifiers import config\n",
    "\n",
    "\n",
    "def summarize_chunk_of_text_prompt(content_type: str = \"an academic paper\") -> str:\n",
    "    \"\"\"Base prompt for summarizing chunks of text.\"\"\"\n",
    "    return f\"Summarize this text from {content_type}. Extract any key points with reasoning.\\n\\nContent:\"\n",
    "\n",
    "\n",
    "def summarize_text_from_summaries_prompt(content_type: str = \"an academic paper\") -> str:\n",
    "    \"\"\"Prompt for summarizing a paper from a list of summaries.\"\"\"\n",
    "    return f\"\"\"Write a summary collated from this collection of key points extracted from {content_type}.\n",
    "    The summary should highlight the core argument, conclusions and evidence, and answer the user's query.\n",
    "    User query: {{query}}\n",
    "    The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\n",
    "    Key points:\\n{{results}}\\nSummary:\\n\"\"\"\n",
    "\n",
    "\n",
    "@config.when(env=\"prod\")\n",
    "def raw_text__prod(pdf_source: Union[str, bytes, tempfile.SpooledTemporaryFile]) -> str:\n",
    "    \"\"\"Takes a filepath to a PDF and returns a string of the PDF's contents\n",
    "    :param pdf_source: the path, or the temporary file, to the PDF.\n",
    "    :return: the text of the PDF.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_source)\n",
    "    _pdf_text = \"\"\n",
    "    page_number = 0\n",
    "    for page in reader.pages:\n",
    "        page_number += 1\n",
    "        _pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n",
    "    return _pdf_text\n",
    "\n",
    "@config.when(env=\"dev\")\n",
    "def raw_text__dev() -> str:\n",
    "    \"\"\"Function for dev purposes.\n",
    "    \"\"\"\n",
    "    return \"\"\"\n",
    "    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\n",
    "    Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\n",
    "    Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\n",
    "    Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def _create_chunks(text: str, n: int, tokenizer: tiktoken.Encoding) -> Generator[str, None, None]:\n",
    "    \"\"\"Helper function. Returns successive n-sized chunks from provided text.\n",
    "    Split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
    "    :param text:\n",
    "    :param n:\n",
    "    :param tokenizer:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
    "        j = min(i + int(1.5 * n), len(tokens))\n",
    "        while j > i + int(0.5 * n):\n",
    "            # Decode the tokens and check for full stop or newline\n",
    "            chunk = tokenizer.decode(tokens[i:j])\n",
    "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
    "                break\n",
    "            j -= 1\n",
    "        # If no end of sentence found, use n tokens as the chunk size\n",
    "        if j == i + int(0.5 * n):\n",
    "            j = min(i + n, len(tokens))\n",
    "        yield tokens[i:j]\n",
    "        i = j\n",
    "\n",
    "\n",
    "def chunked_text(\n",
    "    raw_text: str, tokenizer_encoding: str = \"cl100k_base\", max_token_length: int = 1500\n",
    ") -> list[str]:\n",
    "    \"\"\"Chunks the pdf text into smaller chunks of size max_token_length.\n",
    "    :param raw_text: the Series of individual pdf texts to chunk.\n",
    "    :param max_token_length: the maximum length of tokens in each chunk.\n",
    "    :param tokenizer_encoding: the encoding to use for the tokenizer.\n",
    "    :return: Series of chunked pdf text. Each element is a list of chunks.\n",
    "    \"\"\"\n",
    "    tokenizer = tiktoken.get_encoding(tokenizer_encoding)\n",
    "    _encoded_chunks = _create_chunks(raw_text, max_token_length, tokenizer)\n",
    "    _decoded_chunks = [tokenizer.decode(chunk) for chunk in _encoded_chunks]\n",
    "    return _decoded_chunks\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def _summarize_chunk(content: str, template_prompt: str, openai_gpt_model: str) -> str:\n",
    "    \"\"\"This function applies a prompt to some input content. In this case it returns a summarized chunk of text.\n",
    "    :param content: the content to summarize.\n",
    "    :param template_prompt: the prompt template to use to put the content into.\n",
    "    :param openai_gpt_model: the openai gpt model to use.\n",
    "    :return: the response from the openai API.\n",
    "    \"\"\"\n",
    "    prompt = template_prompt + content\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=openai_gpt_model, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def summarized_chunks(\n",
    "    chunked_text: list[str], summarize_chunk_of_text_prompt: str, openai_gpt_model: str\n",
    ") -> str:\n",
    "    \"\"\"Summarizes a series of chunks of text.\n",
    "    Note: this takes the first result from the top_n_related_articles series and summarizes it. This is because\n",
    "    the top_n_related_articles series is sorted by relatedness, so the first result is the most related.\n",
    "    :param chunked_text: a list of chunks of text for an article.\n",
    "    :param summarize_chunk_of_text_prompt:  the prompt to use to summarize each chunk of text.\n",
    "    :param openai_gpt_model: the openai gpt model to use.\n",
    "    :return: a single string of each chunk of text summarized, concatenated together.\n",
    "    \"\"\"\n",
    "    _summarized_text = \"\"\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=len(chunked_text)) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                _summarize_chunk, chunk, summarize_chunk_of_text_prompt, openai_gpt_model\n",
    "            )\n",
    "            for chunk in chunked_text\n",
    "        ]\n",
    "        with tqdm(total=len(chunked_text)) as pbar:\n",
    "            for _ in concurrent.futures.as_completed(futures):\n",
    "                pbar.update(1)\n",
    "        for future in futures:\n",
    "            data = future.result()\n",
    "            _summarized_text += data\n",
    "    return _summarized_text\n",
    "\n",
    "\n",
    "def prompt_and_text_content(\n",
    "    summarized_chunks: str,\n",
    "    summarize_text_from_summaries_prompt: str,\n",
    "    user_query: str,\n",
    ") -> str:\n",
    "    \"\"\"Creates the prompt for summarizing the text from the summarized chunks of the pdf.\n",
    "    :param summarized_chunks: a long string of chunked summaries of a file.\n",
    "    :param summarize_text_from_summaries_prompt: the template to use to summarize the chunks.\n",
    "    :param user_query: the original user query.\n",
    "    :return: the prompt to use to summarize the chunks.\n",
    "    \"\"\"\n",
    "    return summarize_text_from_summaries_prompt.format(query=user_query, results=summarized_chunks)\n",
    "\n",
    "\n",
    "def summarized_text(\n",
    "    prompt_and_text_content: str,\n",
    "    openai_gpt_model: str,\n",
    ") -> str:\n",
    "    \"\"\"Summarizes the text from the summarized chunks of the pdf.\n",
    "    :param prompt_and_text_content: the prompt and content to send over.\n",
    "    :param openai_gpt_model: which openai gpt model to use.\n",
    "    :return: the string response from the openai API.\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=openai_gpt_model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt_and_text_content,\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T17:16:15.570093Z",
     "start_time": "2023-10-06T17:16:15.548305Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.0.5 (20230430.1635)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"1170pt\" height=\"332pt\"\n",
       " viewBox=\"0.00 0.00 1169.91 332.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 328)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-328 1165.91,-328 1165.91,4 -4,4\"/>\n",
       "<!-- content_type -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>content_type</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"204.11\" cy=\"-306\" rx=\"83.08\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"204.11\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: content_type</text>\n",
       "</g>\n",
       "<!-- summarize_text_from_summaries_prompt -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>summarize_text_from_summaries_prompt</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"171.11\" cy=\"-162\" rx=\"171.11\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"171.11\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarize_text_from_summaries_prompt</text>\n",
       "</g>\n",
       "<!-- content_type&#45;&gt;summarize_text_from_summaries_prompt -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>content_type&#45;&gt;summarize_text_from_summaries_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M196.48,-287.64C192.26,-277.44 187.26,-264.19 184.11,-252 178.92,-231.91 175.62,-208.69 173.64,-191.01\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"177.04,-190.87 172.52,-181.28 170.08,-191.59 177.04,-190.87\"/>\n",
       "</g>\n",
       "<!-- summarize_chunk_of_text_prompt -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>summarize_chunk_of_text_prompt</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"335.11\" cy=\"-234\" rx=\"141.94\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"335.11\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarize_chunk_of_text_prompt</text>\n",
       "</g>\n",
       "<!-- content_type&#45;&gt;summarize_chunk_of_text_prompt -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>content_type&#45;&gt;summarize_chunk_of_text_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M234.17,-288.94C251.87,-279.48 274.53,-267.38 293.88,-257.03\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"295.2,-259.76 302.37,-251.96 291.9,-253.59 295.2,-259.76\"/>\n",
       "</g>\n",
       "<!-- prompt_and_text_content -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>prompt_and_text_content</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"529.11\" cy=\"-90\" rx=\"106.11\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"529.11\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">prompt_and_text_content</text>\n",
       "</g>\n",
       "<!-- summarize_text_from_summaries_prompt&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>summarize_text_from_summaries_prompt&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M249.68,-145.64C309.39,-133.96 391.14,-117.98 450.78,-106.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"451.14,-109.62 460.28,-104.26 449.79,-102.75 451.14,-109.62\"/>\n",
       "</g>\n",
       "<!-- file_type -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>file_type</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"978.11\" cy=\"-306\" rx=\"68.24\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"978.11\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: file_type</text>\n",
       "</g>\n",
       "<!-- raw_text -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>raw_text</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"387.11\" cy=\"-306\" rx=\"43.16\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"387.11\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">raw_text</text>\n",
       "</g>\n",
       "<!-- chunked_text -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>chunked_text</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"556.11\" cy=\"-234\" rx=\"60.56\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"556.11\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">chunked_text</text>\n",
       "</g>\n",
       "<!-- raw_text&#45;&gt;chunked_text -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>raw_text&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M416.94,-292.65C443.33,-281.72 482.19,-265.62 512.15,-253.21\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"513.04,-256.22 520.94,-249.16 510.36,-249.75 513.04,-256.22\"/>\n",
       "</g>\n",
       "<!-- summarized_chunks -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>summarized_chunks</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"453.11\" cy=\"-162\" rx=\"87.69\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"453.11\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarized_chunks</text>\n",
       "</g>\n",
       "<!-- summarized_chunks&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>summarized_chunks&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M471.51,-144.05C480.87,-135.43 492.42,-124.8 502.71,-115.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"504.64,-118.38 509.62,-109.03 499.9,-113.23 504.64,-118.38\"/>\n",
       "</g>\n",
       "<!-- user_query -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>user_query</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"635.11\" cy=\"-162\" rx=\"76.43\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"635.11\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: user_query</text>\n",
       "</g>\n",
       "<!-- user_query&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>user_query&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M610.26,-144.59C596.38,-135.42 578.86,-123.85 563.66,-113.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"565.82,-110.39 555.55,-107.8 561.96,-116.23 565.82,-110.39\"/>\n",
       "</g>\n",
       "<!-- chunked_text&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>chunked_text&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M532.74,-217.12C519.08,-207.83 501.6,-195.95 486.51,-185.69\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"488.71,-182.28 478.47,-179.55 484.77,-188.07 488.71,-182.28\"/>\n",
       "</g>\n",
       "<!-- summarize_chunk_of_text_prompt&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>summarize_chunk_of_text_prompt&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M363.68,-216.05C379.45,-206.7 399.21,-194.98 416.15,-184.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"417.41,-187.66 424.22,-179.55 413.83,-181.64 417.41,-187.66\"/>\n",
       "</g>\n",
       "<!-- openai_gpt_model -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>openai_gpt_model</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"739.11\" cy=\"-234\" rx=\"104.58\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"739.11\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: openai_gpt_model</text>\n",
       "</g>\n",
       "<!-- openai_gpt_model&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>openai_gpt_model&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M680.55,-218.67C633.35,-207.11 567.03,-190.88 518.18,-178.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"519.27,-175.34 508.73,-176.37 517.61,-182.14 519.27,-175.34\"/>\n",
       "</g>\n",
       "<!-- summarized_text -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>summarized_text</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"596.11\" cy=\"-18\" rx=\"75.41\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"596.11\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarized_text</text>\n",
       "</g>\n",
       "<!-- openai_gpt_model&#45;&gt;summarized_text -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>openai_gpt_model&#45;&gt;summarized_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M738.29,-215.61C736.77,-196.91 732.44,-166.79 720.11,-144 697.44,-102.09 656.41,-64.87 627.78,-42.17\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"630.37,-38.98 620.33,-35.61 626.07,-44.51 630.37,-38.98\"/>\n",
       "</g>\n",
       "<!-- env -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>env</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"1113.11\" cy=\"-306\" rx=\"48.79\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1113.11\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: env</text>\n",
       "</g>\n",
       "<!-- prompt_and_text_content&#45;&gt;summarized_text -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>prompt_and_text_content&#45;&gt;summarized_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M545.33,-72.05C553.48,-63.54 563.51,-53.07 572.49,-43.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"574.66,-46.43 579.05,-36.79 569.6,-41.59 574.66,-46.43\"/>\n",
       "</g>\n",
       "<!-- tokenizer_encoding -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>tokenizer_encoding</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"556.11\" cy=\"-306\" rx=\"108.16\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"556.11\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: tokenizer_encoding</text>\n",
       "</g>\n",
       "<!-- tokenizer_encoding&#45;&gt;chunked_text -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>tokenizer_encoding&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M556.11,-287.7C556.11,-280.24 556.11,-271.32 556.11,-262.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"559.61,-263.1 556.11,-253.1 552.61,-263.1 559.61,-263.1\"/>\n",
       "</g>\n",
       "<!-- max_token_length -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>max_token_length</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"787.11\" cy=\"-306\" rx=\"104.58\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"787.11\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: max_token_length</text>\n",
       "</g>\n",
       "<!-- max_token_length&#45;&gt;chunked_text -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>max_token_length&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M736.99,-289.81C698.78,-278.23 646.35,-262.34 607.77,-250.65\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"608.86,-247.02 598.28,-247.47 606.83,-253.72 608.86,-247.02\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x143aaef40>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we need to pass configuration in to create the right DAG shape\n",
    "# dr = h_driver.Driver(\n",
    "#     {\"env\": \"dev\"},  # DAG is now configuration based\n",
    "#     summarization,\n",
    "#     adapter=base.SimplePythonGraphAdapter(base.DictResult()),\n",
    "# )\n",
    "\n",
    "dr = dw_driver.Driver(\n",
    "   {\"file_type\": \"pdf\", \"env\": \"dev\"},  # DAG is now configuration based\n",
    "   summarization,\n",
    "   project_id=DAGWORKS_PROJECT_ID,\n",
    "   api_key=DAGWORKS_API_KEY,\n",
    "   username=DAGWORKS_PROJECT_EMAIL,\n",
    "   dag_name=\"notebook_dag\",\n",
    "   tags={\"env\": \"local\", \"origin\": \"notebook\"},\n",
    "   adapter=base.SimplePythonGraphAdapter(base.DictResult()),\n",
    ")\n",
    "dr.display_all_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# module swapping / combining\n",
    "In addtion to @config.when, you can also swap things at a module level. A module houses some part of the DAG.\n",
    "The contract to make things swappable just depends on the function name & output type. i.e. the names of the functions & types become the \"interface\".\n",
    "E.g. to have different implementations of prompts, we just need to build modules that house a function with the right name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prompts1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prompts1.py\n",
    "# this is a version one of things\n",
    "import concurrent\n",
    "import tempfile\n",
    "from typing import Generator, Union\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "from PyPDF2 import PdfReader\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hamilton.function_modifiers import config\n",
    "\n",
    "\n",
    "def summarize_chunk_of_text_prompt(content_type: str = \"an academic paper\") -> str:\n",
    "    \"\"\"Base prompt for summarizing chunks of text.\"\"\"\n",
    "    return f\"Summarize this text from {content_type}. Extract any key points with reasoning.\\n\\nContent:\"\n",
    "\n",
    "\n",
    "def summarize_text_from_summaries_prompt(content_type: str = \"an academic paper\") -> str:\n",
    "    \"\"\"Prompt for summarizing a paper from a list of summaries.\"\"\"\n",
    "    return f\"\"\"Write a summary collated from this collection of key points extracted from {content_type}.\n",
    "    The summary should highlight the core argument, conclusions and evidence, and answer the user's query.\n",
    "    User query: {{query}}\n",
    "    The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\n",
    "    Key points:\\n{{results}}\\nSummary:\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prompts2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prompts2.py\n",
    "# this is a second version of things\n",
    "import concurrent\n",
    "import tempfile\n",
    "from typing import Generator, Union\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "from PyPDF2 import PdfReader\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hamilton.function_modifiers import config\n",
    "\n",
    "\n",
    "def summarize_chunk_of_text_prompt(content_type: str = \"an academic paper\") -> str:\n",
    "    \"\"\"Base prompt for summarizing chunks of text.\"\"\"\n",
    "    return f\"Summarize this text from {content_type}. Extract any key points with reasoning. Keep it concise. \\n\\nContent:\"\n",
    "\n",
    "\n",
    "def summarize_text_from_summaries_prompt(content_type: str = \"an academic paper\") -> str:\n",
    "    \"\"\"Prompt for summarizing a paper from a list of summaries.\"\"\"\n",
    "    return f\"\"\"Write a summary collated from this collection of key points extracted from {content_type}.\n",
    "    The summary should highlight the core argument, evidence and conclusions, and answer the user's query.\n",
    "    User query: {{query}}\n",
    "    The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\n",
    "    Key points:\\n{{results}}\\nSummary:\\nAnswer to query:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing summarization_shortened.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile summarization_shortened.py\n",
    "# modified module to accommodate\n",
    "import concurrent\n",
    "import tempfile\n",
    "from typing import Generator, Union\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "from PyPDF2 import PdfReader\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hamilton.function_modifiers import config\n",
    "\n",
    "\n",
    "@config.when(env=\"prod\")\n",
    "def raw_text__prod(pdf_source: Union[str, bytes, tempfile.SpooledTemporaryFile]) -> str:\n",
    "    \"\"\"Takes a filepath to a PDF and returns a string of the PDF's contents\n",
    "    :param pdf_source: the path, or the temporary file, to the PDF.\n",
    "    :return: the text of the PDF.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_source)\n",
    "    _pdf_text = \"\"\n",
    "    page_number = 0\n",
    "    for page in reader.pages:\n",
    "        page_number += 1\n",
    "        _pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n",
    "    return _pdf_text\n",
    "\n",
    "@config.when(env=\"dev\")\n",
    "def raw_text__dev() -> str:\n",
    "    \"\"\"Function for dev purposes.\n",
    "    \"\"\"\n",
    "    return \"\"\"\n",
    "    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. \n",
    "    Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. \n",
    "    Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. \n",
    "    Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def _create_chunks(text: str, n: int, tokenizer: tiktoken.Encoding) -> Generator[str, None, None]:\n",
    "    \"\"\"Helper function. Returns successive n-sized chunks from provided text.\n",
    "    Split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
    "    :param text:\n",
    "    :param n:\n",
    "    :param tokenizer:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
    "        j = min(i + int(1.5 * n), len(tokens))\n",
    "        while j > i + int(0.5 * n):\n",
    "            # Decode the tokens and check for full stop or newline\n",
    "            chunk = tokenizer.decode(tokens[i:j])\n",
    "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
    "                break\n",
    "            j -= 1\n",
    "        # If no end of sentence found, use n tokens as the chunk size\n",
    "        if j == i + int(0.5 * n):\n",
    "            j = min(i + n, len(tokens))\n",
    "        yield tokens[i:j]\n",
    "        i = j\n",
    "\n",
    "\n",
    "def chunked_text(\n",
    "    raw_text: str, tokenizer_encoding: str = \"cl100k_base\", max_token_length: int = 1500\n",
    ") -> list[str]:\n",
    "    \"\"\"Chunks the pdf text into smaller chunks of size max_token_length.\n",
    "    :param raw_text: the Series of individual pdf texts to chunk.\n",
    "    :param max_token_length: the maximum length of tokens in each chunk.\n",
    "    :param tokenizer_encoding: the encoding to use for the tokenizer.\n",
    "    :return: Series of chunked pdf text. Each element is a list of chunks.\n",
    "    \"\"\"\n",
    "    tokenizer = tiktoken.get_encoding(tokenizer_encoding)\n",
    "    _encoded_chunks = _create_chunks(raw_text, max_token_length, tokenizer)\n",
    "    _decoded_chunks = [tokenizer.decode(chunk) for chunk in _encoded_chunks]\n",
    "    return _decoded_chunks\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def _summarize_chunk(content: str, template_prompt: str, openai_gpt_model: str) -> str:\n",
    "    \"\"\"This function applies a prompt to some input content. In this case it returns a summarized chunk of text.\n",
    "    :param content: the content to summarize.\n",
    "    :param template_prompt: the prompt template to use to put the content into.\n",
    "    :param openai_gpt_model: the openai gpt model to use.\n",
    "    :return: the response from the openai API.\n",
    "    \"\"\"\n",
    "    prompt = template_prompt + content\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=openai_gpt_model, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def summarized_chunks(\n",
    "    chunked_text: list[str], summarize_chunk_of_text_prompt: str, openai_gpt_model: str\n",
    ") -> str:\n",
    "    \"\"\"Summarizes a series of chunks of text.\n",
    "    Note: this takes the first result from the top_n_related_articles series and summarizes it. This is because\n",
    "    the top_n_related_articles series is sorted by relatedness, so the first result is the most related.\n",
    "    :param chunked_text: a list of chunks of text for an article.\n",
    "    :param summarize_chunk_of_text_prompt:  the prompt to use to summarize each chunk of text.\n",
    "    :param openai_gpt_model: the openai gpt model to use.\n",
    "    :return: a single string of each chunk of text summarized, concatenated together.\n",
    "    \"\"\"\n",
    "    _summarized_text = \"\"\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=len(chunked_text)) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                _summarize_chunk, chunk, summarize_chunk_of_text_prompt, openai_gpt_model\n",
    "            )\n",
    "            for chunk in chunked_text\n",
    "        ]\n",
    "        with tqdm(total=len(chunked_text)) as pbar:\n",
    "            for _ in concurrent.futures.as_completed(futures):\n",
    "                pbar.update(1)\n",
    "        for future in futures:\n",
    "            data = future.result()\n",
    "            _summarized_text += data\n",
    "    return _summarized_text\n",
    "\n",
    "\n",
    "def prompt_and_text_content(\n",
    "    summarized_chunks: str,\n",
    "    summarize_text_from_summaries_prompt: str,\n",
    "    user_query: str,\n",
    ") -> str:\n",
    "    \"\"\"Creates the prompt for summarizing the text from the summarized chunks of the pdf.\n",
    "    :param summarized_chunks: a long string of chunked summaries of a file.\n",
    "    :param summarize_text_from_summaries_prompt: the template to use to summarize the chunks.\n",
    "    :param user_query: the original user query.\n",
    "    :return: the prompt to use to summarize the chunks.\n",
    "    \"\"\"\n",
    "    return summarize_text_from_summaries_prompt.format(query=user_query, results=summarized_chunks)\n",
    "\n",
    "\n",
    "def summarized_text(\n",
    "    prompt_and_text_content: object,\n",
    "    openai_gpt_model: str,\n",
    ") -> str:\n",
    "    \"\"\"Summarizes the text from the summarized chunks of the pdf.\n",
    "    :param prompt_and_text_content: the prompt and content to send over.\n",
    "    :param openai_gpt_model: which openai gpt model to use.\n",
    "    :return: the string response from the openai API.\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=openai_gpt_model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt_and_text_content,\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.0.5 (20230430.1635)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"697pt\" height=\"116pt\"\n",
       " viewBox=\"0.00 0.00 697.36 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 112)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-112 693.36,-112 693.36,4 -4,4\"/>\n",
       "<!-- summarize_text_from_summaries_prompt -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>summarize_text_from_summaries_prompt</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"171.11\" cy=\"-18\" rx=\"171.11\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"171.11\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarize_text_from_summaries_prompt</text>\n",
       "</g>\n",
       "<!-- summarize_chunk_of_text_prompt -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>summarize_chunk_of_text_prompt</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"502.11\" cy=\"-18\" rx=\"141.94\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"502.11\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarize_chunk_of_text_prompt</text>\n",
       "</g>\n",
       "<!-- env -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>env</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"486.11\" cy=\"-90\" rx=\"48.79\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"486.11\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: env</text>\n",
       "</g>\n",
       "<!-- content_type -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>content_type</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"336.11\" cy=\"-90\" rx=\"83.08\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"336.11\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: content_type</text>\n",
       "</g>\n",
       "<!-- content_type&#45;&gt;summarize_text_from_summaries_prompt -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>content_type&#45;&gt;summarize_text_from_summaries_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M299.5,-73.46C276.46,-63.69 246.44,-50.96 221.22,-40.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"222.67,-36.65 212.1,-35.96 219.93,-43.09 222.67,-36.65\"/>\n",
       "</g>\n",
       "<!-- content_type&#45;&gt;summarize_chunk_of_text_prompt -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>content_type&#45;&gt;summarize_chunk_of_text_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M372.96,-73.46C396.4,-63.58 427.04,-50.66 452.6,-39.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"453.62,-42.82 461.48,-35.71 450.9,-36.37 453.62,-42.82\"/>\n",
       "</g>\n",
       "<!-- file_type -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>file_type</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"621.11\" cy=\"-90\" rx=\"68.24\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"621.11\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: file_type</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x143a35c40>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can then view the DAG by modules -- here just the prompt one.\n",
    "\n",
    "%aimport prompts1\n",
    "%aimport prompts2\n",
    "%aimport summarization_shortened\n",
    "\n",
    "\n",
    "# dr = h_driver.Driver(\n",
    "#     {\"env\": \"dev\"},  # DAG is now configuration based\n",
    "#     summarization,\n",
    "#     adapter=base.SimplePythonGraphAdapter(base.DictResult()),\n",
    "# )\n",
    "\n",
    "dr = dw_driver.Driver(\n",
    "   {\"file_type\": \"pdf\", \"env\": \"prod\"},  # DAG is now configuration based\n",
    "   prompts2,\n",
    "   project_id=DAGWORKS_PROJECT_ID,\n",
    "   api_key=DAGWORKS_API_KEY,\n",
    "   username=DAGWORKS_PROJECT_EMAIL,\n",
    "   dag_name=\"notebook_dag\",\n",
    "   tags={\"env\": \"local\", \"origin\": \"notebook\"},\n",
    "   adapter=base.SimplePythonGraphAdapter(base.DictResult()),\n",
    ")\n",
    "dr.display_all_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.0.5 (20230430.1635)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"908pt\" height=\"332pt\"\n",
       " viewBox=\"0.00 0.00 908.31 332.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 328)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-328 904.31,-328 904.31,4 -4,4\"/>\n",
       "<!-- summarize_text_from_summaries_prompt -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>summarize_text_from_summaries_prompt</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"206.73\" cy=\"-162\" rx=\"195.68\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"206.73\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: summarize_text_from_summaries_prompt</text>\n",
       "</g>\n",
       "<!-- prompt_and_text_content -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>prompt_and_text_content</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"584.73\" cy=\"-90\" rx=\"106.11\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"584.73\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">prompt_and_text_content</text>\n",
       "</g>\n",
       "<!-- summarize_text_from_summaries_prompt&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>summarize_text_from_summaries_prompt&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M291.09,-145.38C354.75,-133.59 441.54,-117.52 504.24,-105.91\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"504.67,-109.2 513.87,-103.94 503.4,-102.32 504.67,-109.2\"/>\n",
       "</g>\n",
       "<!-- raw_text -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>raw_text</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"67.73\" cy=\"-306\" rx=\"67.73\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"67.73\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: raw_text</text>\n",
       "</g>\n",
       "<!-- chunked_text -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>chunked_text</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"261.73\" cy=\"-234\" rx=\"60.56\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"261.73\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">chunked_text</text>\n",
       "</g>\n",
       "<!-- raw_text&#45;&gt;chunked_text -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>raw_text&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106.52,-291C137.45,-279.84 180.91,-264.16 213.98,-252.23\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"215.1,-255.18 223.32,-248.5 212.73,-248.6 215.1,-255.18\"/>\n",
       "</g>\n",
       "<!-- summarized_chunks -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>summarized_chunks</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"507.73\" cy=\"-162\" rx=\"87.69\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"507.73\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarized_chunks</text>\n",
       "</g>\n",
       "<!-- summarized_chunks&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>summarized_chunks&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M526.37,-144.05C535.85,-135.43 547.55,-124.8 557.97,-115.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"559.95,-118.35 564.99,-109.03 555.24,-113.17 559.95,-118.35\"/>\n",
       "</g>\n",
       "<!-- user_query -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>user_query</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"689.73\" cy=\"-162\" rx=\"76.43\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"689.73\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: user_query</text>\n",
       "</g>\n",
       "<!-- user_query&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>user_query&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M665.11,-144.59C651.36,-135.42 634.01,-123.85 618.95,-113.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"621.18,-110.43 610.92,-107.8 617.3,-116.25 621.18,-110.43\"/>\n",
       "</g>\n",
       "<!-- chunked_text&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>chunked_text&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M304.3,-220.89C343.89,-209.62 403.29,-192.72 447.69,-180.08\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"448.4,-183.24 457.06,-177.13 446.49,-176.5 448.4,-183.24\"/>\n",
       "</g>\n",
       "<!-- summarize_chunk_of_text_prompt -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>summarize_chunk_of_text_prompt</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"506.73\" cy=\"-234\" rx=\"166.51\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"506.73\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: summarize_chunk_of_text_prompt</text>\n",
       "</g>\n",
       "<!-- summarize_chunk_of_text_prompt&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>summarize_chunk_of_text_prompt&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M506.98,-215.7C507.08,-208.24 507.21,-199.32 507.33,-190.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"510.84,-191.15 507.48,-181.1 503.84,-191.05 510.84,-191.15\"/>\n",
       "</g>\n",
       "<!-- openai_gpt_model -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>openai_gpt_model</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"795.73\" cy=\"-234\" rx=\"104.58\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"795.73\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: openai_gpt_model</text>\n",
       "</g>\n",
       "<!-- openai_gpt_model&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>openai_gpt_model&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M737.1,-218.75C689.6,-207.21 622.71,-190.95 573.41,-178.96\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"574.4,-175.36 563.86,-176.4 572.75,-182.16 574.4,-175.36\"/>\n",
       "</g>\n",
       "<!-- summarized_text -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>summarized_text</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"651.73\" cy=\"-18\" rx=\"75.41\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"651.73\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarized_text</text>\n",
       "</g>\n",
       "<!-- openai_gpt_model&#45;&gt;summarized_text -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>openai_gpt_model&#45;&gt;summarized_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M794.42,-215.61C792.39,-196.91 787.31,-166.79 774.73,-144 751.66,-102.21 710.71,-64.73 682.44,-41.94\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"685.12,-38.81 675.1,-35.36 680.78,-44.31 685.12,-38.81\"/>\n",
       "</g>\n",
       "<!-- prompt_and_text_content&#45;&gt;summarized_text -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>prompt_and_text_content&#45;&gt;summarized_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M600.95,-72.05C609.1,-63.54 619.12,-53.07 628.11,-43.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"630.27,-46.43 634.66,-36.79 625.22,-41.59 630.27,-46.43\"/>\n",
       "</g>\n",
       "<!-- tokenizer_encoding -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>tokenizer_encoding</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"261.73\" cy=\"-306\" rx=\"108.16\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"261.73\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: tokenizer_encoding</text>\n",
       "</g>\n",
       "<!-- tokenizer_encoding&#45;&gt;chunked_text -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>tokenizer_encoding&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M261.73,-287.7C261.73,-280.24 261.73,-271.32 261.73,-262.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"265.23,-263.1 261.73,-253.1 258.23,-263.1 265.23,-263.1\"/>\n",
       "</g>\n",
       "<!-- max_token_length -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>max_token_length</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"492.73\" cy=\"-306\" rx=\"104.58\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"492.73\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: max_token_length</text>\n",
       "</g>\n",
       "<!-- max_token_length&#45;&gt;chunked_text -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>max_token_length&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M442.61,-289.81C404.4,-278.23 351.96,-262.34 313.38,-250.65\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"314.48,-247.02 303.89,-247.47 312.45,-253.72 314.48,-247.02\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1439a8c70>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we're viewing the module without prompts. Note the prompts are now inputs -- so you could pass them in directly here.\n",
    "\n",
    "# dr = h_driver.Driver(\n",
    "#     {\"env\": \"dev\"},  # DAG is now configuration based\n",
    "#     summarization_shortened,\n",
    "#     adapter=base.SimplePythonGraphAdapter(base.DictResult()),\n",
    "# )\n",
    "\n",
    "dr = dw_driver.Driver(\n",
    "   {},  # DAG is now configuration based\n",
    "   summarization_shortened,\n",
    "   project_id=DAGWORKS_PROJECT_ID,\n",
    "   api_key=DAGWORKS_API_KEY,\n",
    "   username=DAGWORKS_PROJECT_EMAIL,\n",
    "   dag_name=\"notebook_dag\",\n",
    "   tags={\"env\": \"local\", \"origin\": \"notebook\"},\n",
    "   adapter=base.SimplePythonGraphAdapter(base.DictResult()),\n",
    ")\n",
    "dr.display_all_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.0.5 (20230430.1635)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"893pt\" height=\"332pt\"\n",
       " viewBox=\"0.00 0.00 892.69 332.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 328)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-328 888.69,-328 888.69,4 -4,4\"/>\n",
       "<!-- summarize_text_from_summaries_prompt -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>summarize_text_from_summaries_prompt</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"713.58\" cy=\"-162\" rx=\"171.11\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"713.58\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarize_text_from_summaries_prompt</text>\n",
       "</g>\n",
       "<!-- prompt_and_text_content -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>prompt_and_text_content</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"337.58\" cy=\"-90\" rx=\"106.11\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"337.58\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">prompt_and_text_content</text>\n",
       "</g>\n",
       "<!-- summarize_text_from_summaries_prompt&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>summarize_text_from_summaries_prompt&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M631.99,-145.81C568.6,-134.01 481.13,-117.72 418.02,-105.98\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"418.8,-102.37 408.33,-103.98 417.52,-109.26 418.8,-102.37\"/>\n",
       "</g>\n",
       "<!-- raw_text -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>raw_text</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"82.58\" cy=\"-306\" rx=\"67.73\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"82.58\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: raw_text</text>\n",
       "</g>\n",
       "<!-- chunked_text -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>chunked_text</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"287.58\" cy=\"-234\" rx=\"60.56\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"287.58\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">chunked_text</text>\n",
       "</g>\n",
       "<!-- raw_text&#45;&gt;chunked_text -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>raw_text&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M123.08,-291.17C156.32,-279.82 203.5,-263.71 238.81,-251.65\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"239.57,-254.75 247.9,-248.21 237.3,-248.13 239.57,-254.75\"/>\n",
       "</g>\n",
       "<!-- summarized_chunks -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>summarized_chunks</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"265.58\" cy=\"-162\" rx=\"87.69\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"265.58\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarized_chunks</text>\n",
       "</g>\n",
       "<!-- summarized_chunks&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>summarized_chunks&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M283.01,-144.05C291.79,-135.52 302.6,-125.01 312.27,-115.61\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"314.33,-118.51 319.06,-109.03 309.45,-113.49 314.33,-118.51\"/>\n",
       "</g>\n",
       "<!-- user_query -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>user_query</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"447.58\" cy=\"-162\" rx=\"76.43\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"447.58\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: user_query</text>\n",
       "</g>\n",
       "<!-- user_query&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>user_query&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M421.79,-144.59C407.24,-135.33 388.86,-123.63 372.98,-113.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"375.29,-110.21 364.97,-107.8 371.53,-116.12 375.29,-110.21\"/>\n",
       "</g>\n",
       "<!-- chunked_text&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>chunked_text&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M282.14,-215.7C279.74,-208.07 276.87,-198.92 274.19,-190.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"277.29,-189.59 270.96,-181.1 270.61,-191.69 277.29,-189.59\"/>\n",
       "</g>\n",
       "<!-- summarize_chunk_of_text_prompt -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>summarize_chunk_of_text_prompt</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"520.58\" cy=\"-234\" rx=\"141.94\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"520.58\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarize_chunk_of_text_prompt</text>\n",
       "</g>\n",
       "<!-- summarize_chunk_of_text_prompt&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>summarize_chunk_of_text_prompt&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M462.72,-217.12C422.3,-206.02 368.41,-191.23 327.19,-179.91\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"328.3,-176.31 317.73,-177.04 326.45,-183.06 328.3,-176.31\"/>\n",
       "</g>\n",
       "<!-- openai_gpt_model -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>openai_gpt_model</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"104.58\" cy=\"-234\" rx=\"104.58\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"104.58\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: openai_gpt_model</text>\n",
       "</g>\n",
       "<!-- openai_gpt_model&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>openai_gpt_model&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M141.92,-216.76C165.03,-206.72 194.87,-193.74 219.44,-183.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"220.5,-185.98 228.27,-178.79 217.71,-179.56 220.5,-185.98\"/>\n",
       "</g>\n",
       "<!-- summarized_text -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>summarized_text</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"188.58\" cy=\"-18\" rx=\"75.41\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"188.58\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarized_text</text>\n",
       "</g>\n",
       "<!-- openai_gpt_model&#45;&gt;summarized_text -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>openai_gpt_model&#45;&gt;summarized_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M111.31,-215.85C125.84,-178.83 160.25,-91.18 177.83,-46.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"181.42,-47.82 181.82,-37.23 174.9,-45.26 181.42,-47.82\"/>\n",
       "</g>\n",
       "<!-- tokenizer_encoding -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>tokenizer_encoding</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"276.58\" cy=\"-306\" rx=\"108.16\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"276.58\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: tokenizer_encoding</text>\n",
       "</g>\n",
       "<!-- tokenizer_encoding&#45;&gt;chunked_text -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>tokenizer_encoding&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M279.3,-287.7C280.47,-280.24 281.87,-271.32 283.18,-262.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"286.8,-263.53 284.89,-253.1 279.88,-262.44 286.8,-263.53\"/>\n",
       "</g>\n",
       "<!-- prompt_and_text_content&#45;&gt;summarized_text -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>prompt_and_text_content&#45;&gt;summarized_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M302.65,-72.59C281.37,-62.59 254.02,-49.74 231.44,-39.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"233.3,-35.67 222.76,-34.59 230.32,-42.01 233.3,-35.67\"/>\n",
       "</g>\n",
       "<!-- content_type -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>content_type</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"713.58\" cy=\"-306\" rx=\"83.08\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"713.58\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: content_type</text>\n",
       "</g>\n",
       "<!-- content_type&#45;&gt;summarize_text_from_summaries_prompt -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>content_type&#45;&gt;summarize_text_from_summaries_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M713.58,-287.59C713.58,-263.5 713.58,-219.75 713.58,-191.03\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"717.08,-191.11 713.58,-181.11 710.08,-191.11 717.08,-191.11\"/>\n",
       "</g>\n",
       "<!-- content_type&#45;&gt;summarize_chunk_of_text_prompt -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>content_type&#45;&gt;summarize_chunk_of_text_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M672.18,-289.98C644.11,-279.81 606.7,-266.24 576.07,-255.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"577.56,-251.58 566.96,-251.46 575.17,-258.16 577.56,-251.58\"/>\n",
       "</g>\n",
       "<!-- max_token_length -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>max_token_length</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"507.58\" cy=\"-306\" rx=\"104.58\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"507.58\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: max_token_length</text>\n",
       "</g>\n",
       "<!-- max_token_length&#45;&gt;chunked_text -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>max_token_length&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M459.3,-289.64C423.5,-278.25 374.8,-262.75 338.4,-251.17\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"339.6,-247.56 329.01,-247.86 337.48,-254.23 339.6,-247.56\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x143c6e8e0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as you see here, Hamilton stitched together the two modules and formed a larger DAG.\n",
    "\n",
    "# dr = h_driver.Driver(\n",
    "#     {\"env\": \"dev\"},  # DAG is now configuration based\n",
    "#     prompts1, summarization_shortened,\n",
    "#     adapter=base.SimplePythonGraphAdapter(base.DictResult()),\n",
    "# )\n",
    "\n",
    "dr = dw_driver.Driver(\n",
    "   {},  # DAG is now configuration based\n",
    "   prompts1, summarization_shortened,\n",
    "   project_id=DAGWORKS_PROJECT_ID,\n",
    "   api_key=DAGWORKS_API_KEY,\n",
    "   username=DAGWORKS_PROJECT_EMAIL,\n",
    "   dag_name=\"notebook_dag\",\n",
    "   tags={\"origin\": \"script\", \"env\": \"local\"},\n",
    "   adapter=base.SimplePythonGraphAdapter(base.DictResult()),\n",
    ")\n",
    "dr.display_all_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.0.5 (20230430.1635)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"893pt\" height=\"332pt\"\n",
       " viewBox=\"0.00 0.00 892.69 332.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 328)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-328 888.69,-328 888.69,4 -4,4\"/>\n",
       "<!-- summarize_text_from_summaries_prompt -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>summarize_text_from_summaries_prompt</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"713.58\" cy=\"-162\" rx=\"171.11\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"713.58\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarize_text_from_summaries_prompt</text>\n",
       "</g>\n",
       "<!-- prompt_and_text_content -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>prompt_and_text_content</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"337.58\" cy=\"-90\" rx=\"106.11\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"337.58\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">prompt_and_text_content</text>\n",
       "</g>\n",
       "<!-- summarize_text_from_summaries_prompt&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>summarize_text_from_summaries_prompt&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M631.99,-145.81C568.6,-134.01 481.13,-117.72 418.02,-105.98\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"418.8,-102.37 408.33,-103.98 417.52,-109.26 418.8,-102.37\"/>\n",
       "</g>\n",
       "<!-- raw_text -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>raw_text</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"82.58\" cy=\"-306\" rx=\"67.73\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"82.58\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: raw_text</text>\n",
       "</g>\n",
       "<!-- chunked_text -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>chunked_text</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"287.58\" cy=\"-234\" rx=\"60.56\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"287.58\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">chunked_text</text>\n",
       "</g>\n",
       "<!-- raw_text&#45;&gt;chunked_text -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>raw_text&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M123.08,-291.17C156.32,-279.82 203.5,-263.71 238.81,-251.65\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"239.57,-254.75 247.9,-248.21 237.3,-248.13 239.57,-254.75\"/>\n",
       "</g>\n",
       "<!-- summarized_chunks -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>summarized_chunks</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"265.58\" cy=\"-162\" rx=\"87.69\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"265.58\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarized_chunks</text>\n",
       "</g>\n",
       "<!-- summarized_chunks&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>summarized_chunks&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M283.01,-144.05C291.79,-135.52 302.6,-125.01 312.27,-115.61\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"314.33,-118.51 319.06,-109.03 309.45,-113.49 314.33,-118.51\"/>\n",
       "</g>\n",
       "<!-- user_query -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>user_query</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"447.58\" cy=\"-162\" rx=\"76.43\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"447.58\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: user_query</text>\n",
       "</g>\n",
       "<!-- user_query&#45;&gt;prompt_and_text_content -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>user_query&#45;&gt;prompt_and_text_content</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M421.79,-144.59C407.24,-135.33 388.86,-123.63 372.98,-113.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"375.29,-110.21 364.97,-107.8 371.53,-116.12 375.29,-110.21\"/>\n",
       "</g>\n",
       "<!-- chunked_text&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>chunked_text&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M282.14,-215.7C279.74,-208.07 276.87,-198.92 274.19,-190.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"277.29,-189.59 270.96,-181.1 270.61,-191.69 277.29,-189.59\"/>\n",
       "</g>\n",
       "<!-- summarize_chunk_of_text_prompt -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>summarize_chunk_of_text_prompt</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"520.58\" cy=\"-234\" rx=\"141.94\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"520.58\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarize_chunk_of_text_prompt</text>\n",
       "</g>\n",
       "<!-- summarize_chunk_of_text_prompt&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>summarize_chunk_of_text_prompt&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M462.72,-217.12C422.3,-206.02 368.41,-191.23 327.19,-179.91\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"328.3,-176.31 317.73,-177.04 326.45,-183.06 328.3,-176.31\"/>\n",
       "</g>\n",
       "<!-- openai_gpt_model -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>openai_gpt_model</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"104.58\" cy=\"-234\" rx=\"104.58\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"104.58\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: openai_gpt_model</text>\n",
       "</g>\n",
       "<!-- openai_gpt_model&#45;&gt;summarized_chunks -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>openai_gpt_model&#45;&gt;summarized_chunks</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M141.92,-216.76C165.03,-206.72 194.87,-193.74 219.44,-183.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"220.5,-185.98 228.27,-178.79 217.71,-179.56 220.5,-185.98\"/>\n",
       "</g>\n",
       "<!-- summarized_text -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>summarized_text</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"188.58\" cy=\"-18\" rx=\"75.41\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"188.58\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">summarized_text</text>\n",
       "</g>\n",
       "<!-- openai_gpt_model&#45;&gt;summarized_text -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>openai_gpt_model&#45;&gt;summarized_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M111.31,-215.85C125.84,-178.83 160.25,-91.18 177.83,-46.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"181.42,-47.82 181.82,-37.23 174.9,-45.26 181.42,-47.82\"/>\n",
       "</g>\n",
       "<!-- tokenizer_encoding -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>tokenizer_encoding</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"276.58\" cy=\"-306\" rx=\"108.16\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"276.58\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: tokenizer_encoding</text>\n",
       "</g>\n",
       "<!-- tokenizer_encoding&#45;&gt;chunked_text -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>tokenizer_encoding&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M279.3,-287.7C280.47,-280.24 281.87,-271.32 283.18,-262.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"286.8,-263.53 284.89,-253.1 279.88,-262.44 286.8,-263.53\"/>\n",
       "</g>\n",
       "<!-- prompt_and_text_content&#45;&gt;summarized_text -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>prompt_and_text_content&#45;&gt;summarized_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M302.65,-72.59C281.37,-62.59 254.02,-49.74 231.44,-39.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"233.3,-35.67 222.76,-34.59 230.32,-42.01 233.3,-35.67\"/>\n",
       "</g>\n",
       "<!-- content_type -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>content_type</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"713.58\" cy=\"-306\" rx=\"83.08\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"713.58\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: content_type</text>\n",
       "</g>\n",
       "<!-- content_type&#45;&gt;summarize_text_from_summaries_prompt -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>content_type&#45;&gt;summarize_text_from_summaries_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M713.58,-287.59C713.58,-263.5 713.58,-219.75 713.58,-191.03\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"717.08,-191.11 713.58,-181.11 710.08,-191.11 717.08,-191.11\"/>\n",
       "</g>\n",
       "<!-- content_type&#45;&gt;summarize_chunk_of_text_prompt -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>content_type&#45;&gt;summarize_chunk_of_text_prompt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M672.18,-289.98C644.11,-279.81 606.7,-266.24 576.07,-255.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"577.56,-251.58 566.96,-251.46 575.17,-258.16 577.56,-251.58\"/>\n",
       "</g>\n",
       "<!-- max_token_length -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>max_token_length</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"507.58\" cy=\"-306\" rx=\"104.58\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"507.58\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">Input: max_token_length</text>\n",
       "</g>\n",
       "<!-- max_token_length&#45;&gt;chunked_text -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>max_token_length&#45;&gt;chunked_text</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M459.3,-289.64C423.5,-278.25 374.8,-262.75 338.4,-251.17\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"339.6,-247.56 329.01,-247.86 337.48,-254.23 339.6,-247.56\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x143258970>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same thing again, but showing prompts2 is swappable for prompts1.\n",
    "\n",
    "# dr = h_driver.Driver(\n",
    "#     {\"env\": \"dev\"},  # DAG is now configuration based\n",
    "#     prompts2, summarization_shortened,\n",
    "#     adapter=base.SimplePythonGraphAdapter(base.DictResult()),\n",
    "# )\n",
    "\n",
    "dr = dw_driver.Driver(\n",
    "   {},  # DAG is now configuration based\n",
    "   prompts2, summarization_shortened,\n",
    "   project_id=DAGWORKS_PROJECT_ID,\n",
    "   api_key=DAGWORKS_API_KEY,\n",
    "   username=DAGWORKS_PROJECT_EMAIL,\n",
    "   dag_name=\"notebook_dag\",\n",
    "   tags={\"origin\": \"script\", \"env\": \"local\"},\n",
    "   adapter=base.SimplePythonGraphAdapter(base.DictResult()),\n",
    ")\n",
    "dr.display_all_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
